{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cdjLL9gvyVCR"
   },
   "source": [
    "\n",
    "# Pytorch + HuggingFace \n",
    "## KoElectra Model\n",
    "박장원님의 KoElectra-small 사용<br>\n",
    "https://monologg.kr/2020/05/02/koelectra-part1/<br>\n",
    "https://github.com/monologg/KoELECTRA\n",
    "\n",
    "## Dataset\n",
    "네이버 영화 리뷰 데이터셋<br>\n",
    "https://github.com/e9t/nsmc\n",
    "\n",
    "## References\n",
    "- https://huggingface.co/transformers/training.html\n",
    "- https://tutorials.pytorch.kr/beginner/data_loading_tutorial.html\n",
    "- https://tutorials.pytorch.kr/beginner/blitz/cifar10_tutorial.html\n",
    "- https://wikidocs.net/44249\n",
    "\n",
    "## 주의사항\n",
    "꼭 GPU로 해주세요 - 1epoch 당 약 20분 소요"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hc7P9wzHv0LE",
    "outputId": "0973267f-e37a-4be4-a109-29e9288b1cdc"
   },
   "outputs": [],
   "source": [
    "# HuggingFace transformers 설치 및 NSMC 데이터셋 다운로드\n",
    "#!pip install transformers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "5SeEVOGb-t2M"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import AutoTokenizer, ElectraForSequenceClassification, AdamW\n",
    "from tqdm.notebook import tqdm\n",
    "import gc\n",
    "from sklearn.model_selection import KFold\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "-i7pg7DaGsxp"
   },
   "outputs": [],
   "source": [
    "# GPU 사용\n",
    "device = torch.device(\"cuda\")\n",
    "MODEL_NAME = \"monologg/koelectra-small-v3-discriminator\"\n",
    "pre_MODEL_NAME = \"result/MODEL_KoELECTRA-Small-v3-discriminator__3.pt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "#naver movie, Hotel, shopping \n",
    "class NHSDataset(Dataset): \n",
    "  \n",
    "    def __init__(self, csv_file):\n",
    "        # NaN값 제거...\n",
    "        self.dataset = pd.read_csv(csv_file, sep='\\t').dropna(axis=0) \n",
    "        # 중복제거\n",
    "        #self.dataset.drop_duplicates(subset=['text'], inplace=True)\n",
    "        \n",
    "        print(self.dataset)\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "        \n",
    "  \n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "  \n",
    "    def __getitem__(self, idx):\n",
    "        row = self.dataset.iloc[idx, :].values\n",
    "        text = row[0]\n",
    "        #y = row[1]\n",
    "\n",
    "        inputs = self.tokenizer(\n",
    "            text, \n",
    "            return_tensors='pt',\n",
    "            truncation=True,\n",
    "            max_length=256,\n",
    "            pad_to_max_length=True,\n",
    "            add_special_tokens=True\n",
    "            )\n",
    "\n",
    "        input_ids = inputs['input_ids'][0]\n",
    "        attention_mask = inputs['attention_mask'][0]\n",
    "\n",
    "        return input_ids, attention_mask, text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                  text\n",
      "0                    친구들하고 오랜만에 급 계획으로 예약하여 호캉스 다녀왔습니다\n",
      "1                  호텔 주변에 음식점도 많고 부대시설도 잘 되어 있어서 편햇습니다\n",
      "2                         그리고 바로 역도 가까이 있어서 이동이 편하더라구요\n",
      "3                              그런데 객실이 좀 많이 낙후되어 있더라구요\n",
      "4                         그 점을 빼곤 서비스 전체 모든 면에서 괜찮았습니다\n",
      "..                                                 ...\n",
      "904                         화장실도 놀랍고 전반적으로 그냥 모텔 수준입니다\n",
      "905                          장점 위치적인 요인이 가장 플러스로 작용합니다\n",
      "906                                  단점 편하지 않은 직원들의 응대\n",
      "907  두 번 다시는 안 가요 복도에서는 삼겹살 냄새나고 방음도 잘 안 되서 옆 방 초인종...\n",
      "908                                      위치 친절한 직원 사우나\n",
      "\n",
      "[909 rows x 1 columns]\n"
     ]
    }
   ],
   "source": [
    "test_dataset = NHSDataset('data/key_pre/small_test.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\build\\project\\programmers\\4_party_project\\env\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2149: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([    2,  6554,  4006,  4279,  4219, 10953,  4073,  2132,  6333, 10749,\n",
       "          8866, 26509,  3802, 28693, 14991,  4576,  6216,     3,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0]),\n",
       " tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
       " '친구들하고 오랜만에 급 계획으로 예약하여 호캉스 다녀왔습니다')"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "WZKIQNjZwdn1"
   },
   "outputs": [],
   "source": [
    "\n",
    "test_dataset = NHSDataset('data/key_pre/small_test.txt')\n",
    "def init():\n",
    "    \n",
    "    \n",
    "def model_setting():\n",
    "    model = ElectraForSequenceClassification.from_pretrained(MODEL_NAME).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lBC_9oFJKbfn",
    "outputId": "e1e576ae-5310-4149-b33b-a02b40c0752f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([    2,  6554,  4006,  4279,  4219, 10953,  4073,  2132,  6333, 10749,\n",
      "         8866, 26509,  3802, 28693, 14991,  4576,  6216,     3,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0]), tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), '친구들하고 오랜만에 급 계획으로 예약하여 호캉스 다녀왔습니다')\n"
     ]
    }
   ],
   "source": [
    "#train데이터 test데이터 NHSDataset class에 넣어줌\n",
    "#train_dataset = NHSDataset('data/sentiment_test_dataset/hotel_movie_train.txt')\n",
    "print(test_dataset[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vJiAJPUDz40W"
   },
   "source": [
    "# Create Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f7-jRPQXz2r5",
    "outputId": "e97fe72e-487e-47e0-fa74-27d190a002cf"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at monologg/koelectra-small-v3-discriminator were not used when initializing ElectraForSequenceClassification: ['discriminator_predictions.dense.weight', 'discriminator_predictions.dense.bias', 'discriminator_predictions.dense_prediction.weight', 'discriminator_predictions.dense_prediction.bias']\n",
      "- This IS expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of ElectraForSequenceClassification were not initialized from the model checkpoint at monologg/koelectra-small-v3-discriminator and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# 한번 실행해보기\n",
    "# text, attention_mask, y = train_dataset[0]\n",
    "# model(text.unsqueeze(0).to(device), attention_mask=attention_mask.unsqueeze(0).to(device))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YarUnY3ptG8W"
   },
   "source": [
    "# Model 저장되있는거 돌려보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "edb0aIFaXr4D",
    "outputId": "b0fe2fc6-47f2-4d72-96f0-815fdea5abe6"
   },
   "outputs": [],
   "source": [
    "\n",
    "# model.load_state_dict(torch.load(\"원하는모델.pt\"))\n",
    "# text, attention_mask, y = train_dataset[0]\n",
    "# model(text.unsqueeze(0).to(device), attention_mask=attention_mask.unsqueeze(0).to(device))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Dp6x4GHtz46u",
    "outputId": "16643ea9-9aeb-4f8b-ad43-666ff8892e73"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ElectraForSequenceClassification(\n",
       "  (electra): ElectraModel(\n",
       "    (embeddings): ElectraEmbeddings(\n",
       "      (word_embeddings): Embedding(32200, 128, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 128)\n",
       "      (token_type_embeddings): Embedding(2, 128)\n",
       "      (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (embeddings_project): Linear(in_features=128, out_features=256, bias=True)\n",
       "    (encoder): ElectraEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): ElectraLayer(\n",
       "          (attention): ElectraAttention(\n",
       "            (self): ElectraSelfAttention(\n",
       "              (query): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (key): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (value): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): ElectraSelfOutput(\n",
       "              (dense): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): ElectraIntermediate(\n",
       "            (dense): Linear(in_features=256, out_features=1024, bias=True)\n",
       "          )\n",
       "          (output): ElectraOutput(\n",
       "            (dense): Linear(in_features=1024, out_features=256, bias=True)\n",
       "            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): ElectraLayer(\n",
       "          (attention): ElectraAttention(\n",
       "            (self): ElectraSelfAttention(\n",
       "              (query): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (key): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (value): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): ElectraSelfOutput(\n",
       "              (dense): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): ElectraIntermediate(\n",
       "            (dense): Linear(in_features=256, out_features=1024, bias=True)\n",
       "          )\n",
       "          (output): ElectraOutput(\n",
       "            (dense): Linear(in_features=1024, out_features=256, bias=True)\n",
       "            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): ElectraLayer(\n",
       "          (attention): ElectraAttention(\n",
       "            (self): ElectraSelfAttention(\n",
       "              (query): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (key): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (value): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): ElectraSelfOutput(\n",
       "              (dense): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): ElectraIntermediate(\n",
       "            (dense): Linear(in_features=256, out_features=1024, bias=True)\n",
       "          )\n",
       "          (output): ElectraOutput(\n",
       "            (dense): Linear(in_features=1024, out_features=256, bias=True)\n",
       "            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): ElectraLayer(\n",
       "          (attention): ElectraAttention(\n",
       "            (self): ElectraSelfAttention(\n",
       "              (query): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (key): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (value): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): ElectraSelfOutput(\n",
       "              (dense): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): ElectraIntermediate(\n",
       "            (dense): Linear(in_features=256, out_features=1024, bias=True)\n",
       "          )\n",
       "          (output): ElectraOutput(\n",
       "            (dense): Linear(in_features=1024, out_features=256, bias=True)\n",
       "            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): ElectraLayer(\n",
       "          (attention): ElectraAttention(\n",
       "            (self): ElectraSelfAttention(\n",
       "              (query): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (key): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (value): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): ElectraSelfOutput(\n",
       "              (dense): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): ElectraIntermediate(\n",
       "            (dense): Linear(in_features=256, out_features=1024, bias=True)\n",
       "          )\n",
       "          (output): ElectraOutput(\n",
       "            (dense): Linear(in_features=1024, out_features=256, bias=True)\n",
       "            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): ElectraLayer(\n",
       "          (attention): ElectraAttention(\n",
       "            (self): ElectraSelfAttention(\n",
       "              (query): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (key): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (value): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): ElectraSelfOutput(\n",
       "              (dense): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): ElectraIntermediate(\n",
       "            (dense): Linear(in_features=256, out_features=1024, bias=True)\n",
       "          )\n",
       "          (output): ElectraOutput(\n",
       "            (dense): Linear(in_features=1024, out_features=256, bias=True)\n",
       "            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): ElectraLayer(\n",
       "          (attention): ElectraAttention(\n",
       "            (self): ElectraSelfAttention(\n",
       "              (query): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (key): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (value): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): ElectraSelfOutput(\n",
       "              (dense): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): ElectraIntermediate(\n",
       "            (dense): Linear(in_features=256, out_features=1024, bias=True)\n",
       "          )\n",
       "          (output): ElectraOutput(\n",
       "            (dense): Linear(in_features=1024, out_features=256, bias=True)\n",
       "            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): ElectraLayer(\n",
       "          (attention): ElectraAttention(\n",
       "            (self): ElectraSelfAttention(\n",
       "              (query): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (key): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (value): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): ElectraSelfOutput(\n",
       "              (dense): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): ElectraIntermediate(\n",
       "            (dense): Linear(in_features=256, out_features=1024, bias=True)\n",
       "          )\n",
       "          (output): ElectraOutput(\n",
       "            (dense): Linear(in_features=1024, out_features=256, bias=True)\n",
       "            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): ElectraLayer(\n",
       "          (attention): ElectraAttention(\n",
       "            (self): ElectraSelfAttention(\n",
       "              (query): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (key): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (value): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): ElectraSelfOutput(\n",
       "              (dense): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): ElectraIntermediate(\n",
       "            (dense): Linear(in_features=256, out_features=1024, bias=True)\n",
       "          )\n",
       "          (output): ElectraOutput(\n",
       "            (dense): Linear(in_features=1024, out_features=256, bias=True)\n",
       "            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): ElectraLayer(\n",
       "          (attention): ElectraAttention(\n",
       "            (self): ElectraSelfAttention(\n",
       "              (query): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (key): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (value): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): ElectraSelfOutput(\n",
       "              (dense): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): ElectraIntermediate(\n",
       "            (dense): Linear(in_features=256, out_features=1024, bias=True)\n",
       "          )\n",
       "          (output): ElectraOutput(\n",
       "            (dense): Linear(in_features=1024, out_features=256, bias=True)\n",
       "            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): ElectraLayer(\n",
       "          (attention): ElectraAttention(\n",
       "            (self): ElectraSelfAttention(\n",
       "              (query): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (key): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (value): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): ElectraSelfOutput(\n",
       "              (dense): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): ElectraIntermediate(\n",
       "            (dense): Linear(in_features=256, out_features=1024, bias=True)\n",
       "          )\n",
       "          (output): ElectraOutput(\n",
       "            (dense): Linear(in_features=1024, out_features=256, bias=True)\n",
       "            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): ElectraLayer(\n",
       "          (attention): ElectraAttention(\n",
       "            (self): ElectraSelfAttention(\n",
       "              (query): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (key): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (value): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): ElectraSelfOutput(\n",
       "              (dense): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): ElectraIntermediate(\n",
       "            (dense): Linear(in_features=256, out_features=1024, bias=True)\n",
       "          )\n",
       "          (output): ElectraOutput(\n",
       "            (dense): Linear(in_features=1024, out_features=256, bias=True)\n",
       "            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (classifier): ElectraClassificationHead(\n",
       "    (dense): Linear(in_features=256, out_features=256, bias=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (out_proj): Linear(in_features=256, out_features=2, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 모델 레이어 보기\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wmou0LFl0R_X"
   },
   "source": [
    "# Learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "D3m9wdBzUhti"
   },
   "outputs": [],
   "source": [
    "import gc\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "6NpXwESN0Q4h"
   },
   "outputs": [],
   "source": [
    "epochs = 10\n",
    "batch_size = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "k7JZncS23JIa"
   },
   "outputs": [],
   "source": [
    "optimizer = AdamW(model.parameters(), lr=1e-5)\n",
    "#train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 881,
     "referenced_widgets": [
      "726995c90ca1424c90dcc3ba2a1203f1",
      "7d72674cd0e941a2a58c9477981bd8ff",
      "bd8883390af54afd8a757af54093bef6",
      "b487bf573b7f41398447c5f8f63c1e04",
      "52a0477faf594d9eb4be099faf241034",
      "39250dc036644093a6e93f7f33199ce7",
      "a1d81cedb665463096f3160000c131e5",
      "7a6c71ef3b7a4308b47198739df410e1",
      "f45a29b35b4a44e5bf2a635d7525659c",
      "893d3ddd1faf4261864154222d665926",
      "475a67da89c84bf59da6757d417c2bfb",
      "d777dd668d0f4dc196db2632ac657558",
      "e75e6f02eda7438d869a3c3e0efa2bec",
      "5238716048584ce88492fda4f2188bde",
      "90ec055800934145ada75fec0189d7a8",
      "1a8dc4b92ea249a397303abfd6291294",
      "bdafcfe4bdd04aca9f04479900008253",
      "fa0791a8b0894c91910f7022429778b7",
      "6857abb746dc4b81b0b60b5e5bd1c30e",
      "7f4e77863c944fdd8d9e014109740698",
      "8750469d909844c8a46b175b280c3cef",
      "e9cc6c95f26e4fe98dff51c8a7ac7389",
      "a9b89d30fb9742fd9db5d87ad6f7f77d",
      "bdba4111e4b74175852037315d39696d"
     ]
    },
    "id": "sPBvwvtz3Cbw",
    "outputId": "3fdfc131-fb58-4c86-f374-89b9a82e8010",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a97b73030464d61bb8e35fcccb141eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/12944 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\build\\project\\programmers\\4_party_project\\env\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2149: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch Loss: 67.12117129564285 Accuracy: tensor(0.6000, device='cuda:0')\n",
      "Batch Loss: 117.15007968246937 Accuracy: tensor(0.7025, device='cuda:0')\n",
      "Batch Loss: 158.89970202744007 Accuracy: tensor(0.7450, device='cuda:0')\n",
      "Batch Loss: 196.61772364377975 Accuracy: tensor(0.7709, device='cuda:0')\n",
      "Batch Loss: 236.1447340324521 Accuracy: tensor(0.7835, device='cuda:0')\n",
      "Batch Loss: 276.7157583460212 Accuracy: tensor(0.7912, device='cuda:0')\n",
      "Batch Loss: 313.0681369677186 Accuracy: tensor(0.7993, device='cuda:0')\n",
      "Batch Loss: 351.6782390102744 Accuracy: tensor(0.8034, device='cuda:0')\n",
      "Batch Loss: 386.4174063280225 Accuracy: tensor(0.8092, device='cuda:0')\n",
      "Batch Loss: 416.29794661700726 Accuracy: tensor(0.8163, device='cuda:0')\n",
      "Batch Loss: 452.4189935140312 Accuracy: tensor(0.8193, device='cuda:0')\n",
      "Batch Loss: 484.3220498301089 Accuracy: tensor(0.8235, device='cuda:0')\n",
      "Batch Loss: 519.0525521375239 Accuracy: tensor(0.8256, device='cuda:0')\n",
      "Batch Loss: 552.440062534064 Accuracy: tensor(0.8284, device='cuda:0')\n",
      "Batch Loss: 582.3767743483186 Accuracy: tensor(0.8322, device='cuda:0')\n",
      "Batch Loss: 612.7458006963134 Accuracy: tensor(0.8358, device='cuda:0')\n",
      "Batch Loss: 645.2152183875442 Accuracy: tensor(0.8363, device='cuda:0')\n",
      "Batch Loss: 675.5521181151271 Accuracy: tensor(0.8385, device='cuda:0')\n",
      "Batch Loss: 704.4696655645967 Accuracy: tensor(0.8401, device='cuda:0')\n",
      "Batch Loss: 733.9065057225525 Accuracy: tensor(0.8421, device='cuda:0')\n",
      "Batch Loss: 767.5907228589058 Accuracy: tensor(0.8430, device='cuda:0')\n",
      "Batch Loss: 795.5809600837529 Accuracy: tensor(0.8444, device='cuda:0')\n",
      "Batch Loss: 827.3958660960197 Accuracy: tensor(0.8456, device='cuda:0')\n",
      "Batch Loss: 854.7560838535428 Accuracy: tensor(0.8476, device='cuda:0')\n",
      "Batch Loss: 884.8820448890328 Accuracy: tensor(0.8485, device='cuda:0')\n",
      "Batch Loss: 915.9275108426809 Accuracy: tensor(0.8492, device='cuda:0')\n",
      "Batch Loss: 949.3718637637794 Accuracy: tensor(0.8497, device='cuda:0')\n",
      "Batch Loss: 976.2649087868631 Accuracy: tensor(0.8513, device='cuda:0')\n",
      "Batch Loss: 1009.4846646264195 Accuracy: tensor(0.8516, device='cuda:0')\n",
      "Batch Loss: 1036.6022652611136 Accuracy: tensor(0.8528, device='cuda:0')\n",
      "Batch Loss: 1065.848731085658 Accuracy: tensor(0.8533, device='cuda:0')\n",
      "Batch Loss: 1096.4242362305522 Accuracy: tensor(0.8539, device='cuda:0')\n",
      "Batch Loss: 1122.6513620913029 Accuracy: tensor(0.8552, device='cuda:0')\n",
      "Batch Loss: 1150.7569301016629 Accuracy: tensor(0.8560, device='cuda:0')\n",
      "Batch Loss: 1180.6847592033446 Accuracy: tensor(0.8565, device='cuda:0')\n",
      "Batch Loss: 1210.930723041296 Accuracy: tensor(0.8569, device='cuda:0')\n",
      "Batch Loss: 1241.3803499490023 Accuracy: tensor(0.8575, device='cuda:0')\n",
      "Batch Loss: 1273.1399306636304 Accuracy: tensor(0.8581, device='cuda:0')\n",
      "Batch Loss: 1305.330024247989 Accuracy: tensor(0.8581, device='cuda:0')\n",
      "Batch Loss: 1336.4541970565915 Accuracy: tensor(0.8584, device='cuda:0')\n",
      "Batch Loss: 1363.515887958929 Accuracy: tensor(0.8590, device='cuda:0')\n",
      "Batch Loss: 1391.7343964334577 Accuracy: tensor(0.8596, device='cuda:0')\n",
      "Batch Loss: 1421.2157674003392 Accuracy: tensor(0.8602, device='cuda:0')\n",
      "Batch Loss: 1450.1987021695822 Accuracy: tensor(0.8609, device='cuda:0')\n",
      "Batch Loss: 1482.5590791832656 Accuracy: tensor(0.8609, device='cuda:0')\n",
      "Batch Loss: 1512.2911803964525 Accuracy: tensor(0.8614, device='cuda:0')\n",
      "Batch Loss: 1542.9627217482775 Accuracy: tensor(0.8617, device='cuda:0')\n",
      "Batch Loss: 1571.908237138763 Accuracy: tensor(0.8621, device='cuda:0')\n",
      "Batch Loss: 1603.8349440936 Accuracy: tensor(0.8622, device='cuda:0')\n",
      "Batch Loss: 1633.0828502047807 Accuracy: tensor(0.8625, device='cuda:0')\n",
      "Batch Loss: 1663.0025067608804 Accuracy: tensor(0.8628, device='cuda:0')\n",
      "Batch Loss: 1691.241376021877 Accuracy: tensor(0.8632, device='cuda:0')\n",
      "Batch Loss: 1721.368694709614 Accuracy: tensor(0.8634, device='cuda:0')\n",
      "Batch Loss: 1763.149873489514 Accuracy: tensor(0.8619, device='cuda:0')\n",
      "Batch Loss: 1830.544018799439 Accuracy: tensor(0.8562, device='cuda:0')\n",
      "Batch Loss: 1897.0250585917383 Accuracy: tensor(0.8506, device='cuda:0')\n",
      "Batch Loss: 1966.8363695386797 Accuracy: tensor(0.8447, device='cuda:0')\n",
      "Batch Loss: 2036.984073067084 Accuracy: tensor(0.8382, device='cuda:0')\n",
      "Batch Loss: 2106.539502048865 Accuracy: tensor(0.8323, device='cuda:0')\n",
      "Batch Loss: 2175.815368557349 Accuracy: tensor(0.8271, device='cuda:0')\n",
      "Batch Loss: 2245.6636331323534 Accuracy: tensor(0.8219, device='cuda:0')\n",
      "Batch Loss: 2315.3794113043696 Accuracy: tensor(0.8164, device='cuda:0')\n",
      "Batch Loss: 2384.7290154937655 Accuracy: tensor(0.8115, device='cuda:0')\n",
      "Batch Loss: 2454.294672453776 Accuracy: tensor(0.8065, device='cuda:0')\n",
      "Batch Loss: 2523.4765669945627 Accuracy: tensor(0.8022, device='cuda:0')\n",
      "Batch Loss: 2592.984198534861 Accuracy: tensor(0.7975, device='cuda:0')\n",
      "Batch Loss: 2662.749497557059 Accuracy: tensor(0.7928, device='cuda:0')\n",
      "Batch Loss: 2731.9243561867625 Accuracy: tensor(0.7889, device='cuda:0')\n",
      "Batch Loss: 2801.688531482592 Accuracy: tensor(0.7845, device='cuda:0')\n",
      "Batch Loss: 2871.3417572621256 Accuracy: tensor(0.7799, device='cuda:0')\n",
      "Batch Loss: 2940.694391930476 Accuracy: tensor(0.7761, device='cuda:0')\n",
      "Batch Loss: 3010.1605069879442 Accuracy: tensor(0.7723, device='cuda:0')\n",
      "Batch Loss: 3079.887142682448 Accuracy: tensor(0.7686, device='cuda:0')\n",
      "Batch Loss: 3149.279796684161 Accuracy: tensor(0.7654, device='cuda:0')\n",
      "Batch Loss: 3218.7293717507273 Accuracy: tensor(0.7616, device='cuda:0')\n",
      "Batch Loss: 3288.2884546164423 Accuracy: tensor(0.7583, device='cuda:0')\n",
      "Batch Loss: 3357.9190663341433 Accuracy: tensor(0.7550, device='cuda:0')\n",
      "Batch Loss: 3427.196584666148 Accuracy: tensor(0.7520, device='cuda:0')\n",
      "Batch Loss: 3496.7083453778177 Accuracy: tensor(0.7486, device='cuda:0')\n",
      "Batch Loss: 3566.1461464408785 Accuracy: tensor(0.7454, device='cuda:0')\n",
      "Batch Loss: 3635.8874144200236 Accuracy: tensor(0.7424, device='cuda:0')\n",
      "Batch Loss: 3705.4283522013575 Accuracy: tensor(0.7396, device='cuda:0')\n",
      "Batch Loss: 3775.1427958849818 Accuracy: tensor(0.7367, device='cuda:0')\n",
      "Batch Loss: 3844.4555665496737 Accuracy: tensor(0.7341, device='cuda:0')\n",
      "Batch Loss: 3914.0627869013697 Accuracy: tensor(0.7310, device='cuda:0')\n",
      "Batch Loss: 3983.6373510006815 Accuracy: tensor(0.7282, device='cuda:0')\n",
      "Batch Loss: 4052.9843786600977 Accuracy: tensor(0.7258, device='cuda:0')\n",
      "Batch Loss: 4122.540405714884 Accuracy: tensor(0.7229, device='cuda:0')\n",
      "Batch Loss: 4191.890436435118 Accuracy: tensor(0.7205, device='cuda:0')\n",
      "Batch Loss: 4261.305132472888 Accuracy: tensor(0.7184, device='cuda:0')\n",
      "Batch Loss: 4330.998821342364 Accuracy: tensor(0.7157, device='cuda:0')\n",
      "Batch Loss: 4400.346012914553 Accuracy: tensor(0.7135, device='cuda:0')\n",
      "Batch Loss: 4469.720274293795 Accuracy: tensor(0.7114, device='cuda:0')\n",
      "Batch Loss: 4539.100123727694 Accuracy: tensor(0.7093, device='cuda:0')\n",
      "Batch Loss: 4608.846667552367 Accuracy: tensor(0.7072, device='cuda:0')\n",
      "Batch Loss: 4678.206315303221 Accuracy: tensor(0.7052, device='cuda:0')\n",
      "Batch Loss: 4747.624026978388 Accuracy: tensor(0.7030, device='cuda:0')\n",
      "Batch Loss: 4816.76683136262 Accuracy: tensor(0.7013, device='cuda:0')\n",
      "Batch Loss: 4886.4609663728625 Accuracy: tensor(0.6991, device='cuda:0')\n",
      "Batch Loss: 4956.029215538874 Accuracy: tensor(0.6970, device='cuda:0')\n",
      "Batch Loss: 5025.441631222144 Accuracy: tensor(0.6950, device='cuda:0')\n",
      "Batch Loss: 5094.731559777632 Accuracy: tensor(0.6932, device='cuda:0')\n",
      "Batch Loss: 5164.265147352591 Accuracy: tensor(0.6912, device='cuda:0')\n",
      "Batch Loss: 5233.62086799182 Accuracy: tensor(0.6896, device='cuda:0')\n",
      "Batch Loss: 5302.979054057971 Accuracy: tensor(0.6877, device='cuda:0')\n",
      "Batch Loss: 5372.455644750968 Accuracy: tensor(0.6861, device='cuda:0')\n",
      "Batch Loss: 5441.7750844005495 Accuracy: tensor(0.6846, device='cuda:0')\n",
      "Batch Loss: 5510.939117277041 Accuracy: tensor(0.6830, device='cuda:0')\n",
      "Batch Loss: 5580.2790542487055 Accuracy: tensor(0.6815, device='cuda:0')\n",
      "Batch Loss: 5649.451371097937 Accuracy: tensor(0.6800, device='cuda:0')\n",
      "Batch Loss: 5718.842903340235 Accuracy: tensor(0.6784, device='cuda:0')\n",
      "Batch Loss: 5788.023694181815 Accuracy: tensor(0.6770, device='cuda:0')\n",
      "Batch Loss: 5857.421578729525 Accuracy: tensor(0.6757, device='cuda:0')\n",
      "Batch Loss: 5926.848695600405 Accuracy: tensor(0.6742, device='cuda:0')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch Loss: 5996.049318337813 Accuracy: tensor(0.6729, device='cuda:0')\n",
      "Batch Loss: 6065.499453449622 Accuracy: tensor(0.6716, device='cuda:0')\n",
      "Batch Loss: 6134.885305786505 Accuracy: tensor(0.6701, device='cuda:0')\n",
      "Batch Loss: 6204.286832058802 Accuracy: tensor(0.6687, device='cuda:0')\n",
      "Batch Loss: 6273.840327048674 Accuracy: tensor(0.6672, device='cuda:0')\n",
      "Batch Loss: 6343.508880281821 Accuracy: tensor(0.6656, device='cuda:0')\n",
      "Batch Loss: 6413.13424625434 Accuracy: tensor(0.6641, device='cuda:0')\n",
      "Batch Loss: 6482.46738162078 Accuracy: tensor(0.6628, device='cuda:0')\n",
      "Batch Loss: 6551.94710620679 Accuracy: tensor(0.6615, device='cuda:0')\n",
      "Batch Loss: 6621.31428267993 Accuracy: tensor(0.6604, device='cuda:0')\n",
      "Batch Loss: 6690.639566982165 Accuracy: tensor(0.6593, device='cuda:0')\n",
      "Batch Loss: 6759.999844217673 Accuracy: tensor(0.6580, device='cuda:0')\n",
      "Batch Loss: 6829.232450867072 Accuracy: tensor(0.6569, device='cuda:0')\n",
      "Batch Loss: 6898.6151551846415 Accuracy: tensor(0.6556, device='cuda:0')\n",
      "Batch Loss: 6968.20286729373 Accuracy: tensor(0.6543, device='cuda:0')\n",
      "Train Loss: 6998.521679842845 Accuracy: tensor(0.6539, device='cuda:0')\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "28e391b935ec471b8bf9a431e80549f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/12944 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch Loss: 69.29526257514954 Accuracy: tensor(0.5275, device='cuda:0')\n",
      "Batch Loss: 138.69540005922318 Accuracy: tensor(0.5150, device='cuda:0')\n",
      "Batch Loss: 207.6104902625084 Accuracy: tensor(0.5200, device='cuda:0')\n",
      "Batch Loss: 277.4976540207863 Accuracy: tensor(0.5191, device='cuda:0')\n",
      "Batch Loss: 346.9130715727806 Accuracy: tensor(0.5175, device='cuda:0')\n",
      "Batch Loss: 416.4773020148277 Accuracy: tensor(0.5117, device='cuda:0')\n",
      "Batch Loss: 485.97574883699417 Accuracy: tensor(0.5063, device='cuda:0')\n",
      "Batch Loss: 555.2773128151894 Accuracy: tensor(0.5069, device='cuda:0')\n",
      "Batch Loss: 624.7878538966179 Accuracy: tensor(0.5067, device='cuda:0')\n",
      "Batch Loss: 694.1517677307129 Accuracy: tensor(0.5063, device='cuda:0')\n",
      "Batch Loss: 763.4847666621208 Accuracy: tensor(0.5073, device='cuda:0')\n",
      "Batch Loss: 832.589603304863 Accuracy: tensor(0.5092, device='cuda:0')\n",
      "Batch Loss: 902.1076142787933 Accuracy: tensor(0.5091, device='cuda:0')\n",
      "Batch Loss: 971.4994589090347 Accuracy: tensor(0.5080, device='cuda:0')\n",
      "Batch Loss: 1040.7582592368126 Accuracy: tensor(0.5080, device='cuda:0')\n",
      "Batch Loss: 1110.153592467308 Accuracy: tensor(0.5074, device='cuda:0')\n",
      "Batch Loss: 1179.411998450756 Accuracy: tensor(0.5085, device='cuda:0')\n",
      "Batch Loss: 1248.9669821858406 Accuracy: tensor(0.5068, device='cuda:0')\n",
      "Batch Loss: 1318.2651527523994 Accuracy: tensor(0.5062, device='cuda:0')\n",
      "Batch Loss: 1387.7955330014229 Accuracy: tensor(0.5059, device='cuda:0')\n",
      "Batch Loss: 1457.1265150904655 Accuracy: tensor(0.5053, device='cuda:0')\n",
      "Batch Loss: 1526.430918097496 Accuracy: tensor(0.5056, device='cuda:0')\n",
      "Batch Loss: 1595.8699927330017 Accuracy: tensor(0.5052, device='cuda:0')\n",
      "Batch Loss: 1665.310311794281 Accuracy: tensor(0.5054, device='cuda:0')\n",
      "Batch Loss: 1734.8131439089775 Accuracy: tensor(0.5046, device='cuda:0')\n",
      "Batch Loss: 1804.192431986332 Accuracy: tensor(0.5046, device='cuda:0')\n",
      "Batch Loss: 1873.5248140096664 Accuracy: tensor(0.5047, device='cuda:0')\n",
      "Batch Loss: 1942.5831357836723 Accuracy: tensor(0.5052, device='cuda:0')\n",
      "Batch Loss: 2011.9776715636253 Accuracy: tensor(0.5052, device='cuda:0')\n",
      "Batch Loss: 2080.8252645730972 Accuracy: tensor(0.5064, device='cuda:0')\n",
      "Batch Loss: 2150.4011219739914 Accuracy: tensor(0.5059, device='cuda:0')\n",
      "Batch Loss: 2219.286779642105 Accuracy: tensor(0.5070, device='cuda:0')\n",
      "Batch Loss: 2288.7193026542664 Accuracy: tensor(0.5068, device='cuda:0')\n",
      "Batch Loss: 2357.96080493927 Accuracy: tensor(0.5077, device='cuda:0')\n",
      "Batch Loss: 2427.3822180628777 Accuracy: tensor(0.5079, device='cuda:0')\n",
      "Batch Loss: 2497.1044113636017 Accuracy: tensor(0.5073, device='cuda:0')\n",
      "Batch Loss: 2566.3306999206543 Accuracy: tensor(0.5073, device='cuda:0')\n",
      "Batch Loss: 2635.764270424843 Accuracy: tensor(0.5073, device='cuda:0')\n",
      "Batch Loss: 2705.041144132614 Accuracy: tensor(0.5073, device='cuda:0')\n",
      "Batch Loss: 2774.354356765747 Accuracy: tensor(0.5072, device='cuda:0')\n",
      "Batch Loss: 2843.9066719412804 Accuracy: tensor(0.5066, device='cuda:0')\n",
      "Batch Loss: 2913.2220135331154 Accuracy: tensor(0.5068, device='cuda:0')\n",
      "Batch Loss: 2982.6168370246887 Accuracy: tensor(0.5070, device='cuda:0')\n",
      "Batch Loss: 3052.159721970558 Accuracy: tensor(0.5069, device='cuda:0')\n",
      "Batch Loss: 3121.4064612984657 Accuracy: tensor(0.5073, device='cuda:0')\n",
      "Batch Loss: 3190.84556722641 Accuracy: tensor(0.5074, device='cuda:0')\n",
      "Batch Loss: 3260.1286643743515 Accuracy: tensor(0.5076, device='cuda:0')\n",
      "Batch Loss: 3329.5579102039337 Accuracy: tensor(0.5073, device='cuda:0')\n",
      "Batch Loss: 3399.1116836071014 Accuracy: tensor(0.5074, device='cuda:0')\n",
      "Batch Loss: 3468.569596350193 Accuracy: tensor(0.5069, device='cuda:0')\n",
      "Batch Loss: 3537.614471256733 Accuracy: tensor(0.5075, device='cuda:0')\n",
      "Batch Loss: 3607.019814670086 Accuracy: tensor(0.5076, device='cuda:0')\n",
      "Batch Loss: 3676.413765132427 Accuracy: tensor(0.5074, device='cuda:0')\n",
      "Batch Loss: 3745.8668951392174 Accuracy: tensor(0.5072, device='cuda:0')\n",
      "Batch Loss: 3815.2478606700897 Accuracy: tensor(0.5068, device='cuda:0')\n",
      "Batch Loss: 3884.4627768993378 Accuracy: tensor(0.5069, device='cuda:0')\n",
      "Batch Loss: 3953.7639610767365 Accuracy: tensor(0.5071, device='cuda:0')\n",
      "Batch Loss: 4023.191476583481 Accuracy: tensor(0.5070, device='cuda:0')\n",
      "Batch Loss: 4092.504155278206 Accuracy: tensor(0.5069, device='cuda:0')\n",
      "Batch Loss: 4161.735321640968 Accuracy: tensor(0.5069, device='cuda:0')\n",
      "Batch Loss: 4231.026933789253 Accuracy: tensor(0.5071, device='cuda:0')\n",
      "Batch Loss: 4300.299924254417 Accuracy: tensor(0.5071, device='cuda:0')\n",
      "Batch Loss: 4369.516454458237 Accuracy: tensor(0.5075, device='cuda:0')\n",
      "Batch Loss: 4438.963037371635 Accuracy: tensor(0.5075, device='cuda:0')\n",
      "Batch Loss: 4508.402172982693 Accuracy: tensor(0.5075, device='cuda:0')\n",
      "Batch Loss: 4577.64157897234 Accuracy: tensor(0.5073, device='cuda:0')\n",
      "Batch Loss: 4646.954295217991 Accuracy: tensor(0.5072, device='cuda:0')\n",
      "Batch Loss: 4716.305335819721 Accuracy: tensor(0.5073, device='cuda:0')\n",
      "Batch Loss: 4785.365989685059 Accuracy: tensor(0.5077, device='cuda:0')\n",
      "Batch Loss: 4854.301081776619 Accuracy: tensor(0.5083, device='cuda:0')\n",
      "Batch Loss: 4923.796480298042 Accuracy: tensor(0.5082, device='cuda:0')\n",
      "Batch Loss: 4993.351306438446 Accuracy: tensor(0.5079, device='cuda:0')\n",
      "Batch Loss: 5062.709263205528 Accuracy: tensor(0.5080, device='cuda:0')\n",
      "Batch Loss: 5131.788810133934 Accuracy: tensor(0.5084, device='cuda:0')\n",
      "Batch Loss: 5200.98719394207 Accuracy: tensor(0.5085, device='cuda:0')\n",
      "Batch Loss: 5270.774118483067 Accuracy: tensor(0.5080, device='cuda:0')\n",
      "Batch Loss: 5340.15320789814 Accuracy: tensor(0.5077, device='cuda:0')\n",
      "Batch Loss: 5409.38405585289 Accuracy: tensor(0.5079, device='cuda:0')\n",
      "Batch Loss: 5478.5841832757 Accuracy: tensor(0.5081, device='cuda:0')\n",
      "Batch Loss: 5548.021040201187 Accuracy: tensor(0.5080, device='cuda:0')\n",
      "Batch Loss: 5617.490748643875 Accuracy: tensor(0.5077, device='cuda:0')\n",
      "Batch Loss: 5686.750441491604 Accuracy: tensor(0.5078, device='cuda:0')\n",
      "Batch Loss: 5755.924585044384 Accuracy: tensor(0.5080, device='cuda:0')\n",
      "Batch Loss: 5825.301759123802 Accuracy: tensor(0.5082, device='cuda:0')\n",
      "Batch Loss: 5894.751021325588 Accuracy: tensor(0.5081, device='cuda:0')\n",
      "Batch Loss: 5963.991003096104 Accuracy: tensor(0.5082, device='cuda:0')\n",
      "Batch Loss: 6033.537047863007 Accuracy: tensor(0.5079, device='cuda:0')\n",
      "Batch Loss: 6102.820930480957 Accuracy: tensor(0.5079, device='cuda:0')\n",
      "Batch Loss: 6172.152904987335 Accuracy: tensor(0.5079, device='cuda:0')\n",
      "Batch Loss: 6241.409725964069 Accuracy: tensor(0.5078, device='cuda:0')\n",
      "Batch Loss: 6310.671601235867 Accuracy: tensor(0.5079, device='cuda:0')\n",
      "Batch Loss: 6380.184369981289 Accuracy: tensor(0.5077, device='cuda:0')\n",
      "Batch Loss: 6449.504746079445 Accuracy: tensor(0.5076, device='cuda:0')\n",
      "Batch Loss: 6518.762959063053 Accuracy: tensor(0.5076, device='cuda:0')\n",
      "Batch Loss: 6588.219430267811 Accuracy: tensor(0.5074, device='cuda:0')\n",
      "Batch Loss: 6657.515760183334 Accuracy: tensor(0.5074, device='cuda:0')\n",
      "Batch Loss: 6726.85994052887 Accuracy: tensor(0.5073, device='cuda:0')\n",
      "Batch Loss: 6795.849503397942 Accuracy: tensor(0.5077, device='cuda:0')\n",
      "Batch Loss: 6865.377910017967 Accuracy: tensor(0.5077, device='cuda:0')\n",
      "Batch Loss: 6934.70705640316 Accuracy: tensor(0.5076, device='cuda:0')\n",
      "Batch Loss: 7003.9503272771835 Accuracy: tensor(0.5076, device='cuda:0')\n",
      "Batch Loss: 7073.4049988389015 Accuracy: tensor(0.5075, device='cuda:0')\n",
      "Batch Loss: 7142.832071065903 Accuracy: tensor(0.5073, device='cuda:0')\n",
      "Batch Loss: 7212.127944052219 Accuracy: tensor(0.5073, device='cuda:0')\n",
      "Batch Loss: 7281.602188706398 Accuracy: tensor(0.5073, device='cuda:0')\n",
      "Batch Loss: 7351.099379003048 Accuracy: tensor(0.5072, device='cuda:0')\n",
      "Batch Loss: 7420.506592869759 Accuracy: tensor(0.5071, device='cuda:0')\n",
      "Batch Loss: 7489.789537250996 Accuracy: tensor(0.5071, device='cuda:0')\n",
      "Batch Loss: 7559.118222773075 Accuracy: tensor(0.5073, device='cuda:0')\n",
      "Batch Loss: 7628.621221482754 Accuracy: tensor(0.5071, device='cuda:0')\n",
      "Batch Loss: 7697.957561075687 Accuracy: tensor(0.5072, device='cuda:0')\n",
      "Batch Loss: 7767.3660071492195 Accuracy: tensor(0.5071, device='cuda:0')\n",
      "Batch Loss: 7836.658752977848 Accuracy: tensor(0.5071, device='cuda:0')\n",
      "Batch Loss: 7906.295819044113 Accuracy: tensor(0.5070, device='cuda:0')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch Loss: 7975.448747217655 Accuracy: tensor(0.5072, device='cuda:0')\n",
      "Batch Loss: 8044.536619544029 Accuracy: tensor(0.5076, device='cuda:0')\n",
      "Batch Loss: 8113.639064848423 Accuracy: tensor(0.5078, device='cuda:0')\n",
      "Batch Loss: 8182.757758796215 Accuracy: tensor(0.5080, device='cuda:0')\n",
      "Batch Loss: 8252.099557816982 Accuracy: tensor(0.5080, device='cuda:0')\n",
      "Batch Loss: 8321.618214726448 Accuracy: tensor(0.5080, device='cuda:0')\n",
      "Batch Loss: 8390.643522918224 Accuracy: tensor(0.5080, device='cuda:0')\n",
      "Batch Loss: 8459.985080778599 Accuracy: tensor(0.5080, device='cuda:0')\n",
      "Batch Loss: 8529.403462588787 Accuracy: tensor(0.5078, device='cuda:0')\n",
      "Batch Loss: 8598.835371851921 Accuracy: tensor(0.5077, device='cuda:0')\n",
      "Batch Loss: 8668.373564362526 Accuracy: tensor(0.5076, device='cuda:0')\n",
      "Batch Loss: 8737.99673807621 Accuracy: tensor(0.5075, device='cuda:0')\n",
      "Batch Loss: 8807.096265673637 Accuracy: tensor(0.5076, device='cuda:0')\n",
      "Batch Loss: 8876.249681472778 Accuracy: tensor(0.5077, device='cuda:0')\n",
      "Batch Loss: 8945.39315623045 Accuracy: tensor(0.5078, device='cuda:0')\n",
      "Train Loss: 8976.074173092842 Accuracy: tensor(0.5077, device='cuda:0')\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6b86b9338b2442884727ab05adaeb0b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/12944 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch Loss: 69.21166932582855 Accuracy: tensor(0.5225, device='cuda:0')\n",
      "Batch Loss: 138.87648230791092 Accuracy: tensor(0.4912, device='cuda:0')\n",
      "Batch Loss: 208.24039036035538 Accuracy: tensor(0.4954, device='cuda:0')\n",
      "Batch Loss: 277.6249935030937 Accuracy: tensor(0.4984, device='cuda:0')\n",
      "Batch Loss: 346.44950371980667 Accuracy: tensor(0.5107, device='cuda:0')\n",
      "Batch Loss: 415.64442068338394 Accuracy: tensor(0.5119, device='cuda:0')\n",
      "Batch Loss: 485.2058104276657 Accuracy: tensor(0.5107, device='cuda:0')\n",
      "Batch Loss: 553.4209378957748 Accuracy: tensor(0.5109, device='cuda:0')\n",
      "Batch Loss: 622.7827941775322 Accuracy: tensor(0.5082, device='cuda:0')\n",
      "Batch Loss: 692.114251434803 Accuracy: tensor(0.5060, device='cuda:0')\n",
      "Batch Loss: 760.0778877735138 Accuracy: tensor(0.5075, device='cuda:0')\n",
      "Batch Loss: 825.4144473075867 Accuracy: tensor(0.5158, device='cuda:0')\n",
      "Batch Loss: 891.9550866484642 Accuracy: tensor(0.5202, device='cuda:0')\n",
      "Batch Loss: 959.8562199473381 Accuracy: tensor(0.5209, device='cuda:0')\n",
      "Batch Loss: 1029.4805663228035 Accuracy: tensor(0.5187, device='cuda:0')\n",
      "Batch Loss: 1098.8483207821846 Accuracy: tensor(0.5181, device='cuda:0')\n",
      "Batch Loss: 1168.1096591949463 Accuracy: tensor(0.5179, device='cuda:0')\n",
      "Batch Loss: 1237.508216559887 Accuracy: tensor(0.5173, device='cuda:0')\n",
      "Batch Loss: 1306.7262563109398 Accuracy: tensor(0.5180, device='cuda:0')\n",
      "Batch Loss: 1375.8279907107353 Accuracy: tensor(0.5186, device='cuda:0')\n",
      "Batch Loss: 1445.0713860988617 Accuracy: tensor(0.5180, device='cuda:0')\n",
      "Batch Loss: 1514.4492950439453 Accuracy: tensor(0.5176, device='cuda:0')\n",
      "Batch Loss: 1583.8466356992722 Accuracy: tensor(0.5173, device='cuda:0')\n",
      "Batch Loss: 1653.028192281723 Accuracy: tensor(0.5163, device='cuda:0')\n",
      "Batch Loss: 1722.2002550959587 Accuracy: tensor(0.5167, device='cuda:0')\n",
      "Batch Loss: 1791.3311638832092 Accuracy: tensor(0.5162, device='cuda:0')\n",
      "Batch Loss: 1859.9670785665512 Accuracy: tensor(0.5160, device='cuda:0')\n",
      "Batch Loss: 1925.0329867601395 Accuracy: tensor(0.5173, device='cuda:0')\n",
      "Batch Loss: 1979.8878884464502 Accuracy: tensor(0.5251, device='cuda:0')\n",
      "Batch Loss: 2022.6713169515133 Accuracy: tensor(0.5349, device='cuda:0')\n",
      "Batch Loss: 2063.356039777398 Accuracy: tensor(0.5443, device='cuda:0')\n",
      "Batch Loss: 2102.303773716092 Accuracy: tensor(0.5538, device='cuda:0')\n",
      "Batch Loss: 2138.8363910466433 Accuracy: tensor(0.5627, device='cuda:0')\n",
      "Batch Loss: 2176.5230445005 Accuracy: tensor(0.5709, device='cuda:0')\n",
      "Batch Loss: 2212.091356113553 Accuracy: tensor(0.5789, device='cuda:0')\n",
      "Batch Loss: 2244.6427680775523 Accuracy: tensor(0.5866, device='cuda:0')\n",
      "Batch Loss: 2278.418055202812 Accuracy: tensor(0.5939, device='cuda:0')\n",
      "Batch Loss: 2312.409895412624 Accuracy: tensor(0.6008, device='cuda:0')\n",
      "Batch Loss: 2347.696731336415 Accuracy: tensor(0.6069, device='cuda:0')\n",
      "Batch Loss: 2375.710921678692 Accuracy: tensor(0.6140, device='cuda:0')\n",
      "Batch Loss: 2411.3394963927567 Accuracy: tensor(0.6196, device='cuda:0')\n",
      "Batch Loss: 2443.3175809159875 Accuracy: tensor(0.6255, device='cuda:0')\n",
      "Batch Loss: 2473.391641676426 Accuracy: tensor(0.6314, device='cuda:0')\n",
      "Batch Loss: 2505.1939735226333 Accuracy: tensor(0.6363, device='cuda:0')\n",
      "Batch Loss: 2536.22476888448 Accuracy: tensor(0.6414, device='cuda:0')\n",
      "Batch Loss: 2567.604252617806 Accuracy: tensor(0.6465, device='cuda:0')\n",
      "Batch Loss: 2596.157182801515 Accuracy: tensor(0.6513, device='cuda:0')\n",
      "Batch Loss: 2629.1521546430886 Accuracy: tensor(0.6556, device='cuda:0')\n",
      "Batch Loss: 2658.7502107322216 Accuracy: tensor(0.6602, device='cuda:0')\n",
      "Batch Loss: 2688.1347097866237 Accuracy: tensor(0.6647, device='cuda:0')\n",
      "Batch Loss: 2719.085794562474 Accuracy: tensor(0.6686, device='cuda:0')\n",
      "Batch Loss: 2744.831200422719 Accuracy: tensor(0.6729, device='cuda:0')\n",
      "Batch Loss: 2772.30308483541 Accuracy: tensor(0.6769, device='cuda:0')\n",
      "Batch Loss: 2803.868142064661 Accuracy: tensor(0.6804, device='cuda:0')\n",
      "Batch Loss: 2830.2428950071335 Accuracy: tensor(0.6842, device='cuda:0')\n",
      "Batch Loss: 2857.8279820475727 Accuracy: tensor(0.6878, device='cuda:0')\n",
      "Batch Loss: 2883.02813164331 Accuracy: tensor(0.6915, device='cuda:0')\n",
      "Batch Loss: 2911.3884362038225 Accuracy: tensor(0.6948, device='cuda:0')\n",
      "Batch Loss: 2936.49770979397 Accuracy: tensor(0.6983, device='cuda:0')\n",
      "Batch Loss: 2963.6407535281032 Accuracy: tensor(0.7014, device='cuda:0')\n",
      "Batch Loss: 2989.164296206087 Accuracy: tensor(0.7045, device='cuda:0')\n",
      "Batch Loss: 3016.594221804291 Accuracy: tensor(0.7075, device='cuda:0')\n",
      "Batch Loss: 3044.2824795767665 Accuracy: tensor(0.7104, device='cuda:0')\n",
      "Batch Loss: 3071.5615787170827 Accuracy: tensor(0.7133, device='cuda:0')\n",
      "Batch Loss: 3094.2230714727193 Accuracy: tensor(0.7163, device='cuda:0')\n",
      "Batch Loss: 3120.42404429242 Accuracy: tensor(0.7189, device='cuda:0')\n",
      "Batch Loss: 3147.5040606856346 Accuracy: tensor(0.7215, device='cuda:0')\n",
      "Batch Loss: 3176.9123627133667 Accuracy: tensor(0.7238, device='cuda:0')\n",
      "Batch Loss: 3201.6144127268344 Accuracy: tensor(0.7264, device='cuda:0')\n",
      "Batch Loss: 3227.7516915593296 Accuracy: tensor(0.7288, device='cuda:0')\n",
      "Batch Loss: 3257.2214835099876 Accuracy: tensor(0.7309, device='cuda:0')\n",
      "Batch Loss: 3281.25499782525 Accuracy: tensor(0.7334, device='cuda:0')\n",
      "Batch Loss: 3306.4092099890113 Accuracy: tensor(0.7358, device='cuda:0')\n",
      "Batch Loss: 3328.3562087435275 Accuracy: tensor(0.7381, device='cuda:0')\n",
      "Batch Loss: 3354.254038741812 Accuracy: tensor(0.7402, device='cuda:0')\n",
      "Batch Loss: 3379.966454063542 Accuracy: tensor(0.7422, device='cuda:0')\n",
      "Batch Loss: 3403.0982103338465 Accuracy: tensor(0.7445, device='cuda:0')\n",
      "Batch Loss: 3428.4136455906555 Accuracy: tensor(0.7465, device='cuda:0')\n",
      "Batch Loss: 3453.7001695195213 Accuracy: tensor(0.7484, device='cuda:0')\n",
      "Batch Loss: 3476.7266430128366 Accuracy: tensor(0.7503, device='cuda:0')\n",
      "Batch Loss: 3503.3287165593356 Accuracy: tensor(0.7521, device='cuda:0')\n",
      "Batch Loss: 3534.4178944732994 Accuracy: tensor(0.7536, device='cuda:0')\n",
      "Batch Loss: 3563.5192358177155 Accuracy: tensor(0.7550, device='cuda:0')\n",
      "Batch Loss: 3594.8733332045376 Accuracy: tensor(0.7563, device='cuda:0')\n",
      "Batch Loss: 3618.8187866229564 Accuracy: tensor(0.7581, device='cuda:0')\n",
      "Batch Loss: 3646.8330331221223 Accuracy: tensor(0.7596, device='cuda:0')\n",
      "Batch Loss: 3672.1216201111674 Accuracy: tensor(0.7611, device='cuda:0')\n",
      "Batch Loss: 3697.2905527297407 Accuracy: tensor(0.7626, device='cuda:0')\n",
      "Batch Loss: 3721.4758279602975 Accuracy: tensor(0.7642, device='cuda:0')\n",
      "Batch Loss: 3748.2095977645367 Accuracy: tensor(0.7657, device='cuda:0')\n",
      "Batch Loss: 3772.3841405697167 Accuracy: tensor(0.7672, device='cuda:0')\n",
      "Batch Loss: 3799.901791255921 Accuracy: tensor(0.7685, device='cuda:0')\n",
      "Batch Loss: 3825.4703770857304 Accuracy: tensor(0.7698, device='cuda:0')\n",
      "Batch Loss: 3848.672099661082 Accuracy: tensor(0.7713, device='cuda:0')\n",
      "Batch Loss: 3872.5970897749066 Accuracy: tensor(0.7726, device='cuda:0')\n",
      "Batch Loss: 3898.9942626170814 Accuracy: tensor(0.7738, device='cuda:0')\n",
      "Batch Loss: 3925.9814463965595 Accuracy: tensor(0.7749, device='cuda:0')\n",
      "Batch Loss: 3950.8172704018652 Accuracy: tensor(0.7763, device='cuda:0')\n",
      "Batch Loss: 3975.8506623972207 Accuracy: tensor(0.7775, device='cuda:0')\n",
      "Batch Loss: 4002.3321824837476 Accuracy: tensor(0.7786, device='cuda:0')\n",
      "Batch Loss: 4028.8107878156006 Accuracy: tensor(0.7798, device='cuda:0')\n",
      "Batch Loss: 4051.6685414500535 Accuracy: tensor(0.7810, device='cuda:0')\n",
      "Batch Loss: 4077.133794879541 Accuracy: tensor(0.7823, device='cuda:0')\n",
      "Batch Loss: 4104.375750215724 Accuracy: tensor(0.7832, device='cuda:0')\n",
      "Batch Loss: 4130.789626520127 Accuracy: tensor(0.7843, device='cuda:0')\n",
      "Batch Loss: 4156.719711942598 Accuracy: tensor(0.7854, device='cuda:0')\n",
      "Batch Loss: 4186.3517234083265 Accuracy: tensor(0.7863, device='cuda:0')\n",
      "Batch Loss: 4217.408246491104 Accuracy: tensor(0.7871, device='cuda:0')\n",
      "Batch Loss: 4251.700746007264 Accuracy: tensor(0.7878, device='cuda:0')\n",
      "Batch Loss: 4282.490473579615 Accuracy: tensor(0.7886, device='cuda:0')\n",
      "Batch Loss: 4309.953972229734 Accuracy: tensor(0.7894, device='cuda:0')\n",
      "Batch Loss: 4338.087005248293 Accuracy: tensor(0.7901, device='cuda:0')\n",
      "Batch Loss: 4371.715993512422 Accuracy: tensor(0.7906, device='cuda:0')\n",
      "Batch Loss: 4401.988164305687 Accuracy: tensor(0.7914, device='cuda:0')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch Loss: 4434.660204865038 Accuracy: tensor(0.7921, device='cuda:0')\n",
      "Batch Loss: 4461.585526533425 Accuracy: tensor(0.7930, device='cuda:0')\n",
      "Batch Loss: 4491.385843940079 Accuracy: tensor(0.7937, device='cuda:0')\n",
      "Batch Loss: 4514.110393386334 Accuracy: tensor(0.7946, device='cuda:0')\n",
      "Batch Loss: 4544.640861157328 Accuracy: tensor(0.7953, device='cuda:0')\n",
      "Batch Loss: 4573.625990409404 Accuracy: tensor(0.7960, device='cuda:0')\n",
      "Batch Loss: 4603.756244562566 Accuracy: tensor(0.7965, device='cuda:0')\n",
      "Batch Loss: 4633.006824154407 Accuracy: tensor(0.7972, device='cuda:0')\n",
      "Batch Loss: 4660.387511430308 Accuracy: tensor(0.7980, device='cuda:0')\n",
      "Batch Loss: 4688.783083878458 Accuracy: tensor(0.7987, device='cuda:0')\n",
      "Batch Loss: 4719.34219019115 Accuracy: tensor(0.7992, device='cuda:0')\n",
      "Batch Loss: 4746.173683913425 Accuracy: tensor(0.8000, device='cuda:0')\n",
      "Batch Loss: 4776.042416879907 Accuracy: tensor(0.8006, device='cuda:0')\n",
      "Batch Loss: 4805.5563887842 Accuracy: tensor(0.8013, device='cuda:0')\n",
      "Batch Loss: 4833.145173858851 Accuracy: tensor(0.8019, device='cuda:0')\n",
      "Train Loss: 4844.082913466729 Accuracy: tensor(0.8022, device='cuda:0')\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6127435348004618b9d64e70e9e23dc3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/12944 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch Loss: 22.01587924733758 Accuracy: tensor(0.9137, device='cuda:0')\n",
      "Batch Loss: 50.65532027371228 Accuracy: tensor(0.9013, device='cuda:0')\n",
      "Batch Loss: 78.53205647971481 Accuracy: tensor(0.8958, device='cuda:0')\n",
      "Batch Loss: 103.20021567959338 Accuracy: tensor(0.8991, device='cuda:0')\n",
      "Batch Loss: 127.593963409774 Accuracy: tensor(0.9005, device='cuda:0')\n",
      "Batch Loss: 150.983164800331 Accuracy: tensor(0.9029, device='cuda:0')\n",
      "Batch Loss: 172.81306499056518 Accuracy: tensor(0.9055, device='cuda:0')\n",
      "Batch Loss: 195.4840580355376 Accuracy: tensor(0.9041, device='cuda:0')\n",
      "Batch Loss: 222.17656906321645 Accuracy: tensor(0.9032, device='cuda:0')\n",
      "Batch Loss: 245.8228003643453 Accuracy: tensor(0.9039, device='cuda:0')\n",
      "Batch Loss: 272.1063588745892 Accuracy: tensor(0.9017, device='cuda:0')\n",
      "Batch Loss: 294.3952406877652 Accuracy: tensor(0.9022, device='cuda:0')\n",
      "Batch Loss: 316.0994184212759 Accuracy: tensor(0.9031, device='cuda:0')\n",
      "Batch Loss: 339.39149618241936 Accuracy: tensor(0.9033, device='cuda:0')\n",
      "Batch Loss: 364.1218691179529 Accuracy: tensor(0.9033, device='cuda:0')\n",
      "Batch Loss: 386.6503483755514 Accuracy: tensor(0.9036, device='cuda:0')\n",
      "Batch Loss: 412.5585697060451 Accuracy: tensor(0.9034, device='cuda:0')\n",
      "Batch Loss: 437.9001429947093 Accuracy: tensor(0.9035, device='cuda:0')\n",
      "Batch Loss: 459.63053322862834 Accuracy: tensor(0.9039, device='cuda:0')\n",
      "Batch Loss: 482.5225821165368 Accuracy: tensor(0.9039, device='cuda:0')\n",
      "Batch Loss: 504.5907445475459 Accuracy: tensor(0.9044, device='cuda:0')\n",
      "Batch Loss: 528.1859703334048 Accuracy: tensor(0.9050, device='cuda:0')\n",
      "Batch Loss: 554.0058703990653 Accuracy: tensor(0.9048, device='cuda:0')\n",
      "Batch Loss: 576.8941496638581 Accuracy: tensor(0.9051, device='cuda:0')\n",
      "Batch Loss: 603.6436541071162 Accuracy: tensor(0.9044, device='cuda:0')\n",
      "Batch Loss: 628.5063720513135 Accuracy: tensor(0.9041, device='cuda:0')\n",
      "Batch Loss: 656.2944327574223 Accuracy: tensor(0.9034, device='cuda:0')\n",
      "Batch Loss: 684.8669180497527 Accuracy: tensor(0.9027, device='cuda:0')\n",
      "Batch Loss: 713.1467111557722 Accuracy: tensor(0.9021, device='cuda:0')\n",
      "Batch Loss: 738.247285483405 Accuracy: tensor(0.9019, device='cuda:0')\n",
      "Batch Loss: 761.954003283754 Accuracy: tensor(0.9015, device='cuda:0')\n",
      "Batch Loss: 785.4412143817171 Accuracy: tensor(0.9016, device='cuda:0')\n",
      "Batch Loss: 808.3661841219291 Accuracy: tensor(0.9018, device='cuda:0')\n",
      "Batch Loss: 834.1915902644396 Accuracy: tensor(0.9018, device='cuda:0')\n",
      "Batch Loss: 856.4506230112165 Accuracy: tensor(0.9020, device='cuda:0')\n",
      "Batch Loss: 879.2402909696102 Accuracy: tensor(0.9020, device='cuda:0')\n",
      "Batch Loss: 905.8750321120024 Accuracy: tensor(0.9016, device='cuda:0')\n",
      "Batch Loss: 928.2165412958711 Accuracy: tensor(0.9018, device='cuda:0')\n",
      "Batch Loss: 952.1670372188091 Accuracy: tensor(0.9015, device='cuda:0')\n",
      "Batch Loss: 980.4175360333174 Accuracy: tensor(0.9011, device='cuda:0')\n",
      "Batch Loss: 1003.229959435761 Accuracy: tensor(0.9012, device='cuda:0')\n",
      "Batch Loss: 1027.5086912093684 Accuracy: tensor(0.9014, device='cuda:0')\n",
      "Batch Loss: 1052.4001214271411 Accuracy: tensor(0.9012, device='cuda:0')\n",
      "Batch Loss: 1073.2109215892851 Accuracy: tensor(0.9016, device='cuda:0')\n",
      "Batch Loss: 1098.599465791136 Accuracy: tensor(0.9018, device='cuda:0')\n",
      "Batch Loss: 1122.5223240964115 Accuracy: tensor(0.9019, device='cuda:0')\n",
      "Batch Loss: 1144.1991138719022 Accuracy: tensor(0.9021, device='cuda:0')\n",
      "Batch Loss: 1170.932560969144 Accuracy: tensor(0.9020, device='cuda:0')\n",
      "Batch Loss: 1194.9706781096756 Accuracy: tensor(0.9020, device='cuda:0')\n",
      "Batch Loss: 1220.142858395353 Accuracy: tensor(0.9019, device='cuda:0')\n",
      "Batch Loss: 1248.9713768344373 Accuracy: tensor(0.9013, device='cuda:0')\n",
      "Batch Loss: 1272.9222459252924 Accuracy: tensor(0.9013, device='cuda:0')\n",
      "Batch Loss: 1297.089262880385 Accuracy: tensor(0.9012, device='cuda:0')\n",
      "Batch Loss: 1320.6654976513237 Accuracy: tensor(0.9013, device='cuda:0')\n",
      "Batch Loss: 1345.0025378204882 Accuracy: tensor(0.9012, device='cuda:0')\n",
      "Batch Loss: 1370.011731935665 Accuracy: tensor(0.9012, device='cuda:0')\n",
      "Batch Loss: 1397.833023140207 Accuracy: tensor(0.9011, device='cuda:0')\n",
      "Batch Loss: 1421.6881030220538 Accuracy: tensor(0.9011, device='cuda:0')\n",
      "Batch Loss: 1444.0947835389525 Accuracy: tensor(0.9013, device='cuda:0')\n",
      "Batch Loss: 1469.3090907931328 Accuracy: tensor(0.9011, device='cuda:0')\n",
      "Batch Loss: 1497.6131269335747 Accuracy: tensor(0.9008, device='cuda:0')\n",
      "Batch Loss: 1522.308192403987 Accuracy: tensor(0.9008, device='cuda:0')\n",
      "Batch Loss: 1542.4468597322702 Accuracy: tensor(0.9013, device='cuda:0')\n",
      "Batch Loss: 1569.3667790694162 Accuracy: tensor(0.9011, device='cuda:0')\n",
      "Batch Loss: 1595.9280197070912 Accuracy: tensor(0.9010, device='cuda:0')\n",
      "Batch Loss: 1622.3339605266228 Accuracy: tensor(0.9009, device='cuda:0')\n",
      "Batch Loss: 1642.9163800030947 Accuracy: tensor(0.9012, device='cuda:0')\n",
      "Batch Loss: 1664.126205407083 Accuracy: tensor(0.9013, device='cuda:0')\n",
      "Batch Loss: 1686.8915518214926 Accuracy: tensor(0.9015, device='cuda:0')\n",
      "Batch Loss: 1708.7050723265857 Accuracy: tensor(0.9017, device='cuda:0')\n",
      "Batch Loss: 1732.477158905007 Accuracy: tensor(0.9018, device='cuda:0')\n",
      "Batch Loss: 1755.5681885806844 Accuracy: tensor(0.9019, device='cuda:0')\n",
      "Batch Loss: 1779.6891621137038 Accuracy: tensor(0.9019, device='cuda:0')\n",
      "Batch Loss: 1801.630529537797 Accuracy: tensor(0.9020, device='cuda:0')\n",
      "Batch Loss: 1824.2325959485024 Accuracy: tensor(0.9021, device='cuda:0')\n",
      "Batch Loss: 1848.4028336070478 Accuracy: tensor(0.9022, device='cuda:0')\n",
      "Batch Loss: 1872.6975850649178 Accuracy: tensor(0.9023, device='cuda:0')\n",
      "Batch Loss: 1895.1270809806883 Accuracy: tensor(0.9024, device='cuda:0')\n",
      "Batch Loss: 1915.8435855694115 Accuracy: tensor(0.9027, device='cuda:0')\n",
      "Batch Loss: 1937.0145220253617 Accuracy: tensor(0.9028, device='cuda:0')\n",
      "Batch Loss: 1959.4331838265061 Accuracy: tensor(0.9029, device='cuda:0')\n",
      "Batch Loss: 1981.9470505099744 Accuracy: tensor(0.9030, device='cuda:0')\n",
      "Batch Loss: 2003.6828926503658 Accuracy: tensor(0.9031, device='cuda:0')\n",
      "Batch Loss: 2026.4304707422853 Accuracy: tensor(0.9032, device='cuda:0')\n",
      "Batch Loss: 2047.780689690262 Accuracy: tensor(0.9034, device='cuda:0')\n",
      "Batch Loss: 2067.432914598845 Accuracy: tensor(0.9035, device='cuda:0')\n",
      "Batch Loss: 2090.01875541918 Accuracy: tensor(0.9037, device='cuda:0')\n",
      "Batch Loss: 2113.761261280626 Accuracy: tensor(0.9037, device='cuda:0')\n",
      "Batch Loss: 2133.6628545625135 Accuracy: tensor(0.9038, device='cuda:0')\n",
      "Batch Loss: 2162.2233216045424 Accuracy: tensor(0.9036, device='cuda:0')\n",
      "Batch Loss: 2183.5890841381624 Accuracy: tensor(0.9037, device='cuda:0')\n",
      "Batch Loss: 2205.2073374651372 Accuracy: tensor(0.9037, device='cuda:0')\n",
      "Batch Loss: 2227.5684109069407 Accuracy: tensor(0.9038, device='cuda:0')\n",
      "Batch Loss: 2247.463494570926 Accuracy: tensor(0.9041, device='cuda:0')\n",
      "Batch Loss: 2270.4385209158063 Accuracy: tensor(0.9042, device='cuda:0')\n",
      "Batch Loss: 2291.963098477572 Accuracy: tensor(0.9043, device='cuda:0')\n",
      "Batch Loss: 2312.438912767917 Accuracy: tensor(0.9044, device='cuda:0')\n",
      "Batch Loss: 2336.7727567339316 Accuracy: tensor(0.9044, device='cuda:0')\n",
      "Batch Loss: 2363.3095435528085 Accuracy: tensor(0.9042, device='cuda:0')\n",
      "Batch Loss: 2384.3845946639776 Accuracy: tensor(0.9043, device='cuda:0')\n",
      "Batch Loss: 2406.7114405483007 Accuracy: tensor(0.9043, device='cuda:0')\n",
      "Batch Loss: 2427.9400435443968 Accuracy: tensor(0.9044, device='cuda:0')\n",
      "Batch Loss: 2448.162184118293 Accuracy: tensor(0.9045, device='cuda:0')\n",
      "Batch Loss: 2469.297313177027 Accuracy: tensor(0.9046, device='cuda:0')\n",
      "Batch Loss: 2495.339500802569 Accuracy: tensor(0.9046, device='cuda:0')\n",
      "Batch Loss: 2516.540459885262 Accuracy: tensor(0.9047, device='cuda:0')\n",
      "Batch Loss: 2536.4471427686512 Accuracy: tensor(0.9048, device='cuda:0')\n",
      "Batch Loss: 2560.6546011250466 Accuracy: tensor(0.9048, device='cuda:0')\n",
      "Batch Loss: 2585.342382262461 Accuracy: tensor(0.9048, device='cuda:0')\n",
      "Batch Loss: 2608.2274913704023 Accuracy: tensor(0.9049, device='cuda:0')\n",
      "Batch Loss: 2632.2706875707954 Accuracy: tensor(0.9049, device='cuda:0')\n",
      "Batch Loss: 2652.1643900806084 Accuracy: tensor(0.9050, device='cuda:0')\n",
      "Batch Loss: 2675.573631626554 Accuracy: tensor(0.9051, device='cuda:0')\n",
      "Batch Loss: 2697.457382339053 Accuracy: tensor(0.9051, device='cuda:0')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch Loss: 2719.1262877127156 Accuracy: tensor(0.9051, device='cuda:0')\n",
      "Batch Loss: 2740.726310084574 Accuracy: tensor(0.9053, device='cuda:0')\n",
      "Batch Loss: 2763.779820665717 Accuracy: tensor(0.9053, device='cuda:0')\n",
      "Batch Loss: 2787.6582448743284 Accuracy: tensor(0.9053, device='cuda:0')\n",
      "Batch Loss: 2813.3369348570704 Accuracy: tensor(0.9053, device='cuda:0')\n",
      "Batch Loss: 2833.348502198234 Accuracy: tensor(0.9054, device='cuda:0')\n",
      "Batch Loss: 2857.147782390937 Accuracy: tensor(0.9054, device='cuda:0')\n",
      "Batch Loss: 2882.6330180093646 Accuracy: tensor(0.9054, device='cuda:0')\n",
      "Batch Loss: 2907.0319920554757 Accuracy: tensor(0.9054, device='cuda:0')\n",
      "Batch Loss: 2931.0828027073294 Accuracy: tensor(0.9052, device='cuda:0')\n",
      "Batch Loss: 2955.482524957508 Accuracy: tensor(0.9051, device='cuda:0')\n",
      "Batch Loss: 2977.2121002767235 Accuracy: tensor(0.9051, device='cuda:0')\n",
      "Batch Loss: 3000.650106418878 Accuracy: tensor(0.9051, device='cuda:0')\n",
      "Batch Loss: 3022.6999760176986 Accuracy: tensor(0.9050, device='cuda:0')\n",
      "Batch Loss: 3042.368811579421 Accuracy: tensor(0.9053, device='cuda:0')\n",
      "Train Loss: 3052.535625178367 Accuracy: tensor(0.9053, device='cuda:0')\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "30548261bb4f481ba7063bba8ddda7c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/12944 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch Loss: 17.257269660010934 Accuracy: tensor(0.9225, device='cuda:0')\n",
      "Batch Loss: 35.30554376449436 Accuracy: tensor(0.9287, device='cuda:0')\n",
      "Batch Loss: 52.71096785739064 Accuracy: tensor(0.9338, device='cuda:0')\n",
      "Batch Loss: 72.59537161700428 Accuracy: tensor(0.9322, device='cuda:0')\n",
      "Batch Loss: 91.95562836527824 Accuracy: tensor(0.9313, device='cuda:0')\n",
      "Batch Loss: 107.76801470387727 Accuracy: tensor(0.9321, device='cuda:0')\n",
      "Batch Loss: 128.05205613188446 Accuracy: tensor(0.9286, device='cuda:0')\n",
      "Batch Loss: 145.891491307877 Accuracy: tensor(0.9286, device='cuda:0')\n",
      "Batch Loss: 160.3741290392354 Accuracy: tensor(0.9304, device='cuda:0')\n",
      "Batch Loss: 175.91060996847227 Accuracy: tensor(0.9319, device='cuda:0')\n",
      "Batch Loss: 194.82347092684358 Accuracy: tensor(0.9317, device='cuda:0')\n",
      "Batch Loss: 211.15229026135057 Accuracy: tensor(0.9323, device='cuda:0')\n",
      "Batch Loss: 229.0019555669278 Accuracy: tensor(0.9315, device='cuda:0')\n",
      "Batch Loss: 248.89510705228895 Accuracy: tensor(0.9312, device='cuda:0')\n",
      "Batch Loss: 266.63719328306615 Accuracy: tensor(0.9312, device='cuda:0')\n",
      "Batch Loss: 286.1181901060045 Accuracy: tensor(0.9312, device='cuda:0')\n",
      "Batch Loss: 306.0047781141475 Accuracy: tensor(0.9309, device='cuda:0')\n",
      "Batch Loss: 322.9311212366447 Accuracy: tensor(0.9314, device='cuda:0')\n",
      "Batch Loss: 338.4979258500971 Accuracy: tensor(0.9321, device='cuda:0')\n",
      "Batch Loss: 356.53916456224397 Accuracy: tensor(0.9324, device='cuda:0')\n",
      "Batch Loss: 373.80042007099837 Accuracy: tensor(0.9326, device='cuda:0')\n",
      "Batch Loss: 390.3077620710246 Accuracy: tensor(0.9330, device='cuda:0')\n",
      "Batch Loss: 405.6903110211715 Accuracy: tensor(0.9333, device='cuda:0')\n",
      "Batch Loss: 421.20090873679146 Accuracy: tensor(0.9333, device='cuda:0')\n",
      "Batch Loss: 440.3519872264005 Accuracy: tensor(0.9331, device='cuda:0')\n",
      "Batch Loss: 458.60350471781567 Accuracy: tensor(0.9330, device='cuda:0')\n",
      "Batch Loss: 478.3512339456938 Accuracy: tensor(0.9327, device='cuda:0')\n",
      "Batch Loss: 497.8930143122561 Accuracy: tensor(0.9322, device='cuda:0')\n",
      "Batch Loss: 514.4779289369471 Accuracy: tensor(0.9323, device='cuda:0')\n",
      "Batch Loss: 534.8943795035593 Accuracy: tensor(0.9321, device='cuda:0')\n",
      "Batch Loss: 553.0827561444603 Accuracy: tensor(0.9318, device='cuda:0')\n",
      "Batch Loss: 569.6341091240756 Accuracy: tensor(0.9320, device='cuda:0')\n",
      "Batch Loss: 587.6587528497912 Accuracy: tensor(0.9319, device='cuda:0')\n",
      "Batch Loss: 607.8619454535656 Accuracy: tensor(0.9318, device='cuda:0')\n",
      "Batch Loss: 625.0571208857 Accuracy: tensor(0.9319, device='cuda:0')\n",
      "Batch Loss: 641.6740587796085 Accuracy: tensor(0.9319, device='cuda:0')\n",
      "Batch Loss: 658.5474130776711 Accuracy: tensor(0.9323, device='cuda:0')\n",
      "Batch Loss: 679.6667177970521 Accuracy: tensor(0.9320, device='cuda:0')\n",
      "Batch Loss: 698.4071186981164 Accuracy: tensor(0.9320, device='cuda:0')\n",
      "Batch Loss: 717.0121385981329 Accuracy: tensor(0.9321, device='cuda:0')\n",
      "Batch Loss: 737.7998971720226 Accuracy: tensor(0.9319, device='cuda:0')\n",
      "Batch Loss: 755.2042763014324 Accuracy: tensor(0.9321, device='cuda:0')\n",
      "Batch Loss: 775.0723956250586 Accuracy: tensor(0.9320, device='cuda:0')\n",
      "Batch Loss: 791.9792646192946 Accuracy: tensor(0.9320, device='cuda:0')\n",
      "Batch Loss: 809.4674182818271 Accuracy: tensor(0.9319, device='cuda:0')\n",
      "Batch Loss: 830.7524841181003 Accuracy: tensor(0.9316, device='cuda:0')\n",
      "Batch Loss: 848.8912567105144 Accuracy: tensor(0.9313, device='cuda:0')\n",
      "Batch Loss: 868.3993448987603 Accuracy: tensor(0.9311, device='cuda:0')\n",
      "Batch Loss: 889.6561446059495 Accuracy: tensor(0.9308, device='cuda:0')\n",
      "Batch Loss: 908.9090997390449 Accuracy: tensor(0.9305, device='cuda:0')\n",
      "Batch Loss: 929.3761845557019 Accuracy: tensor(0.9304, device='cuda:0')\n",
      "Batch Loss: 948.0644686543383 Accuracy: tensor(0.9305, device='cuda:0')\n",
      "Batch Loss: 965.4793712734245 Accuracy: tensor(0.9304, device='cuda:0')\n",
      "Batch Loss: 985.6806677752174 Accuracy: tensor(0.9303, device='cuda:0')\n",
      "Batch Loss: 1005.8506262763403 Accuracy: tensor(0.9301, device='cuda:0')\n",
      "Batch Loss: 1021.7255293568596 Accuracy: tensor(0.9304, device='cuda:0')\n",
      "Batch Loss: 1038.459673970472 Accuracy: tensor(0.9305, device='cuda:0')\n",
      "Batch Loss: 1060.4634695784189 Accuracy: tensor(0.9302, device='cuda:0')\n",
      "Batch Loss: 1075.3296711426228 Accuracy: tensor(0.9303, device='cuda:0')\n",
      "Batch Loss: 1098.5913116661832 Accuracy: tensor(0.9299, device='cuda:0')\n",
      "Batch Loss: 1115.8722252957523 Accuracy: tensor(0.9300, device='cuda:0')\n",
      "Batch Loss: 1137.1331711467355 Accuracy: tensor(0.9297, device='cuda:0')\n",
      "Batch Loss: 1152.3500968166627 Accuracy: tensor(0.9297, device='cuda:0')\n",
      "Batch Loss: 1173.4423830858432 Accuracy: tensor(0.9297, device='cuda:0')\n",
      "Batch Loss: 1194.0679407729767 Accuracy: tensor(0.9296, device='cuda:0')\n",
      "Batch Loss: 1212.4884200277738 Accuracy: tensor(0.9297, device='cuda:0')\n",
      "Batch Loss: 1228.1772175454535 Accuracy: tensor(0.9298, device='cuda:0')\n",
      "Batch Loss: 1250.1189057375304 Accuracy: tensor(0.9296, device='cuda:0')\n",
      "Batch Loss: 1269.5115995095111 Accuracy: tensor(0.9295, device='cuda:0')\n",
      "Batch Loss: 1289.831270148512 Accuracy: tensor(0.9295, device='cuda:0')\n",
      "Batch Loss: 1310.3631332363002 Accuracy: tensor(0.9294, device='cuda:0')\n",
      "Batch Loss: 1328.3815193041228 Accuracy: tensor(0.9293, device='cuda:0')\n",
      "Batch Loss: 1343.21681024367 Accuracy: tensor(0.9294, device='cuda:0')\n",
      "Batch Loss: 1364.0782069033012 Accuracy: tensor(0.9292, device='cuda:0')\n",
      "Batch Loss: 1384.176849831827 Accuracy: tensor(0.9291, device='cuda:0')\n",
      "Batch Loss: 1406.7524816766381 Accuracy: tensor(0.9288, device='cuda:0')\n",
      "Batch Loss: 1425.546358589083 Accuracy: tensor(0.9288, device='cuda:0')\n",
      "Batch Loss: 1446.0418400149792 Accuracy: tensor(0.9287, device='cuda:0')\n",
      "Batch Loss: 1464.697366268374 Accuracy: tensor(0.9287, device='cuda:0')\n",
      "Batch Loss: 1481.5331136947498 Accuracy: tensor(0.9287, device='cuda:0')\n",
      "Batch Loss: 1499.702183449408 Accuracy: tensor(0.9288, device='cuda:0')\n",
      "Batch Loss: 1519.2633348980453 Accuracy: tensor(0.9287, device='cuda:0')\n",
      "Batch Loss: 1536.2436616953928 Accuracy: tensor(0.9288, device='cuda:0')\n",
      "Batch Loss: 1554.0579549309332 Accuracy: tensor(0.9287, device='cuda:0')\n",
      "Batch Loss: 1569.8704860231373 Accuracy: tensor(0.9289, device='cuda:0')\n",
      "Batch Loss: 1589.1967721024994 Accuracy: tensor(0.9289, device='cuda:0')\n",
      "Batch Loss: 1610.6240561867598 Accuracy: tensor(0.9287, device='cuda:0')\n",
      "Batch Loss: 1629.2068690585438 Accuracy: tensor(0.9287, device='cuda:0')\n",
      "Batch Loss: 1649.6514844114427 Accuracy: tensor(0.9286, device='cuda:0')\n",
      "Batch Loss: 1666.9166026946623 Accuracy: tensor(0.9286, device='cuda:0')\n",
      "Batch Loss: 1682.892601228552 Accuracy: tensor(0.9287, device='cuda:0')\n",
      "Batch Loss: 1703.337597036967 Accuracy: tensor(0.9286, device='cuda:0')\n",
      "Batch Loss: 1725.6559546135832 Accuracy: tensor(0.9284, device='cuda:0')\n",
      "Batch Loss: 1746.4478285668883 Accuracy: tensor(0.9283, device='cuda:0')\n",
      "Batch Loss: 1765.938836245099 Accuracy: tensor(0.9282, device='cuda:0')\n",
      "Batch Loss: 1785.8121085653547 Accuracy: tensor(0.9283, device='cuda:0')\n",
      "Batch Loss: 1803.1685011538211 Accuracy: tensor(0.9284, device='cuda:0')\n",
      "Batch Loss: 1821.3501574855763 Accuracy: tensor(0.9285, device='cuda:0')\n",
      "Batch Loss: 1840.7438390862662 Accuracy: tensor(0.9285, device='cuda:0')\n",
      "Batch Loss: 1860.804989531869 Accuracy: tensor(0.9284, device='cuda:0')\n",
      "Batch Loss: 1877.9571015893016 Accuracy: tensor(0.9285, device='cuda:0')\n",
      "Batch Loss: 1896.619926584186 Accuracy: tensor(0.9285, device='cuda:0')\n",
      "Batch Loss: 1915.1644694956485 Accuracy: tensor(0.9286, device='cuda:0')\n",
      "Batch Loss: 1934.1955811635125 Accuracy: tensor(0.9286, device='cuda:0')\n",
      "Batch Loss: 1951.721058404306 Accuracy: tensor(0.9287, device='cuda:0')\n",
      "Batch Loss: 1972.7482095735613 Accuracy: tensor(0.9285, device='cuda:0')\n",
      "Batch Loss: 1992.664462550776 Accuracy: tensor(0.9285, device='cuda:0')\n",
      "Batch Loss: 2010.426335561322 Accuracy: tensor(0.9285, device='cuda:0')\n",
      "Batch Loss: 2029.6208555225749 Accuracy: tensor(0.9284, device='cuda:0')\n",
      "Batch Loss: 2048.2552722825203 Accuracy: tensor(0.9284, device='cuda:0')\n",
      "Batch Loss: 2069.8789930392522 Accuracy: tensor(0.9283, device='cuda:0')\n",
      "Batch Loss: 2089.0155809328426 Accuracy: tensor(0.9284, device='cuda:0')\n",
      "Batch Loss: 2108.575026266044 Accuracy: tensor(0.9284, device='cuda:0')\n",
      "Batch Loss: 2129.9351428707596 Accuracy: tensor(0.9283, device='cuda:0')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch Loss: 2151.269168983912 Accuracy: tensor(0.9281, device='cuda:0')\n",
      "Batch Loss: 2168.4163940015715 Accuracy: tensor(0.9281, device='cuda:0')\n",
      "Batch Loss: 2187.200666025048 Accuracy: tensor(0.9282, device='cuda:0')\n",
      "Batch Loss: 2207.3219709719997 Accuracy: tensor(0.9282, device='cuda:0')\n",
      "Batch Loss: 2229.7861578396987 Accuracy: tensor(0.9281, device='cuda:0')\n",
      "Batch Loss: 2248.7044899265748 Accuracy: tensor(0.9281, device='cuda:0')\n",
      "Batch Loss: 2266.1283456094097 Accuracy: tensor(0.9281, device='cuda:0')\n",
      "Batch Loss: 2288.0148432168644 Accuracy: tensor(0.9280, device='cuda:0')\n",
      "Batch Loss: 2308.626939837588 Accuracy: tensor(0.9279, device='cuda:0')\n",
      "Batch Loss: 2325.8428014197852 Accuracy: tensor(0.9279, device='cuda:0')\n",
      "Batch Loss: 2346.547527007526 Accuracy: tensor(0.9278, device='cuda:0')\n",
      "Batch Loss: 2367.941919226898 Accuracy: tensor(0.9277, device='cuda:0')\n",
      "Batch Loss: 2386.442492240807 Accuracy: tensor(0.9277, device='cuda:0')\n",
      "Batch Loss: 2406.754202099284 Accuracy: tensor(0.9277, device='cuda:0')\n",
      "Batch Loss: 2427.3026477096137 Accuracy: tensor(0.9277, device='cuda:0')\n",
      "Train Loss: 2435.7873588090297 Accuracy: tensor(0.9277, device='cuda:0')\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b0bd6e7675e743978d5f57574789968e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/12944 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch Loss: 12.104400697629899 Accuracy: tensor(0.9613, device='cuda:0')\n",
      "Batch Loss: 25.21407294878736 Accuracy: tensor(0.9581, device='cuda:0')\n",
      "Batch Loss: 37.082583635579795 Accuracy: tensor(0.9575, device='cuda:0')\n",
      "Batch Loss: 49.67619282566011 Accuracy: tensor(0.9587, device='cuda:0')\n",
      "Batch Loss: 62.817865853663534 Accuracy: tensor(0.9575, device='cuda:0')\n",
      "Batch Loss: 73.60514885839075 Accuracy: tensor(0.9588, device='cuda:0')\n",
      "Batch Loss: 84.81498421076685 Accuracy: tensor(0.9589, device='cuda:0')\n",
      "Batch Loss: 97.63317142101005 Accuracy: tensor(0.9580, device='cuda:0')\n",
      "Batch Loss: 113.54329439578578 Accuracy: tensor(0.9565, device='cuda:0')\n",
      "Batch Loss: 125.41939761769027 Accuracy: tensor(0.9566, device='cuda:0')\n",
      "Batch Loss: 142.12836645450443 Accuracy: tensor(0.9556, device='cuda:0')\n",
      "Batch Loss: 154.5522202020511 Accuracy: tensor(0.9557, device='cuda:0')\n",
      "Batch Loss: 166.90996492980048 Accuracy: tensor(0.9559, device='cuda:0')\n",
      "Batch Loss: 178.46645452827215 Accuracy: tensor(0.9559, device='cuda:0')\n",
      "Batch Loss: 189.5639050421305 Accuracy: tensor(0.9561, device='cuda:0')\n",
      "Batch Loss: 201.96022838400677 Accuracy: tensor(0.9565, device='cuda:0')\n",
      "Batch Loss: 214.42673945659772 Accuracy: tensor(0.9564, device='cuda:0')\n",
      "Batch Loss: 227.18027107277885 Accuracy: tensor(0.9565, device='cuda:0')\n",
      "Batch Loss: 239.59087441582233 Accuracy: tensor(0.9566, device='cuda:0')\n",
      "Batch Loss: 252.42980479239486 Accuracy: tensor(0.9568, device='cuda:0')\n",
      "Batch Loss: 266.73976956144907 Accuracy: tensor(0.9564, device='cuda:0')\n",
      "Batch Loss: 280.7148925808724 Accuracy: tensor(0.9559, device='cuda:0')\n",
      "Batch Loss: 293.1214904820081 Accuracy: tensor(0.9559, device='cuda:0')\n",
      "Batch Loss: 304.77765042032115 Accuracy: tensor(0.9558, device='cuda:0')\n",
      "Batch Loss: 321.49206997430883 Accuracy: tensor(0.9553, device='cuda:0')\n",
      "Batch Loss: 330.54435161477886 Accuracy: tensor(0.9556, device='cuda:0')\n",
      "Batch Loss: 343.2335687179584 Accuracy: tensor(0.9555, device='cuda:0')\n",
      "Batch Loss: 355.7544753176626 Accuracy: tensor(0.9557, device='cuda:0')\n",
      "Batch Loss: 368.9017720783595 Accuracy: tensor(0.9554, device='cuda:0')\n",
      "Batch Loss: 385.6322708192747 Accuracy: tensor(0.9549, device='cuda:0')\n",
      "Batch Loss: 401.2901265777182 Accuracy: tensor(0.9546, device='cuda:0')\n",
      "Batch Loss: 413.0964379135985 Accuracy: tensor(0.9545, device='cuda:0')\n",
      "Batch Loss: 428.2754676754121 Accuracy: tensor(0.9540, device='cuda:0')\n",
      "Batch Loss: 440.6453714405652 Accuracy: tensor(0.9540, device='cuda:0')\n",
      "Batch Loss: 454.5044207109604 Accuracy: tensor(0.9538, device='cuda:0')\n",
      "Batch Loss: 467.2268902391661 Accuracy: tensor(0.9536, device='cuda:0')\n",
      "Batch Loss: 480.6683635977097 Accuracy: tensor(0.9539, device='cuda:0')\n",
      "Batch Loss: 498.1426121555269 Accuracy: tensor(0.9534, device='cuda:0')\n",
      "Batch Loss: 513.9965911619365 Accuracy: tensor(0.9531, device='cuda:0')\n",
      "Batch Loss: 529.3867475283332 Accuracy: tensor(0.9529, device='cuda:0')\n",
      "Batch Loss: 542.7041386482306 Accuracy: tensor(0.9530, device='cuda:0')\n",
      "Batch Loss: 555.6038604744244 Accuracy: tensor(0.9532, device='cuda:0')\n",
      "Batch Loss: 569.8453105886001 Accuracy: tensor(0.9530, device='cuda:0')\n",
      "Batch Loss: 583.4160806250293 Accuracy: tensor(0.9530, device='cuda:0')\n",
      "Batch Loss: 595.4662650846876 Accuracy: tensor(0.9531, device='cuda:0')\n",
      "Batch Loss: 609.0299852224998 Accuracy: tensor(0.9530, device='cuda:0')\n",
      "Batch Loss: 623.6247252565809 Accuracy: tensor(0.9530, device='cuda:0')\n",
      "Batch Loss: 636.5880766017362 Accuracy: tensor(0.9532, device='cuda:0')\n",
      "Batch Loss: 648.9471392789856 Accuracy: tensor(0.9532, device='cuda:0')\n",
      "Batch Loss: 661.4922551396303 Accuracy: tensor(0.9533, device='cuda:0')\n",
      "Batch Loss: 676.7300078347325 Accuracy: tensor(0.9532, device='cuda:0')\n",
      "Batch Loss: 687.9853366001043 Accuracy: tensor(0.9533, device='cuda:0')\n",
      "Batch Loss: 704.4856461270247 Accuracy: tensor(0.9531, device='cuda:0')\n",
      "Batch Loss: 719.4034026789013 Accuracy: tensor(0.9529, device='cuda:0')\n",
      "Batch Loss: 731.0483951081987 Accuracy: tensor(0.9531, device='cuda:0')\n",
      "Batch Loss: 741.0888513454702 Accuracy: tensor(0.9532, device='cuda:0')\n",
      "Batch Loss: 754.2274614737835 Accuracy: tensor(0.9531, device='cuda:0')\n",
      "Batch Loss: 766.1726873775478 Accuracy: tensor(0.9533, device='cuda:0')\n",
      "Batch Loss: 781.8155873056967 Accuracy: tensor(0.9531, device='cuda:0')\n",
      "Batch Loss: 793.6674528608564 Accuracy: tensor(0.9529, device='cuda:0')\n",
      "Batch Loss: 806.3250895750243 Accuracy: tensor(0.9529, device='cuda:0')\n",
      "Batch Loss: 822.589640732389 Accuracy: tensor(0.9527, device='cuda:0')\n",
      "Batch Loss: 837.6273532751948 Accuracy: tensor(0.9527, device='cuda:0')\n",
      "Batch Loss: 852.3631416754797 Accuracy: tensor(0.9527, device='cuda:0')\n",
      "Batch Loss: 865.2080740970559 Accuracy: tensor(0.9527, device='cuda:0')\n",
      "Batch Loss: 880.4605466723442 Accuracy: tensor(0.9525, device='cuda:0')\n",
      "Batch Loss: 896.0043666041456 Accuracy: tensor(0.9523, device='cuda:0')\n",
      "Batch Loss: 910.9197884309106 Accuracy: tensor(0.9520, device='cuda:0')\n",
      "Batch Loss: 924.4430657811463 Accuracy: tensor(0.9520, device='cuda:0')\n",
      "Batch Loss: 938.9575849282555 Accuracy: tensor(0.9520, device='cuda:0')\n",
      "Batch Loss: 956.2523512854241 Accuracy: tensor(0.9517, device='cuda:0')\n",
      "Batch Loss: 968.4250152241439 Accuracy: tensor(0.9518, device='cuda:0')\n",
      "Batch Loss: 984.6012107525021 Accuracy: tensor(0.9517, device='cuda:0')\n",
      "Batch Loss: 999.0118197039701 Accuracy: tensor(0.9516, device='cuda:0')\n",
      "Batch Loss: 1014.4081672886387 Accuracy: tensor(0.9516, device='cuda:0')\n",
      "Batch Loss: 1028.9610159983858 Accuracy: tensor(0.9515, device='cuda:0')\n",
      "Batch Loss: 1043.8173954533413 Accuracy: tensor(0.9515, device='cuda:0')\n",
      "Batch Loss: 1055.6826406568289 Accuracy: tensor(0.9516, device='cuda:0')\n",
      "Batch Loss: 1071.0020300019532 Accuracy: tensor(0.9515, device='cuda:0')\n",
      "Batch Loss: 1087.6795510174707 Accuracy: tensor(0.9513, device='cuda:0')\n",
      "Batch Loss: 1102.017322030384 Accuracy: tensor(0.9513, device='cuda:0')\n",
      "Batch Loss: 1115.9477776647545 Accuracy: tensor(0.9513, device='cuda:0')\n",
      "Batch Loss: 1127.8241559728049 Accuracy: tensor(0.9515, device='cuda:0')\n",
      "Batch Loss: 1141.253834489733 Accuracy: tensor(0.9515, device='cuda:0')\n",
      "Batch Loss: 1157.0324050583877 Accuracy: tensor(0.9514, device='cuda:0')\n",
      "Batch Loss: 1168.4451244617812 Accuracy: tensor(0.9514, device='cuda:0')\n",
      "Batch Loss: 1181.0073517845012 Accuracy: tensor(0.9516, device='cuda:0')\n",
      "Batch Loss: 1195.7251855907962 Accuracy: tensor(0.9515, device='cuda:0')\n",
      "Batch Loss: 1206.9790279080626 Accuracy: tensor(0.9517, device='cuda:0')\n",
      "Batch Loss: 1218.687902646372 Accuracy: tensor(0.9517, device='cuda:0')\n",
      "Batch Loss: 1232.6925051149447 Accuracy: tensor(0.9517, device='cuda:0')\n",
      "Batch Loss: 1249.7520385433454 Accuracy: tensor(0.9515, device='cuda:0')\n",
      "Batch Loss: 1263.214554146165 Accuracy: tensor(0.9516, device='cuda:0')\n",
      "Batch Loss: 1276.583922186168 Accuracy: tensor(0.9516, device='cuda:0')\n",
      "Batch Loss: 1288.853179480182 Accuracy: tensor(0.9517, device='cuda:0')\n",
      "Batch Loss: 1302.8161257079337 Accuracy: tensor(0.9517, device='cuda:0')\n",
      "Batch Loss: 1315.265003572451 Accuracy: tensor(0.9517, device='cuda:0')\n",
      "Batch Loss: 1329.7850458265748 Accuracy: tensor(0.9517, device='cuda:0')\n",
      "Batch Loss: 1346.735190430889 Accuracy: tensor(0.9516, device='cuda:0')\n",
      "Batch Loss: 1357.664146832889 Accuracy: tensor(0.9516, device='cuda:0')\n",
      "Batch Loss: 1374.3568289314862 Accuracy: tensor(0.9515, device='cuda:0')\n",
      "Batch Loss: 1386.6973115918227 Accuracy: tensor(0.9516, device='cuda:0')\n",
      "Batch Loss: 1400.6006549429148 Accuracy: tensor(0.9516, device='cuda:0')\n",
      "Batch Loss: 1412.931512646377 Accuracy: tensor(0.9516, device='cuda:0')\n",
      "Batch Loss: 1427.9648787504993 Accuracy: tensor(0.9515, device='cuda:0')\n",
      "Batch Loss: 1443.2895316132344 Accuracy: tensor(0.9514, device='cuda:0')\n",
      "Batch Loss: 1456.3364755464718 Accuracy: tensor(0.9514, device='cuda:0')\n",
      "Batch Loss: 1467.9341678051278 Accuracy: tensor(0.9515, device='cuda:0')\n",
      "Batch Loss: 1483.8684905334376 Accuracy: tensor(0.9514, device='cuda:0')\n",
      "Batch Loss: 1498.5455115162767 Accuracy: tensor(0.9514, device='cuda:0')\n",
      "Batch Loss: 1513.1188056902029 Accuracy: tensor(0.9514, device='cuda:0')\n",
      "Batch Loss: 1524.5414718263783 Accuracy: tensor(0.9515, device='cuda:0')\n",
      "Batch Loss: 1537.9983972678892 Accuracy: tensor(0.9514, device='cuda:0')\n",
      "Batch Loss: 1556.3760261381976 Accuracy: tensor(0.9513, device='cuda:0')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch Loss: 1569.7228345558979 Accuracy: tensor(0.9512, device='cuda:0')\n",
      "Batch Loss: 1585.7783178910613 Accuracy: tensor(0.9512, device='cuda:0')\n",
      "Batch Loss: 1596.35533518577 Accuracy: tensor(0.9513, device='cuda:0')\n",
      "Batch Loss: 1611.5898187216371 Accuracy: tensor(0.9513, device='cuda:0')\n",
      "Batch Loss: 1624.2089816462249 Accuracy: tensor(0.9513, device='cuda:0')\n",
      "Batch Loss: 1640.0186712369323 Accuracy: tensor(0.9512, device='cuda:0')\n",
      "Batch Loss: 1653.4021547334269 Accuracy: tensor(0.9513, device='cuda:0')\n",
      "Batch Loss: 1668.3400461361744 Accuracy: tensor(0.9512, device='cuda:0')\n",
      "Batch Loss: 1681.8853479414247 Accuracy: tensor(0.9512, device='cuda:0')\n",
      "Batch Loss: 1697.38760387199 Accuracy: tensor(0.9512, device='cuda:0')\n",
      "Batch Loss: 1709.55822016811 Accuracy: tensor(0.9512, device='cuda:0')\n",
      "Batch Loss: 1722.9140025041997 Accuracy: tensor(0.9513, device='cuda:0')\n",
      "Batch Loss: 1738.2721437094733 Accuracy: tensor(0.9512, device='cuda:0')\n",
      "Batch Loss: 1751.7469689729623 Accuracy: tensor(0.9513, device='cuda:0')\n",
      "Batch Loss: 1761.8840117524378 Accuracy: tensor(0.9514, device='cuda:0')\n",
      "Train Loss: 1769.1256516035646 Accuracy: tensor(0.9513, device='cuda:0')\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a84707c6526e49b19f36dfeca694ae5c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/12944 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch Loss: 9.160263858735561 Accuracy: tensor(0.9675, device='cuda:0')\n",
      "Batch Loss: 19.095966510474682 Accuracy: tensor(0.9688, device='cuda:0')\n",
      "Batch Loss: 25.03695741156116 Accuracy: tensor(0.9725, device='cuda:0')\n",
      "Batch Loss: 33.199891278054565 Accuracy: tensor(0.9709, device='cuda:0')\n",
      "Batch Loss: 45.64072379376739 Accuracy: tensor(0.9700, device='cuda:0')\n",
      "Batch Loss: 53.271736808819696 Accuracy: tensor(0.9710, device='cuda:0')\n",
      "Batch Loss: 64.34646213217638 Accuracy: tensor(0.9696, device='cuda:0')\n",
      "Batch Loss: 70.96881410037167 Accuracy: tensor(0.9712, device='cuda:0')\n",
      "Batch Loss: 80.93796176998876 Accuracy: tensor(0.9703, device='cuda:0')\n",
      "Batch Loss: 90.2369861039333 Accuracy: tensor(0.9700, device='cuda:0')\n",
      "Batch Loss: 102.02562608290464 Accuracy: tensor(0.9699, device='cuda:0')\n",
      "Batch Loss: 108.69309343886562 Accuracy: tensor(0.9702, device='cuda:0')\n",
      "Batch Loss: 117.8297777872067 Accuracy: tensor(0.9703, device='cuda:0')\n",
      "Batch Loss: 127.38034621882252 Accuracy: tensor(0.9700, device='cuda:0')\n",
      "Batch Loss: 134.92501903674565 Accuracy: tensor(0.9707, device='cuda:0')\n",
      "Batch Loss: 142.68454633583315 Accuracy: tensor(0.9710, device='cuda:0')\n",
      "Batch Loss: 153.70688412245363 Accuracy: tensor(0.9706, device='cuda:0')\n",
      "Batch Loss: 163.76328002335504 Accuracy: tensor(0.9703, device='cuda:0')\n",
      "Batch Loss: 173.86466740514152 Accuracy: tensor(0.9697, device='cuda:0')\n",
      "Batch Loss: 184.4353893904481 Accuracy: tensor(0.9693, device='cuda:0')\n",
      "Batch Loss: 191.52804416557774 Accuracy: tensor(0.9696, device='cuda:0')\n",
      "Batch Loss: 199.95818288833834 Accuracy: tensor(0.9698, device='cuda:0')\n",
      "Batch Loss: 209.20699493098073 Accuracy: tensor(0.9699, device='cuda:0')\n",
      "Batch Loss: 219.70322992443107 Accuracy: tensor(0.9698, device='cuda:0')\n",
      "Batch Loss: 228.96091724326834 Accuracy: tensor(0.9695, device='cuda:0')\n",
      "Batch Loss: 238.33822419517674 Accuracy: tensor(0.9696, device='cuda:0')\n",
      "Batch Loss: 250.5725207712967 Accuracy: tensor(0.9694, device='cuda:0')\n",
      "Batch Loss: 262.00628211838193 Accuracy: tensor(0.9689, device='cuda:0')\n",
      "Batch Loss: 271.18641706649214 Accuracy: tensor(0.9689, device='cuda:0')\n",
      "Batch Loss: 283.42224539257586 Accuracy: tensor(0.9687, device='cuda:0')\n",
      "Batch Loss: 294.03782606171444 Accuracy: tensor(0.9684, device='cuda:0')\n",
      "Batch Loss: 306.07419263990596 Accuracy: tensor(0.9682, device='cuda:0')\n",
      "Batch Loss: 315.1372208399698 Accuracy: tensor(0.9684, device='cuda:0')\n",
      "Batch Loss: 327.6081418278627 Accuracy: tensor(0.9679, device='cuda:0')\n",
      "Batch Loss: 339.09346059453674 Accuracy: tensor(0.9676, device='cuda:0')\n",
      "Batch Loss: 348.6854919579346 Accuracy: tensor(0.9675, device='cuda:0')\n",
      "Batch Loss: 354.9273720725905 Accuracy: tensor(0.9676, device='cuda:0')\n",
      "Batch Loss: 363.7253968871664 Accuracy: tensor(0.9677, device='cuda:0')\n",
      "Batch Loss: 374.66645336383954 Accuracy: tensor(0.9675, device='cuda:0')\n",
      "Batch Loss: 384.0729070547968 Accuracy: tensor(0.9676, device='cuda:0')\n",
      "Batch Loss: 391.7260556819383 Accuracy: tensor(0.9680, device='cuda:0')\n",
      "Batch Loss: 401.9178820808884 Accuracy: tensor(0.9680, device='cuda:0')\n",
      "Batch Loss: 412.2301310875919 Accuracy: tensor(0.9681, device='cuda:0')\n",
      "Batch Loss: 424.5091647526715 Accuracy: tensor(0.9680, device='cuda:0')\n",
      "Batch Loss: 433.9220260141883 Accuracy: tensor(0.9680, device='cuda:0')\n",
      "Batch Loss: 448.51694935397245 Accuracy: tensor(0.9677, device='cuda:0')\n",
      "Batch Loss: 459.28667856170796 Accuracy: tensor(0.9676, device='cuda:0')\n",
      "Batch Loss: 470.08861684030853 Accuracy: tensor(0.9674, device='cuda:0')\n",
      "Batch Loss: 481.6497933526989 Accuracy: tensor(0.9673, device='cuda:0')\n",
      "Batch Loss: 491.5883405387867 Accuracy: tensor(0.9674, device='cuda:0')\n",
      "Batch Loss: 500.9574691436719 Accuracy: tensor(0.9674, device='cuda:0')\n",
      "Batch Loss: 511.12500914605334 Accuracy: tensor(0.9673, device='cuda:0')\n",
      "Batch Loss: 519.5317922169343 Accuracy: tensor(0.9674, device='cuda:0')\n",
      "Batch Loss: 526.1307884226553 Accuracy: tensor(0.9676, device='cuda:0')\n",
      "Batch Loss: 537.2442070464604 Accuracy: tensor(0.9675, device='cuda:0')\n",
      "Batch Loss: 546.7663042582572 Accuracy: tensor(0.9675, device='cuda:0')\n",
      "Batch Loss: 554.5207671332173 Accuracy: tensor(0.9675, device='cuda:0')\n",
      "Batch Loss: 564.1479487993056 Accuracy: tensor(0.9675, device='cuda:0')\n",
      "Batch Loss: 575.1104054917814 Accuracy: tensor(0.9674, device='cuda:0')\n",
      "Batch Loss: 586.3779344059294 Accuracy: tensor(0.9672, device='cuda:0')\n",
      "Batch Loss: 598.292663893546 Accuracy: tensor(0.9671, device='cuda:0')\n",
      "Batch Loss: 606.8675730441464 Accuracy: tensor(0.9672, device='cuda:0')\n",
      "Batch Loss: 616.7828776758397 Accuracy: tensor(0.9671, device='cuda:0')\n",
      "Batch Loss: 627.0758140099933 Accuracy: tensor(0.9672, device='cuda:0')\n",
      "Batch Loss: 635.507646437618 Accuracy: tensor(0.9672, device='cuda:0')\n",
      "Batch Loss: 648.3445088675944 Accuracy: tensor(0.9671, device='cuda:0')\n",
      "Batch Loss: 662.381558176945 Accuracy: tensor(0.9669, device='cuda:0')\n",
      "Batch Loss: 674.5107343344716 Accuracy: tensor(0.9668, device='cuda:0')\n",
      "Batch Loss: 684.2876691179117 Accuracy: tensor(0.9668, device='cuda:0')\n",
      "Batch Loss: 695.7578842056682 Accuracy: tensor(0.9668, device='cuda:0')\n",
      "Batch Loss: 706.4032631557202 Accuracy: tensor(0.9668, device='cuda:0')\n",
      "Batch Loss: 714.0575582933379 Accuracy: tensor(0.9669, device='cuda:0')\n",
      "Batch Loss: 724.8810990903294 Accuracy: tensor(0.9668, device='cuda:0')\n",
      "Batch Loss: 735.319204899366 Accuracy: tensor(0.9668, device='cuda:0')\n",
      "Batch Loss: 745.0406478539808 Accuracy: tensor(0.9668, device='cuda:0')\n",
      "Batch Loss: 755.6582779044984 Accuracy: tensor(0.9668, device='cuda:0')\n",
      "Batch Loss: 764.7440671020886 Accuracy: tensor(0.9668, device='cuda:0')\n",
      "Batch Loss: 775.6182698038174 Accuracy: tensor(0.9667, device='cuda:0')\n",
      "Batch Loss: 784.6043732248945 Accuracy: tensor(0.9668, device='cuda:0')\n",
      "Batch Loss: 795.3663995139068 Accuracy: tensor(0.9669, device='cuda:0')\n",
      "Batch Loss: 805.9825231836876 Accuracy: tensor(0.9668, device='cuda:0')\n",
      "Batch Loss: 819.4812885819701 Accuracy: tensor(0.9666, device='cuda:0')\n",
      "Batch Loss: 829.977573253098 Accuracy: tensor(0.9667, device='cuda:0')\n",
      "Batch Loss: 839.1612336359685 Accuracy: tensor(0.9667, device='cuda:0')\n",
      "Batch Loss: 849.2108758770628 Accuracy: tensor(0.9668, device='cuda:0')\n",
      "Batch Loss: 858.9727691299049 Accuracy: tensor(0.9668, device='cuda:0')\n",
      "Batch Loss: 869.7270469659707 Accuracy: tensor(0.9667, device='cuda:0')\n",
      "Batch Loss: 876.5330225160578 Accuracy: tensor(0.9668, device='cuda:0')\n",
      "Batch Loss: 886.3601641639834 Accuracy: tensor(0.9668, device='cuda:0')\n",
      "Batch Loss: 896.5078826051904 Accuracy: tensor(0.9668, device='cuda:0')\n",
      "Batch Loss: 903.7765067139408 Accuracy: tensor(0.9669, device='cuda:0')\n",
      "Batch Loss: 911.5582575859735 Accuracy: tensor(0.9670, device='cuda:0')\n",
      "Batch Loss: 923.1808905327925 Accuracy: tensor(0.9670, device='cuda:0')\n",
      "Batch Loss: 935.2394539659144 Accuracy: tensor(0.9669, device='cuda:0')\n",
      "Batch Loss: 945.1043706586352 Accuracy: tensor(0.9670, device='cuda:0')\n",
      "Batch Loss: 955.8681413050508 Accuracy: tensor(0.9669, device='cuda:0')\n",
      "Batch Loss: 964.4799972659675 Accuracy: tensor(0.9669, device='cuda:0')\n",
      "Batch Loss: 973.9016782975523 Accuracy: tensor(0.9670, device='cuda:0')\n",
      "Batch Loss: 985.8654941954883 Accuracy: tensor(0.9669, device='cuda:0')\n",
      "Batch Loss: 994.6870849650586 Accuracy: tensor(0.9669, device='cuda:0')\n",
      "Batch Loss: 1006.0066116451053 Accuracy: tensor(0.9669, device='cuda:0')\n",
      "Batch Loss: 1015.6340307641076 Accuracy: tensor(0.9669, device='cuda:0')\n",
      "Batch Loss: 1026.2608721113065 Accuracy: tensor(0.9668, device='cuda:0')\n",
      "Batch Loss: 1035.2456545067253 Accuracy: tensor(0.9669, device='cuda:0')\n",
      "Batch Loss: 1045.113231242285 Accuracy: tensor(0.9668, device='cuda:0')\n",
      "Batch Loss: 1055.4194926192285 Accuracy: tensor(0.9667, device='cuda:0')\n",
      "Batch Loss: 1064.3787723196438 Accuracy: tensor(0.9668, device='cuda:0')\n",
      "Batch Loss: 1070.2021997018019 Accuracy: tensor(0.9669, device='cuda:0')\n",
      "Batch Loss: 1080.7744398355717 Accuracy: tensor(0.9668, device='cuda:0')\n",
      "Batch Loss: 1087.8000916679157 Accuracy: tensor(0.9669, device='cuda:0')\n",
      "Batch Loss: 1096.5649279945064 Accuracy: tensor(0.9669, device='cuda:0')\n",
      "Batch Loss: 1108.5660474298056 Accuracy: tensor(0.9668, device='cuda:0')\n",
      "Batch Loss: 1119.04424321861 Accuracy: tensor(0.9668, device='cuda:0')\n",
      "Batch Loss: 1129.056435051607 Accuracy: tensor(0.9667, device='cuda:0')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch Loss: 1139.6832900014706 Accuracy: tensor(0.9667, device='cuda:0')\n",
      "Batch Loss: 1151.764185778331 Accuracy: tensor(0.9666, device='cuda:0')\n",
      "Batch Loss: 1160.8640633160248 Accuracy: tensor(0.9666, device='cuda:0')\n",
      "Batch Loss: 1172.6023992036935 Accuracy: tensor(0.9666, device='cuda:0')\n",
      "Batch Loss: 1181.200078725582 Accuracy: tensor(0.9666, device='cuda:0')\n",
      "Batch Loss: 1193.3564672180219 Accuracy: tensor(0.9666, device='cuda:0')\n",
      "Batch Loss: 1202.4439393890789 Accuracy: tensor(0.9666, device='cuda:0')\n",
      "Batch Loss: 1211.6446701706154 Accuracy: tensor(0.9666, device='cuda:0')\n",
      "Batch Loss: 1220.1096831386676 Accuracy: tensor(0.9666, device='cuda:0')\n",
      "Batch Loss: 1226.0412374060834 Accuracy: tensor(0.9667, device='cuda:0')\n",
      "Batch Loss: 1234.1567902691895 Accuracy: tensor(0.9667, device='cuda:0')\n",
      "Batch Loss: 1241.8400617671432 Accuracy: tensor(0.9667, device='cuda:0')\n",
      "Batch Loss: 1252.955189191387 Accuracy: tensor(0.9667, device='cuda:0')\n",
      "Batch Loss: 1269.2486194932135 Accuracy: tensor(0.9665, device='cuda:0')\n",
      "Batch Loss: 1277.9965319955954 Accuracy: tensor(0.9666, device='cuda:0')\n",
      "Train Loss: 1282.1287527555833 Accuracy: tensor(0.9666, device='cuda:0')\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75e997aa7ba74651a921fd796b80dda5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/12944 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch Loss: 6.1399983008159325 Accuracy: tensor(0.9812, device='cuda:0')\n",
      "Batch Loss: 12.481002396787517 Accuracy: tensor(0.9806, device='cuda:0')\n",
      "Batch Loss: 17.537441864260472 Accuracy: tensor(0.9829, device='cuda:0')\n",
      "Batch Loss: 25.415259276982397 Accuracy: tensor(0.9809, device='cuda:0')\n",
      "Batch Loss: 34.72425902890973 Accuracy: tensor(0.9793, device='cuda:0')\n",
      "Batch Loss: 42.31078163161874 Accuracy: tensor(0.9788, device='cuda:0')\n",
      "Batch Loss: 49.13722321530804 Accuracy: tensor(0.9780, device='cuda:0')\n",
      "Batch Loss: 54.804958311258815 Accuracy: tensor(0.9783, device='cuda:0')\n",
      "Batch Loss: 63.78942151495721 Accuracy: tensor(0.9779, device='cuda:0')\n",
      "Batch Loss: 69.01796310499776 Accuracy: tensor(0.9785, device='cuda:0')\n",
      "Batch Loss: 75.24204477586318 Accuracy: tensor(0.9783, device='cuda:0')\n",
      "Batch Loss: 83.91831491503399 Accuracy: tensor(0.9778, device='cuda:0')\n",
      "Batch Loss: 90.26184539229143 Accuracy: tensor(0.9775, device='cuda:0')\n",
      "Batch Loss: 98.22347160847858 Accuracy: tensor(0.9776, device='cuda:0')\n",
      "Batch Loss: 104.01975877163932 Accuracy: tensor(0.9778, device='cuda:0')\n",
      "Batch Loss: 110.75495462748222 Accuracy: tensor(0.9781, device='cuda:0')\n",
      "Batch Loss: 116.85739531484433 Accuracy: tensor(0.9782, device='cuda:0')\n",
      "Batch Loss: 123.4570300981868 Accuracy: tensor(0.9783, device='cuda:0')\n",
      "Batch Loss: 132.5726943835616 Accuracy: tensor(0.9781, device='cuda:0')\n",
      "Batch Loss: 140.55068873730488 Accuracy: tensor(0.9779, device='cuda:0')\n",
      "Batch Loss: 146.46426380239427 Accuracy: tensor(0.9778, device='cuda:0')\n",
      "Batch Loss: 153.1208903370425 Accuracy: tensor(0.9780, device='cuda:0')\n",
      "Batch Loss: 161.12413100968115 Accuracy: tensor(0.9777, device='cuda:0')\n",
      "Batch Loss: 166.69030654034577 Accuracy: tensor(0.9778, device='cuda:0')\n",
      "Batch Loss: 173.2344813736854 Accuracy: tensor(0.9778, device='cuda:0')\n",
      "Batch Loss: 181.4196664848132 Accuracy: tensor(0.9777, device='cuda:0')\n",
      "Batch Loss: 190.64044927235227 Accuracy: tensor(0.9774, device='cuda:0')\n",
      "Batch Loss: 197.76684675051365 Accuracy: tensor(0.9773, device='cuda:0')\n",
      "Batch Loss: 206.85335521155503 Accuracy: tensor(0.9772, device='cuda:0')\n",
      "Batch Loss: 214.77797779988032 Accuracy: tensor(0.9770, device='cuda:0')\n",
      "Batch Loss: 222.86565350252204 Accuracy: tensor(0.9770, device='cuda:0')\n",
      "Batch Loss: 228.86261855869088 Accuracy: tensor(0.9770, device='cuda:0')\n",
      "Batch Loss: 234.87403097271454 Accuracy: tensor(0.9771, device='cuda:0')\n",
      "Batch Loss: 238.7861748103751 Accuracy: tensor(0.9774, device='cuda:0')\n",
      "Batch Loss: 246.9708095852984 Accuracy: tensor(0.9774, device='cuda:0')\n",
      "Batch Loss: 252.2464963918319 Accuracy: tensor(0.9776, device='cuda:0')\n",
      "Batch Loss: 257.9172999340808 Accuracy: tensor(0.9778, device='cuda:0')\n",
      "Batch Loss: 263.5030560067389 Accuracy: tensor(0.9780, device='cuda:0')\n",
      "Batch Loss: 271.41761617641896 Accuracy: tensor(0.9779, device='cuda:0')\n",
      "Batch Loss: 276.8775700406404 Accuracy: tensor(0.9783, device='cuda:0')\n",
      "Batch Loss: 287.23491067194846 Accuracy: tensor(0.9780, device='cuda:0')\n",
      "Batch Loss: 292.28259911423083 Accuracy: tensor(0.9781, device='cuda:0')\n",
      "Batch Loss: 297.8985346372938 Accuracy: tensor(0.9783, device='cuda:0')\n",
      "Batch Loss: 304.75061527465004 Accuracy: tensor(0.9782, device='cuda:0')\n",
      "Batch Loss: 314.73896004550625 Accuracy: tensor(0.9781, device='cuda:0')\n",
      "Batch Loss: 323.9074349292787 Accuracy: tensor(0.9779, device='cuda:0')\n",
      "Batch Loss: 333.15379157091957 Accuracy: tensor(0.9778, device='cuda:0')\n",
      "Batch Loss: 342.78775796003174 Accuracy: tensor(0.9776, device='cuda:0')\n",
      "Batch Loss: 350.06979626312386 Accuracy: tensor(0.9776, device='cuda:0')\n",
      "Batch Loss: 357.08627260581125 Accuracy: tensor(0.9775, device='cuda:0')\n",
      "Batch Loss: 363.91870673850644 Accuracy: tensor(0.9775, device='cuda:0')\n",
      "Batch Loss: 369.89639238466043 Accuracy: tensor(0.9776, device='cuda:0')\n",
      "Batch Loss: 377.16660892253276 Accuracy: tensor(0.9775, device='cuda:0')\n",
      "Batch Loss: 383.86576441896614 Accuracy: tensor(0.9775, device='cuda:0')\n",
      "Batch Loss: 392.30740169563796 Accuracy: tensor(0.9775, device='cuda:0')\n",
      "Batch Loss: 400.4608641163213 Accuracy: tensor(0.9775, device='cuda:0')\n",
      "Batch Loss: 406.6554263996659 Accuracy: tensor(0.9775, device='cuda:0')\n",
      "Batch Loss: 412.4105466508772 Accuracy: tensor(0.9775, device='cuda:0')\n",
      "Batch Loss: 418.5819983116817 Accuracy: tensor(0.9774, device='cuda:0')\n",
      "Batch Loss: 425.8906540289754 Accuracy: tensor(0.9774, device='cuda:0')\n",
      "Batch Loss: 434.4742332495516 Accuracy: tensor(0.9774, device='cuda:0')\n",
      "Batch Loss: 438.1419788101921 Accuracy: tensor(0.9774, device='cuda:0')\n",
      "Batch Loss: 444.2739043906913 Accuracy: tensor(0.9775, device='cuda:0')\n",
      "Batch Loss: 450.6837970990455 Accuracy: tensor(0.9776, device='cuda:0')\n",
      "Batch Loss: 457.03155493887607 Accuracy: tensor(0.9775, device='cuda:0')\n",
      "Batch Loss: 465.9794504995225 Accuracy: tensor(0.9774, device='cuda:0')\n",
      "Batch Loss: 475.19416711409576 Accuracy: tensor(0.9773, device='cuda:0')\n",
      "Batch Loss: 484.25281187915243 Accuracy: tensor(0.9772, device='cuda:0')\n",
      "Batch Loss: 493.0988875925541 Accuracy: tensor(0.9772, device='cuda:0')\n",
      "Batch Loss: 501.424057519529 Accuracy: tensor(0.9771, device='cuda:0')\n",
      "Batch Loss: 507.41142592730466 Accuracy: tensor(0.9772, device='cuda:0')\n",
      "Batch Loss: 516.6056709607365 Accuracy: tensor(0.9771, device='cuda:0')\n",
      "Batch Loss: 526.0112072905758 Accuracy: tensor(0.9771, device='cuda:0')\n",
      "Batch Loss: 536.4183806447545 Accuracy: tensor(0.9769, device='cuda:0')\n",
      "Batch Loss: 543.828290993697 Accuracy: tensor(0.9768, device='cuda:0')\n",
      "Batch Loss: 550.8789808755973 Accuracy: tensor(0.9768, device='cuda:0')\n",
      "Batch Loss: 556.645559492521 Accuracy: tensor(0.9769, device='cuda:0')\n",
      "Batch Loss: 562.694832950132 Accuracy: tensor(0.9769, device='cuda:0')\n",
      "Batch Loss: 570.3926867529517 Accuracy: tensor(0.9769, device='cuda:0')\n",
      "Batch Loss: 580.0283566886792 Accuracy: tensor(0.9768, device='cuda:0')\n",
      "Batch Loss: 586.4760806743288 Accuracy: tensor(0.9768, device='cuda:0')\n",
      "Batch Loss: 593.5740299010649 Accuracy: tensor(0.9768, device='cuda:0')\n",
      "Batch Loss: 600.2181423662696 Accuracy: tensor(0.9769, device='cuda:0')\n",
      "Batch Loss: 608.5374625930563 Accuracy: tensor(0.9768, device='cuda:0')\n",
      "Batch Loss: 615.8631401569583 Accuracy: tensor(0.9768, device='cuda:0')\n",
      "Batch Loss: 626.7990662369411 Accuracy: tensor(0.9767, device='cuda:0')\n",
      "Batch Loss: 634.1420967336744 Accuracy: tensor(0.9767, device='cuda:0')\n",
      "Batch Loss: 641.0934543081094 Accuracy: tensor(0.9766, device='cuda:0')\n",
      "Batch Loss: 650.001326235244 Accuracy: tensor(0.9765, device='cuda:0')\n",
      "Batch Loss: 656.0381864274386 Accuracy: tensor(0.9766, device='cuda:0')\n",
      "Batch Loss: 665.8736102711409 Accuracy: tensor(0.9765, device='cuda:0')\n",
      "Batch Loss: 674.7779666935094 Accuracy: tensor(0.9765, device='cuda:0')\n",
      "Batch Loss: 686.2251676928718 Accuracy: tensor(0.9764, device='cuda:0')\n",
      "Batch Loss: 694.6409630726557 Accuracy: tensor(0.9763, device='cuda:0')\n",
      "Batch Loss: 704.8283337801695 Accuracy: tensor(0.9762, device='cuda:0')\n",
      "Batch Loss: 712.3686605563853 Accuracy: tensor(0.9762, device='cuda:0')\n",
      "Batch Loss: 719.7971117021516 Accuracy: tensor(0.9762, device='cuda:0')\n",
      "Batch Loss: 727.9054248251487 Accuracy: tensor(0.9762, device='cuda:0')\n",
      "Batch Loss: 737.8348810323514 Accuracy: tensor(0.9762, device='cuda:0')\n",
      "Batch Loss: 745.6258158178534 Accuracy: tensor(0.9761, device='cuda:0')\n",
      "Batch Loss: 752.7652247559745 Accuracy: tensor(0.9761, device='cuda:0')\n",
      "Batch Loss: 760.461794763105 Accuracy: tensor(0.9762, device='cuda:0')\n",
      "Batch Loss: 768.1321216367651 Accuracy: tensor(0.9762, device='cuda:0')\n",
      "Batch Loss: 777.9830116254743 Accuracy: tensor(0.9762, device='cuda:0')\n",
      "Batch Loss: 785.9581960164942 Accuracy: tensor(0.9762, device='cuda:0')\n",
      "Batch Loss: 795.3987644049339 Accuracy: tensor(0.9761, device='cuda:0')\n",
      "Batch Loss: 802.5269026409369 Accuracy: tensor(0.9761, device='cuda:0')\n",
      "Batch Loss: 810.537176029291 Accuracy: tensor(0.9761, device='cuda:0')\n",
      "Batch Loss: 815.3535446093883 Accuracy: tensor(0.9762, device='cuda:0')\n",
      "Batch Loss: 824.9901683666976 Accuracy: tensor(0.9762, device='cuda:0')\n",
      "Batch Loss: 833.990541451727 Accuracy: tensor(0.9761, device='cuda:0')\n",
      "Batch Loss: 842.6918586628744 Accuracy: tensor(0.9761, device='cuda:0')\n",
      "Batch Loss: 851.0583504476817 Accuracy: tensor(0.9761, device='cuda:0')\n",
      "Batch Loss: 856.9913370086579 Accuracy: tensor(0.9762, device='cuda:0')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch Loss: 865.8357091328362 Accuracy: tensor(0.9761, device='cuda:0')\n",
      "Batch Loss: 876.5898690837203 Accuracy: tensor(0.9760, device='cuda:0')\n",
      "Batch Loss: 884.6039043617202 Accuracy: tensor(0.9760, device='cuda:0')\n",
      "Batch Loss: 894.5767881040229 Accuracy: tensor(0.9759, device='cuda:0')\n",
      "Batch Loss: 905.464553981903 Accuracy: tensor(0.9759, device='cuda:0')\n",
      "Batch Loss: 914.0700443888782 Accuracy: tensor(0.9758, device='cuda:0')\n",
      "Batch Loss: 924.382812136435 Accuracy: tensor(0.9757, device='cuda:0')\n",
      "Batch Loss: 930.1587511216057 Accuracy: tensor(0.9757, device='cuda:0')\n",
      "Batch Loss: 939.3890593297547 Accuracy: tensor(0.9756, device='cuda:0')\n",
      "Batch Loss: 944.7635682959808 Accuracy: tensor(0.9757, device='cuda:0')\n",
      "Batch Loss: 951.627626960515 Accuracy: tensor(0.9758, device='cuda:0')\n",
      "Batch Loss: 958.1722001921153 Accuracy: tensor(0.9758, device='cuda:0')\n",
      "Batch Loss: 968.3972233451204 Accuracy: tensor(0.9757, device='cuda:0')\n",
      "Batch Loss: 977.8448246595217 Accuracy: tensor(0.9757, device='cuda:0')\n",
      "Batch Loss: 985.4204200968379 Accuracy: tensor(0.9756, device='cuda:0')\n",
      "Train Loss: 988.2380357532529 Accuracy: tensor(0.9756, device='cuda:0')\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7878bad698944e109c3124a39b5a0854",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/12944 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch Loss: 5.852655531605706 Accuracy: tensor(0.9800, device='cuda:0')\n",
      "Batch Loss: 13.71093392977491 Accuracy: tensor(0.9769, device='cuda:0')\n",
      "Batch Loss: 18.939179955865256 Accuracy: tensor(0.9792, device='cuda:0')\n",
      "Batch Loss: 23.183268115390092 Accuracy: tensor(0.9812, device='cuda:0')\n",
      "Batch Loss: 27.57419253757689 Accuracy: tensor(0.9818, device='cuda:0')\n",
      "Batch Loss: 32.856477559544146 Accuracy: tensor(0.9815, device='cuda:0')\n",
      "Batch Loss: 41.143200217047706 Accuracy: tensor(0.9805, device='cuda:0')\n",
      "Batch Loss: 45.86200051987544 Accuracy: tensor(0.9809, device='cuda:0')\n",
      "Batch Loss: 50.19208702258766 Accuracy: tensor(0.9815, device='cuda:0')\n",
      "Batch Loss: 56.018194254953414 Accuracy: tensor(0.9813, device='cuda:0')\n",
      "Batch Loss: 60.8090911176987 Accuracy: tensor(0.9819, device='cuda:0')\n",
      "Batch Loss: 67.74303884082474 Accuracy: tensor(0.9817, device='cuda:0')\n",
      "Batch Loss: 74.62837603269145 Accuracy: tensor(0.9815, device='cuda:0')\n",
      "Batch Loss: 79.20470863941591 Accuracy: tensor(0.9818, device='cuda:0')\n",
      "Batch Loss: 85.22987855307292 Accuracy: tensor(0.9816, device='cuda:0')\n",
      "Batch Loss: 91.21194099320564 Accuracy: tensor(0.9816, device='cuda:0')\n",
      "Batch Loss: 98.30054915498476 Accuracy: tensor(0.9812, device='cuda:0')\n",
      "Batch Loss: 103.79720427829307 Accuracy: tensor(0.9810, device='cuda:0')\n",
      "Batch Loss: 107.79380860470701 Accuracy: tensor(0.9814, device='cuda:0')\n",
      "Batch Loss: 117.59784403908998 Accuracy: tensor(0.9809, device='cuda:0')\n",
      "Batch Loss: 123.12694255495444 Accuracy: tensor(0.9812, device='cuda:0')\n",
      "Batch Loss: 130.53942233067937 Accuracy: tensor(0.9811, device='cuda:0')\n",
      "Batch Loss: 138.22627532016486 Accuracy: tensor(0.9813, device='cuda:0')\n",
      "Batch Loss: 142.36632615025155 Accuracy: tensor(0.9815, device='cuda:0')\n",
      "Batch Loss: 146.2879714887822 Accuracy: tensor(0.9817, device='cuda:0')\n",
      "Batch Loss: 150.4723472307669 Accuracy: tensor(0.9820, device='cuda:0')\n",
      "Batch Loss: 156.99316818767693 Accuracy: tensor(0.9820, device='cuda:0')\n",
      "Batch Loss: 161.76111300161574 Accuracy: tensor(0.9821, device='cuda:0')\n",
      "Batch Loss: 167.51118319912348 Accuracy: tensor(0.9822, device='cuda:0')\n",
      "Batch Loss: 171.18009542871732 Accuracy: tensor(0.9826, device='cuda:0')\n",
      "Batch Loss: 177.42432007624302 Accuracy: tensor(0.9825, device='cuda:0')\n",
      "Batch Loss: 184.80578057235107 Accuracy: tensor(0.9823, device='cuda:0')\n",
      "Batch Loss: 188.65351338800974 Accuracy: tensor(0.9825, device='cuda:0')\n",
      "Batch Loss: 192.86356215470005 Accuracy: tensor(0.9825, device='cuda:0')\n",
      "Batch Loss: 199.36547377787065 Accuracy: tensor(0.9825, device='cuda:0')\n",
      "Batch Loss: 205.86785986402538 Accuracy: tensor(0.9824, device='cuda:0')\n",
      "Batch Loss: 212.7937869593734 Accuracy: tensor(0.9822, device='cuda:0')\n",
      "Batch Loss: 217.12456632312387 Accuracy: tensor(0.9821, device='cuda:0')\n",
      "Batch Loss: 223.2934611266246 Accuracy: tensor(0.9821, device='cuda:0')\n",
      "Batch Loss: 229.17880893545225 Accuracy: tensor(0.9821, device='cuda:0')\n",
      "Batch Loss: 232.97850297490368 Accuracy: tensor(0.9823, device='cuda:0')\n",
      "Batch Loss: 240.85985576239182 Accuracy: tensor(0.9820, device='cuda:0')\n",
      "Batch Loss: 247.52809623169014 Accuracy: tensor(0.9819, device='cuda:0')\n",
      "Batch Loss: 250.67545321636135 Accuracy: tensor(0.9821, device='cuda:0')\n",
      "Batch Loss: 256.9060701284907 Accuracy: tensor(0.9822, device='cuda:0')\n",
      "Batch Loss: 263.0012538366136 Accuracy: tensor(0.9821, device='cuda:0')\n",
      "Batch Loss: 272.77856551861623 Accuracy: tensor(0.9818, device='cuda:0')\n",
      "Batch Loss: 279.87104410497705 Accuracy: tensor(0.9817, device='cuda:0')\n",
      "Batch Loss: 284.85546947520925 Accuracy: tensor(0.9817, device='cuda:0')\n",
      "Batch Loss: 291.3781007467187 Accuracy: tensor(0.9816, device='cuda:0')\n",
      "Batch Loss: 297.34334598941496 Accuracy: tensor(0.9817, device='cuda:0')\n",
      "Batch Loss: 305.0241233596462 Accuracy: tensor(0.9816, device='cuda:0')\n",
      "Batch Loss: 310.6408556608367 Accuracy: tensor(0.9817, device='cuda:0')\n",
      "Batch Loss: 314.8528092969791 Accuracy: tensor(0.9818, device='cuda:0')\n",
      "Batch Loss: 319.9441419961513 Accuracy: tensor(0.9818, device='cuda:0')\n",
      "Batch Loss: 325.92241380998166 Accuracy: tensor(0.9818, device='cuda:0')\n",
      "Batch Loss: 330.47700919694034 Accuracy: tensor(0.9818, device='cuda:0')\n",
      "Batch Loss: 336.6031033665058 Accuracy: tensor(0.9819, device='cuda:0')\n",
      "Batch Loss: 342.74582518666284 Accuracy: tensor(0.9818, device='cuda:0')\n",
      "Batch Loss: 349.04508148663444 Accuracy: tensor(0.9817, device='cuda:0')\n",
      "Batch Loss: 357.89532546157716 Accuracy: tensor(0.9816, device='cuda:0')\n",
      "Batch Loss: 363.1028316761949 Accuracy: tensor(0.9816, device='cuda:0')\n",
      "Batch Loss: 366.667876481486 Accuracy: tensor(0.9817, device='cuda:0')\n",
      "Batch Loss: 373.26750010665273 Accuracy: tensor(0.9817, device='cuda:0')\n",
      "Batch Loss: 377.79654404806206 Accuracy: tensor(0.9817, device='cuda:0')\n",
      "Batch Loss: 383.6438959586085 Accuracy: tensor(0.9817, device='cuda:0')\n",
      "Batch Loss: 388.83408955758205 Accuracy: tensor(0.9818, device='cuda:0')\n",
      "Batch Loss: 394.9423612380051 Accuracy: tensor(0.9818, device='cuda:0')\n",
      "Batch Loss: 399.9773410935304 Accuracy: tensor(0.9818, device='cuda:0')\n",
      "Batch Loss: 405.0928849003394 Accuracy: tensor(0.9818, device='cuda:0')\n",
      "Batch Loss: 411.91026031557703 Accuracy: tensor(0.9817, device='cuda:0')\n",
      "Batch Loss: 420.0161270525423 Accuracy: tensor(0.9816, device='cuda:0')\n",
      "Batch Loss: 426.8298477407661 Accuracy: tensor(0.9816, device='cuda:0')\n",
      "Batch Loss: 431.7097046620329 Accuracy: tensor(0.9816, device='cuda:0')\n",
      "Batch Loss: 435.66983282082947 Accuracy: tensor(0.9818, device='cuda:0')\n",
      "Batch Loss: 442.5040712429327 Accuracy: tensor(0.9816, device='cuda:0')\n",
      "Batch Loss: 448.8369684550562 Accuracy: tensor(0.9817, device='cuda:0')\n",
      "Batch Loss: 455.6787624362041 Accuracy: tensor(0.9817, device='cuda:0')\n",
      "Batch Loss: 460.70111214375356 Accuracy: tensor(0.9817, device='cuda:0')\n",
      "Batch Loss: 469.7291808236041 Accuracy: tensor(0.9815, device='cuda:0')\n",
      "Batch Loss: 480.6206185017363 Accuracy: tensor(0.9812, device='cuda:0')\n",
      "Batch Loss: 496.4009969942854 Accuracy: tensor(0.9807, device='cuda:0')\n",
      "Batch Loss: 507.4018516233773 Accuracy: tensor(0.9805, device='cuda:0')\n",
      "Batch Loss: 513.0430048849084 Accuracy: tensor(0.9806, device='cuda:0')\n",
      "Batch Loss: 518.248288606701 Accuracy: tensor(0.9806, device='cuda:0')\n",
      "Batch Loss: 524.2044614158222 Accuracy: tensor(0.9806, device='cuda:0')\n",
      "Batch Loss: 529.9601817899966 Accuracy: tensor(0.9807, device='cuda:0')\n",
      "Batch Loss: 535.4648301993147 Accuracy: tensor(0.9807, device='cuda:0')\n",
      "Batch Loss: 543.1808757993276 Accuracy: tensor(0.9807, device='cuda:0')\n",
      "Batch Loss: 547.3731835057843 Accuracy: tensor(0.9807, device='cuda:0')\n",
      "Batch Loss: 550.6898788407561 Accuracy: tensor(0.9808, device='cuda:0')\n",
      "Batch Loss: 559.5466684917337 Accuracy: tensor(0.9808, device='cuda:0')\n",
      "Batch Loss: 567.0806175402249 Accuracy: tensor(0.9807, device='cuda:0')\n",
      "Batch Loss: 573.7525016692816 Accuracy: tensor(0.9807, device='cuda:0')\n",
      "Batch Loss: 579.1749960572342 Accuracy: tensor(0.9807, device='cuda:0')\n",
      "Batch Loss: 586.2487442930578 Accuracy: tensor(0.9808, device='cuda:0')\n",
      "Batch Loss: 593.4592883858713 Accuracy: tensor(0.9807, device='cuda:0')\n",
      "Batch Loss: 602.4554497249774 Accuracy: tensor(0.9806, device='cuda:0')\n",
      "Batch Loss: 609.7125010592281 Accuracy: tensor(0.9806, device='cuda:0')\n",
      "Batch Loss: 613.6400584648945 Accuracy: tensor(0.9806, device='cuda:0')\n",
      "Batch Loss: 621.1741164697451 Accuracy: tensor(0.9806, device='cuda:0')\n",
      "Batch Loss: 626.0667601365712 Accuracy: tensor(0.9806, device='cuda:0')\n",
      "Batch Loss: 632.0403087399318 Accuracy: tensor(0.9806, device='cuda:0')\n",
      "Batch Loss: 637.9704156427761 Accuracy: tensor(0.9806, device='cuda:0')\n",
      "Batch Loss: 645.1356333420263 Accuracy: tensor(0.9806, device='cuda:0')\n",
      "Batch Loss: 652.166718438908 Accuracy: tensor(0.9805, device='cuda:0')\n",
      "Batch Loss: 658.398097193276 Accuracy: tensor(0.9805, device='cuda:0')\n",
      "Batch Loss: 662.4216553052538 Accuracy: tensor(0.9805, device='cuda:0')\n",
      "Batch Loss: 669.3622480747872 Accuracy: tensor(0.9805, device='cuda:0')\n",
      "Batch Loss: 674.6057208878337 Accuracy: tensor(0.9806, device='cuda:0')\n",
      "Batch Loss: 680.5947028712253 Accuracy: tensor(0.9805, device='cuda:0')\n",
      "Batch Loss: 687.5997631968348 Accuracy: tensor(0.9805, device='cuda:0')\n",
      "Batch Loss: 694.5108952491428 Accuracy: tensor(0.9805, device='cuda:0')\n",
      "Batch Loss: 702.6540768693085 Accuracy: tensor(0.9805, device='cuda:0')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch Loss: 709.6843005364644 Accuracy: tensor(0.9804, device='cuda:0')\n",
      "Batch Loss: 716.3702172573539 Accuracy: tensor(0.9804, device='cuda:0')\n",
      "Batch Loss: 723.3104105949751 Accuracy: tensor(0.9804, device='cuda:0')\n",
      "Batch Loss: 730.6427946821204 Accuracy: tensor(0.9804, device='cuda:0')\n",
      "Batch Loss: 735.1909617237397 Accuracy: tensor(0.9804, device='cuda:0')\n",
      "Batch Loss: 744.0410575460992 Accuracy: tensor(0.9803, device='cuda:0')\n",
      "Batch Loss: 749.4889092170051 Accuracy: tensor(0.9803, device='cuda:0')\n",
      "Batch Loss: 754.4830842196825 Accuracy: tensor(0.9804, device='cuda:0')\n",
      "Batch Loss: 760.2359726301511 Accuracy: tensor(0.9804, device='cuda:0')\n",
      "Batch Loss: 770.0348377879127 Accuracy: tensor(0.9803, device='cuda:0')\n",
      "Batch Loss: 779.3866382280248 Accuracy: tensor(0.9803, device='cuda:0')\n",
      "Batch Loss: 785.8561215835507 Accuracy: tensor(0.9802, device='cuda:0')\n",
      "Batch Loss: 789.9500073363888 Accuracy: tensor(0.9803, device='cuda:0')\n",
      "Batch Loss: 797.7232310465188 Accuracy: tensor(0.9802, device='cuda:0')\n",
      "Batch Loss: 803.4769698911696 Accuracy: tensor(0.9802, device='cuda:0')\n",
      "Train Loss: 805.9719511288567 Accuracy: tensor(0.9802, device='cuda:0')\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d149a803f9af4396a5775b6e29cd6ad4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/12944 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch Loss: 5.058940358925611 Accuracy: tensor(0.9875, device='cuda:0')\n",
      "Batch Loss: 10.276217825012282 Accuracy: tensor(0.9869, device='cuda:0')\n",
      "Batch Loss: 15.239789778832346 Accuracy: tensor(0.9879, device='cuda:0')\n",
      "Batch Loss: 16.712170756072737 Accuracy: tensor(0.9900, device='cuda:0')\n",
      "Batch Loss: 20.892184171359986 Accuracy: tensor(0.9890, device='cuda:0')\n",
      "Batch Loss: 26.03598566353321 Accuracy: tensor(0.9881, device='cuda:0')\n",
      "Batch Loss: 29.030446673219558 Accuracy: tensor(0.9882, device='cuda:0')\n",
      "Batch Loss: 32.76918498094892 Accuracy: tensor(0.9883, device='cuda:0')\n",
      "Batch Loss: 37.152250258775894 Accuracy: tensor(0.9882, device='cuda:0')\n",
      "Batch Loss: 39.682063989457674 Accuracy: tensor(0.9885, device='cuda:0')\n",
      "Batch Loss: 47.35774250025861 Accuracy: tensor(0.9873, device='cuda:0')\n",
      "Batch Loss: 55.579300177516416 Accuracy: tensor(0.9865, device='cuda:0')\n",
      "Batch Loss: 64.62539831490722 Accuracy: tensor(0.9854, device='cuda:0')\n",
      "Batch Loss: 68.95276023715269 Accuracy: tensor(0.9853, device='cuda:0')\n",
      "Batch Loss: 72.82806738745421 Accuracy: tensor(0.9852, device='cuda:0')\n",
      "Batch Loss: 77.34279872372281 Accuracy: tensor(0.9852, device='cuda:0')\n",
      "Batch Loss: 82.89044754719362 Accuracy: tensor(0.9851, device='cuda:0')\n",
      "Batch Loss: 85.9537575643044 Accuracy: tensor(0.9853, device='cuda:0')\n",
      "Batch Loss: 91.71421567699872 Accuracy: tensor(0.9853, device='cuda:0')\n",
      "Batch Loss: 97.25491371541284 Accuracy: tensor(0.9853, device='cuda:0')\n",
      "Batch Loss: 101.68360478512477 Accuracy: tensor(0.9852, device='cuda:0')\n",
      "Batch Loss: 105.53264349966776 Accuracy: tensor(0.9853, device='cuda:0')\n",
      "Batch Loss: 112.06391562393401 Accuracy: tensor(0.9850, device='cuda:0')\n",
      "Batch Loss: 116.5656919577159 Accuracy: tensor(0.9851, device='cuda:0')\n",
      "Batch Loss: 119.75575701752678 Accuracy: tensor(0.9852, device='cuda:0')\n",
      "Batch Loss: 128.0771487801103 Accuracy: tensor(0.9849, device='cuda:0')\n",
      "Batch Loss: 132.22030169074424 Accuracy: tensor(0.9851, device='cuda:0')\n",
      "Batch Loss: 136.75744325038977 Accuracy: tensor(0.9850, device='cuda:0')\n",
      "Batch Loss: 140.77122912521008 Accuracy: tensor(0.9850, device='cuda:0')\n",
      "Batch Loss: 145.832814037567 Accuracy: tensor(0.9850, device='cuda:0')\n",
      "Batch Loss: 149.60814989125356 Accuracy: tensor(0.9851, device='cuda:0')\n",
      "Batch Loss: 157.1050438354141 Accuracy: tensor(0.9850, device='cuda:0')\n",
      "Batch Loss: 162.14438226766651 Accuracy: tensor(0.9849, device='cuda:0')\n",
      "Batch Loss: 168.6960269563715 Accuracy: tensor(0.9847, device='cuda:0')\n",
      "Batch Loss: 172.30920776090352 Accuracy: tensor(0.9849, device='cuda:0')\n",
      "Batch Loss: 175.3330580633483 Accuracy: tensor(0.9852, device='cuda:0')\n",
      "Batch Loss: 178.6289111454389 Accuracy: tensor(0.9853, device='cuda:0')\n",
      "Batch Loss: 183.01953138998942 Accuracy: tensor(0.9853, device='cuda:0')\n",
      "Batch Loss: 187.88392454275163 Accuracy: tensor(0.9853, device='cuda:0')\n",
      "Batch Loss: 194.80871628952445 Accuracy: tensor(0.9851, device='cuda:0')\n",
      "Batch Loss: 200.8813577494002 Accuracy: tensor(0.9849, device='cuda:0')\n",
      "Batch Loss: 206.36860171653097 Accuracy: tensor(0.9849, device='cuda:0')\n",
      "Batch Loss: 212.57516699418193 Accuracy: tensor(0.9848, device='cuda:0')\n",
      "Batch Loss: 216.7723782992107 Accuracy: tensor(0.9848, device='cuda:0')\n",
      "Batch Loss: 221.05901759694098 Accuracy: tensor(0.9849, device='cuda:0')\n",
      "Batch Loss: 225.27302122325636 Accuracy: tensor(0.9849, device='cuda:0')\n",
      "Batch Loss: 229.03371323924512 Accuracy: tensor(0.9851, device='cuda:0')\n",
      "Batch Loss: 232.97434459551005 Accuracy: tensor(0.9851, device='cuda:0')\n",
      "Batch Loss: 240.5124723611516 Accuracy: tensor(0.9851, device='cuda:0')\n",
      "Batch Loss: 244.04198159481166 Accuracy: tensor(0.9852, device='cuda:0')\n",
      "Batch Loss: 249.09299718419788 Accuracy: tensor(0.9852, device='cuda:0')\n",
      "Batch Loss: 252.34940903662937 Accuracy: tensor(0.9853, device='cuda:0')\n",
      "Batch Loss: 257.78873421472963 Accuracy: tensor(0.9852, device='cuda:0')\n",
      "Batch Loss: 262.67674543801695 Accuracy: tensor(0.9852, device='cuda:0')\n",
      "Batch Loss: 264.8162698780652 Accuracy: tensor(0.9854, device='cuda:0')\n",
      "Batch Loss: 267.66129688866204 Accuracy: tensor(0.9855, device='cuda:0')\n",
      "Batch Loss: 270.61418031848734 Accuracy: tensor(0.9856, device='cuda:0')\n",
      "Batch Loss: 276.55372037930647 Accuracy: tensor(0.9855, device='cuda:0')\n",
      "Batch Loss: 280.2057938525104 Accuracy: tensor(0.9856, device='cuda:0')\n",
      "Batch Loss: 285.52757356961956 Accuracy: tensor(0.9854, device='cuda:0')\n",
      "Batch Loss: 289.36043864057865 Accuracy: tensor(0.9853, device='cuda:0')\n",
      "Batch Loss: 294.32457982416963 Accuracy: tensor(0.9853, device='cuda:0')\n",
      "Batch Loss: 300.90007930382853 Accuracy: tensor(0.9853, device='cuda:0')\n",
      "Batch Loss: 306.4165148031316 Accuracy: tensor(0.9852, device='cuda:0')\n",
      "Batch Loss: 310.34613886330044 Accuracy: tensor(0.9852, device='cuda:0')\n",
      "Batch Loss: 316.37219478253974 Accuracy: tensor(0.9851, device='cuda:0')\n",
      "Batch Loss: 321.0800427350914 Accuracy: tensor(0.9851, device='cuda:0')\n",
      "Batch Loss: 326.6872309608152 Accuracy: tensor(0.9851, device='cuda:0')\n",
      "Batch Loss: 330.91699361032806 Accuracy: tensor(0.9852, device='cuda:0')\n",
      "Batch Loss: 337.1588814712595 Accuracy: tensor(0.9852, device='cuda:0')\n",
      "Batch Loss: 341.46639548044186 Accuracy: tensor(0.9852, device='cuda:0')\n",
      "Batch Loss: 347.0616495293798 Accuracy: tensor(0.9852, device='cuda:0')\n",
      "Batch Loss: 351.53350467723794 Accuracy: tensor(0.9852, device='cuda:0')\n",
      "Batch Loss: 356.9209810408065 Accuracy: tensor(0.9852, device='cuda:0')\n",
      "Batch Loss: 360.6961479810998 Accuracy: tensor(0.9853, device='cuda:0')\n",
      "Batch Loss: 366.18013655615505 Accuracy: tensor(0.9852, device='cuda:0')\n",
      "Batch Loss: 373.42585889447946 Accuracy: tensor(0.9851, device='cuda:0')\n",
      "Batch Loss: 378.5340511004906 Accuracy: tensor(0.9850, device='cuda:0')\n",
      "Batch Loss: 382.9018586400198 Accuracy: tensor(0.9851, device='cuda:0')\n",
      "Batch Loss: 386.7911265761941 Accuracy: tensor(0.9850, device='cuda:0')\n",
      "Batch Loss: 392.83202282415004 Accuracy: tensor(0.9850, device='cuda:0')\n",
      "Batch Loss: 396.7965704681701 Accuracy: tensor(0.9850, device='cuda:0')\n",
      "Batch Loss: 400.2287241423037 Accuracy: tensor(0.9851, device='cuda:0')\n",
      "Batch Loss: 407.17886241467204 Accuracy: tensor(0.9851, device='cuda:0')\n",
      "Batch Loss: 412.7780446050456 Accuracy: tensor(0.9850, device='cuda:0')\n",
      "Batch Loss: 418.2869857448386 Accuracy: tensor(0.9850, device='cuda:0')\n",
      "Batch Loss: 424.7381359409774 Accuracy: tensor(0.9849, device='cuda:0')\n",
      "Batch Loss: 429.826834682608 Accuracy: tensor(0.9849, device='cuda:0')\n",
      "Batch Loss: 433.73287132091355 Accuracy: tensor(0.9849, device='cuda:0')\n",
      "Batch Loss: 438.63420329702785 Accuracy: tensor(0.9849, device='cuda:0')\n",
      "Batch Loss: 441.65956696594367 Accuracy: tensor(0.9850, device='cuda:0')\n",
      "Batch Loss: 447.95804900769144 Accuracy: tensor(0.9849, device='cuda:0')\n",
      "Batch Loss: 454.34737082978245 Accuracy: tensor(0.9849, device='cuda:0')\n",
      "Batch Loss: 459.4296552105807 Accuracy: tensor(0.9849, device='cuda:0')\n",
      "Batch Loss: 468.68616935936734 Accuracy: tensor(0.9848, device='cuda:0')\n",
      "Batch Loss: 474.7803068328649 Accuracy: tensor(0.9848, device='cuda:0')\n",
      "Batch Loss: 482.26769068837166 Accuracy: tensor(0.9847, device='cuda:0')\n",
      "Batch Loss: 486.4741487680003 Accuracy: tensor(0.9847, device='cuda:0')\n",
      "Batch Loss: 491.64054103789385 Accuracy: tensor(0.9847, device='cuda:0')\n",
      "Batch Loss: 498.13471547467634 Accuracy: tensor(0.9847, device='cuda:0')\n",
      "Batch Loss: 504.6247171362629 Accuracy: tensor(0.9846, device='cuda:0')\n",
      "Batch Loss: 511.56849590840284 Accuracy: tensor(0.9846, device='cuda:0')\n",
      "Batch Loss: 519.7693450947991 Accuracy: tensor(0.9845, device='cuda:0')\n",
      "Batch Loss: 524.7255176769104 Accuracy: tensor(0.9845, device='cuda:0')\n",
      "Batch Loss: 531.9346724955831 Accuracy: tensor(0.9844, device='cuda:0')\n",
      "Batch Loss: 536.8174223802052 Accuracy: tensor(0.9844, device='cuda:0')\n",
      "Batch Loss: 540.9553886693902 Accuracy: tensor(0.9844, device='cuda:0')\n",
      "Batch Loss: 548.2552360232221 Accuracy: tensor(0.9843, device='cuda:0')\n",
      "Batch Loss: 552.1789004657185 Accuracy: tensor(0.9843, device='cuda:0')\n",
      "Batch Loss: 557.5579740285175 Accuracy: tensor(0.9843, device='cuda:0')\n",
      "Batch Loss: 564.1460490607424 Accuracy: tensor(0.9843, device='cuda:0')\n",
      "Batch Loss: 567.6399993840605 Accuracy: tensor(0.9843, device='cuda:0')\n",
      "Batch Loss: 574.4279086180031 Accuracy: tensor(0.9843, device='cuda:0')\n",
      "Batch Loss: 578.5686124876374 Accuracy: tensor(0.9843, device='cuda:0')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch Loss: 584.1821193032665 Accuracy: tensor(0.9843, device='cuda:0')\n",
      "Batch Loss: 588.7689832437318 Accuracy: tensor(0.9844, device='cuda:0')\n",
      "Batch Loss: 592.3958280609222 Accuracy: tensor(0.9844, device='cuda:0')\n",
      "Batch Loss: 600.5609871849883 Accuracy: tensor(0.9843, device='cuda:0')\n",
      "Batch Loss: 604.8430113089271 Accuracy: tensor(0.9844, device='cuda:0')\n",
      "Batch Loss: 609.4872746473411 Accuracy: tensor(0.9844, device='cuda:0')\n",
      "Batch Loss: 616.0418544050772 Accuracy: tensor(0.9843, device='cuda:0')\n",
      "Batch Loss: 621.8254794834647 Accuracy: tensor(0.9843, device='cuda:0')\n",
      "Batch Loss: 626.0420755945379 Accuracy: tensor(0.9843, device='cuda:0')\n",
      "Batch Loss: 631.4305859507294 Accuracy: tensor(0.9843, device='cuda:0')\n",
      "Batch Loss: 636.8815622739494 Accuracy: tensor(0.9842, device='cuda:0')\n",
      "Batch Loss: 641.2389526104671 Accuracy: tensor(0.9843, device='cuda:0')\n",
      "Batch Loss: 650.2011355275754 Accuracy: tensor(0.9842, device='cuda:0')\n",
      "Batch Loss: 657.315684769419 Accuracy: tensor(0.9841, device='cuda:0')\n",
      "Batch Loss: 661.8708102030214 Accuracy: tensor(0.9842, device='cuda:0')\n",
      "Train Loss: 664.674646972795 Accuracy: tensor(0.9842, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "losses = []\n",
    "accuracies = []\n",
    "\n",
    "for i in range(epochs):\n",
    "    total_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    batches = 0\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    for input_ids_batch, attention_masks_batch, y_batch, original_text in tqdm(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        y_batch = y_batch.to(device)\n",
    "        y_pred = model(input_ids_batch.to(device), attention_mask=attention_masks_batch.to(device))[0]\n",
    "        loss = F.cross_entropy(y_pred, y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        _, predicted = torch.max(y_pred, 1)\n",
    "        correct += (predicted == y_batch).sum()\n",
    "        total += len(y_batch)\n",
    "\n",
    "        batches += 1\n",
    "        if batches % 100 == 0:\n",
    "            print(\"Batch Loss:\", total_loss, \"Accuracy:\", correct.float() / total)\n",
    "  \n",
    "    losses.append(total_loss)\n",
    "    accuracies.append(correct.float() / total)\n",
    "    print(\"Train Loss:\", total_loss, \"Accuracy:\", correct.float() / total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(pre_MODEL_NAME))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qvEB8g7IFbsD"
   },
   "source": [
    "# 테스트 데이터셋 정확도및 강한(긍정,부정) 확률 확인\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "cc5dbed4ea034dc29af76a67f0e36692",
      "bd47b0c2081f4c9e8a5c4214b9704c50",
      "3f97a9a16f394f79a48d4d81c3a8f52a",
      "39bdbe27bc4a4b9ebf1b72b2eb6f1545",
      "88eb7cf803c149d79db9c220894d3d79",
      "6948f6f65b5f442da4a9e69251a26426",
      "e94e58052fd2492483004fb61b91d961",
      "36ac2994831e49b6851fbddda65abb27"
     ]
    },
    "id": "5QiALUqm4juf",
    "outputId": "961ce5d3-6025-41f9-8bc7-69765c29e2dc"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d59660fc5f0847429f29333ac617769c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/114 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\build\\project\\programmers\\4_party_project\\env\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2149: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "e:\\build\\project\\programmers\\4_party_project\\env\\lib\\site-packages\\torch\\nn\\functional.py:1639: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "기존문장: 전철역과 가깝고 호텔 부페는 저렴하고 종류도 많아 놀고 먹기는 딱입니다\n",
      "부정확률=3.56%, 긍정확률=94.45%\n",
      "\n",
      "기존문장: 12시 체크아웃 주말이면 투숙객도 무조건 발렛 강요함 발렛비 개인으로 내야 함 비싼 모텔 수준\n",
      "부정확률=87.01%, 긍정확률=13.91%\n",
      "\n",
      "기존문장: 방음이 잘 안 됐어요\n",
      "부정확률=93.06%, 긍정확률=6.84%\n",
      "\n",
      "기존문장: 너무하다 증 말\n",
      "부정확률=72.48%, 긍정확률=28.2%\n",
      "\n",
      "기존문장: 이 호텔을 예약한 경우 이미 변경할 수 있고 사용하도록 조심하세요\n",
      "부정확률=90.04%, 긍정확률=10.03%\n",
      "\n",
      "기존문장: 퇴실이 12시인데 9시에 차를 빼달라고 아니면 10분에 2천원 이라고 이건 어찌저찌 해경함\n",
      "부정확률=79.11%, 긍정확률=21.79%\n",
      "\n",
      "기존문장: 다음에 또 일이 생기면 여기를 찾게 될 것 같습니다\n",
      "부정확률=9.84%, 긍정확률=87.43%\n",
      "\n",
      "기존문장: 청결 상태가 최악입니다\n",
      "부정확률=91.97%, 긍정확률=8.04%\n",
      "\n",
      "기존문장: 좋은 점이 없습니다\n",
      "부정확률=38.38%, 긍정확률=60.32%\n",
      "\n",
      "기존문장: 외국 손님 오셔서 예약했는데 숙소 자체는 괜찮은데 클럽이 있어서 너무 시끄럽고 주변이 지저분하더라구 요 ㅠㅠ 내국인은 괜찮지만 외국인에게는 추천하고 싶지 않은 호텔입니다\n",
      "부정확률=89.69%, 긍정확률=11.19%\n",
      "\n",
      "기존문장: 이는 음 양성애자 단지 선호하는 딱딱한 침대 방은 조용하고 그래서 전혀 문제가 또한 와이파이 잘 들어 절곡돌기 안 좋았던 점 방은 더럽고 심각한 업데이트 전체 본 제한된 예산 이는 나쁘지 않은 선택 하지만 더 나은 선택이 많이 있다\n",
      "부정확률=90.71%, 긍정확률=9.72%\n",
      "\n",
      "기존문장: 이런 곳을 호텔이라고 부를 수 있을까 싶습니다 시트는 빨아도 더러운 건지 온통 정체를 알고 싶지도 않은 여러 종류의 얼룩들로 가득했고 벽에는 구멍까지 나 고 온갖 신발 자국들로 더러웠습니다\n",
      "부정확률=89.96%, 긍정확률=10.64%\n",
      "\n",
      "기존문장: 있을 거 다 있고 창 밖 한강 풍경도 멋있어요\n",
      "부정확률=4.06%, 긍정확률=93.95%\n",
      "\n",
      "기존문장: 슈피리어 더블을 예약했는데 이제는 예약을 그는 50 할인이 되는 집합 방 업그레이드할 수 있는 디럭스 룸 나는 그를 이용할 경우 추가 비용을 지불하는 적절한 방을 예약했는데 다른 방을 실수 때문에 우리는 이곳에서 묵을 수 없었다\n",
      "부정확률=85.98%, 긍정확률=15.35%\n",
      "\n",
      "기존문장: 와이파이는 저의 방에만 안 되는지 옆에 방꺼 사용을 했습니다\n",
      "부정확률=90.66%, 긍정확률=9.79%\n",
      "\n",
      "기존문장: 가격 대비 방 크기와 침대는 편하고 좋았습니다\n",
      "부정확률=5.44%, 긍정확률=92.22%\n",
      "\n",
      "기존문장: 세월에 흔적이라고 해야 하나요\n",
      "부정확률=89.38%, 긍정확률=10.9%\n",
      "\n",
      "기존문장: 환기라도 좀 시켜야겠다 싶어서 창문을 열었는데 창문이 두겹인데 바깥쪽 창문이 안 열림 창문 도대체 왜 있는지 어쨌든 창문 열다가 때가 너무 껴 있어서 기분 나빠서 창문 다시 닫음 누군가 여기 호텔에서 잔다고 하면 보따리 싸서 뜯어 말리고 싶을 정도임\n",
      "부정확률=91.23%, 긍정확률=9.42%\n",
      "\n",
      "기존문장: 남겨 바로 방 있어 복도에 카펫도 얼룩지고 있습니다\n",
      "부정확률=88.91%, 긍정확률=11.43%\n",
      "\n",
      "기존문장: 위치가 좋고 가격 대비 만족함\n",
      "부정확률=3.56%, 긍정확률=94.49%\n",
      "\n",
      "기존문장: 시설이나 위치 대비 가격 괜찮은 편이고 더 저렴한 가격을 원하시면 가끔 딜이 뜨는 걸 잘 이용하시면 좋습니다\n",
      "부정확률=10.03%, 긍정확률=87.37%\n",
      "\n",
      "기존문장: 강북에 갈 때에도 지하철로도 택시로도 가깝습니다\n",
      "부정확률=4.72%, 긍정확률=93.03%\n",
      "\n",
      "기존문장: 가난한 방 가난한 경험 내 방은 매우 나쁜 공기 어두운 복도에 카펫이 아니고 군데군데 평면 이 밤에 정말 으스스 해요\n",
      "부정확률=39.54%, 긍정확률=58.97%\n",
      "\n",
      "기존문장: 아고다 예약이 엉터리임니다\n",
      "부정확률=91.66%, 긍정확률=8.31%\n",
      "\n",
      "기존문장: 욕조 물샘 변기 물샘 커피포트 작동안함\n",
      "부정확률=87.86%, 긍정확률=12.6%\n",
      "\n",
      "기존문장: 냉장고 물이 두 병 다 얼어 있었어요\n",
      "부정확률=92.33%, 긍정확률=7.66%\n",
      "\n",
      "기존문장: 수 페리어는 의 자 놓을 공간 자체가 없음\n",
      "부정확률=89.17%, 긍정확률=11.32%\n",
      "\n",
      "기존문장: 후기는 처음\n",
      "부정확률=66.1%, 긍정확률=34.82%\n",
      "\n",
      "기존문장: 주변에 돌아다닐 것이라고 했는데 그들이 진정으로 원 및 걱정 하지 않았다\n",
      "부정확률=43.8%, 긍정확률=56.14%\n",
      "\n",
      "기존문장: 객실은 사진과는 전혀 다른 모습이 고 악취가 너무 심하고 복도와 객실 바닥 엘리베이터엔 손 닿으면 바로 병 옮을 고 같은 더러운 매트들이 깔려 있고 호텔 지어진 이후 한 번도 레노베이션 없었더라구요\n",
      "부정확률=89.11%, 긍정확률=11.62%\n",
      "\n",
      "기존문장: 위치가 좋고 주변이 깨끗했습니다\n",
      "부정확률=3.27%, 긍정확률=94.8%\n",
      "\n",
      "기존문장: 뷰가 아주 좋습니다\n",
      "부정확률=3.89%, 긍정확률=94.08%\n",
      "\n",
      "기존문장: 샤워실에 곰팡이가 있는 게 더 좋은 곳이었고 스크 러브 세면대 미러도 저는 이 방이 1층에 있는 두 번째 날 12 깨끗한 것으로 갱신된 작은 카펫은 너무 더러워서 염색 할 수 있다\n",
      "부정확률=66.41%, 긍정확률=34.02%\n",
      "\n",
      "기존문장: 옮긴 방은 더 가관이었어요\n",
      "부정확률=79.11%, 긍정확률=22.23%\n",
      "\n",
      "기존문장: 침구는 보송보송 하지 않았고 화장실 샤워 커튼 등 오래된 물 때가 눈에 확 띄었음\n",
      "부정확률=88.85%, 긍정확률=11.8%\n",
      "\n",
      "기존문장: 조명위에 먼지가 가득함 전체적으로 노후된 호텔 한강뷰를 기대하고 갔는데 지붕 뷰를 보고 옴\n",
      "부정확률=91.54%, 긍정확률=8.6%\n",
      "\n",
      "기존문장: 조식도 먹을 거 없어요\n",
      "부정확률=79.33%, 긍정확률=21.84%\n",
      "\n",
      "기존문장: 이 가격에 이런 객실을 받았다고 생각하니 사기 당했단 생각이 들었다\n",
      "부정확률=79.04%, 긍정확률=22.49%\n",
      "\n",
      "기존문장: 욕실에서는 곰팡이가 껴서 냄새가 나 고 옆 방 소리 하품소리가 들립니다\n",
      "부정확률=92.71%, 긍정확률=7.5%\n",
      "\n",
      "기존문장: 우선 친구들과 호캉스하자며 3인 트리플룸에 묵었습니다\n",
      "부정확률=12.33%, 긍정확률=84.87%\n",
      "\n",
      "기존문장: 객실 천장에 얼룩진 부분도 많고 대부분 청결하지 못함\n",
      "부정확률=92.46%, 긍정확률=7.9%\n",
      "\n",
      "기존문장: 신사역에 있어서 가로수길이나 압구정동은 걸어서도 구경하러 편하게 다녀올 수 있었고 택시를 타도 한남동이나 이태원 강남역까지 10분 안에 이동이 가능했습니다\n",
      "부정확률=3.76%, 긍정확률=94.19%\n",
      "\n",
      "기존문장: 우선 저희는 11층에 머무를 때 가 많은데 더블로 예약해요\n",
      "부정확률=38.05%, 긍정확률=61.07%\n",
      "\n",
      "기존문장: 아무런 사용도 관리도 안 돼 있는 상태였어요\n",
      "부정확률=91.62%, 긍정확률=8.54%\n",
      "\n",
      "기존문장: 무엇보다 역 근 처여서 너무 좋았습니다\n",
      "부정확률=6.85%, 긍정확률=90.57%\n",
      "\n",
      "기존문장: 뷔페 음식이 가격 대비 훌륭합니다\n",
      "부정확률=4.28%, 긍정확률=93.58%\n",
      "\n",
      "기존문장: 확 트인 전망 푹신한 침대\n",
      "부정확률=3.98%, 긍정확률=93.87%\n",
      "\n",
      "기존문장: 비가 와서 입구 찾는 거도 빡치는 기 억 호텔 입구가 지하에 있을 줄이야 ㅜㅜㅜ 입구 못 찾아 해맨건 첨이에요\n",
      "부정확률=76.61%, 긍정확률=24.76%\n",
      "\n",
      "기존문장: 안 되죠 았으면 들어갔을 때 호텔 이틀 전 큰 그룹의 사람들이 떠나 가면 대부분 흡연 전체 로비 담배 냄새 의 나머지 밤 안 그럼 이 있다\n",
      "부정확률=88.3%, 긍정확률=12.14%\n",
      "\n",
      "기존문장: 방의 전경이 아주 구렸음\n",
      "부정확률=78.29%, 긍정확률=22.82%\n",
      "\n",
      "기존문장: 가격 대비 가성비 최고네요\n",
      "부정확률=4.31%, 긍정확률=93.54%\n",
      "\n",
      "기존문장: 방은 넓고 깔끔해서 참 좋았는데요\n",
      "부정확률=48.62%, 긍정확률=51.08%\n",
      "\n",
      "기존문장: 트윈방은 창문이 없어서 환기가 안 되니 들어가면 악취 냄새가 풍겨요\n",
      "부정확률=91.09%, 긍정확률=9.45%\n",
      "\n",
      "기존문장: 외관만 화려하고 속은 다 낙후 5월인데도 아직도 숙소층에 크리스마스 장식 있는 거 보면 얼마나 관리를 안 하는지 알 수 있음\n",
      "부정확률=86.76%, 긍정확률=14.01%\n",
      "\n",
      "기존문장: 한강뷰를 보려고 간 거였는데 이 미 만 실이라 방 교체가 안 된 것 빼고는 괜찮았음\n",
      "부정확률=59.68%, 긍정확률=41.27%\n",
      "\n",
      "기존문장: 아무래도 도시라 창문을 열면 소음이 있지만 닫으면 괜찮았습니다\n",
      "부정확률=82.8%, 긍정확률=18.24%\n",
      "\n",
      "기존문장: 신사 역에서 걸어서 금방\n",
      "부정확률=7.89%, 긍정확률=89.43%\n",
      "\n",
      "기존문장: 깔끔하고 좋았어요\n",
      "부정확률=4.28%, 긍정확률=93.56%\n",
      "\n",
      "기존문장: 어떻게 그렇게 관리가 안 된 호텔이 3 스타 타이틀을 가질 수 있는지 시설은 노후 될 수 있고 이해할 수 있음\n",
      "부정확률=79.05%, 긍정확률=21.95%\n",
      "\n",
      "기존문장: 해서 체크아웃 시간 전에 확인하지 않고 막 들어와서 청소를 하다가 나중에 아웃한 건지 확인 전화 온 것 귀중품은 없었어서 다행 3 청소부와 로비 여직원 태도 문제 정중 한 사과 없음\n",
      "부정확률=90.13%, 긍정확률=10.45%\n",
      "\n",
      "기존문장: 수영장도 너무 관리가 안 되어 있고 사우나에 가면 참견질 하는 할머니들이 너무 많아요 ㅠㅠ\n",
      "부정확률=91.88%, 긍정확률=8.37%\n",
      "\n",
      "기존문장: 할인율이 높은 방을 예약하면 보통 전망은 포기해야 한다는 것을 확인한 예약이었습니다\n",
      "부정확률=77.62%, 긍정확률=24.49%\n",
      "\n",
      "기존문장: 답변하기 저는 3성급 호텔인 원하는 것보다 더 작은 기본적인 객실은 매우 깨끗합니다\n",
      "부정확률=40.63%, 긍정확률=58.87%\n",
      "\n",
      "기존문장: 디저트 종류도 정말 많습니다\n",
      "부정확률=5.85%, 긍정확률=91.66%\n",
      "\n",
      "기존문장: 가격 위치 사우나\n",
      "부정확률=5.07%, 긍정확률=92.72%\n",
      "\n",
      "기존문장: 화장실 욕조라던지 샤워기가 지저분 하긴 했지만 위치적으로 최고였습니다\n",
      "부정확률=6.48%, 긍정확률=91.21%\n",
      "\n",
      "기존문장: 덕분에 좋은 시간 보냈어요\n",
      "부정확률=7.17%, 긍정확률=90.21%\n",
      "\n",
      "기존문장: 아침 식사를 할 수 있는 추천하지 않습니다 한 가게들과 식당들이 괜찮은 강남역에서 어떠한 멋진 시설과 친절한 직원들도 있지만\n",
      "부정확률=73.47%, 긍정확률=28.61%\n",
      "\n",
      "기존문장: 열심히 수영해서 자체 발열 필요 사우나는 그나마 좀 한적함 절대 다신 안감 세상에서 제일 미운 사람에게 강추하고 싶음\n",
      "부정확률=70.31%, 긍정확률=31.08%\n",
      "\n",
      "기존문장: 환풍기 인 듯 합니다\n",
      "부정확률=82.58%, 긍정확률=17.82%\n",
      "\n",
      "기존문장: 직원들은 친절합니다\n",
      "부정확률=19.18%, 긍정확률=78.44%\n",
      "\n",
      "기존문장: 방이 일단 더러웠어요\n",
      "부정확률=91.4%, 긍정확률=8.85%\n",
      "\n",
      "기존문장: 불키면 시스루 다 못해 아예 안이 보이는 화장실 첨에 보고 너무 충격적이라 친구들이랑 한참 웃었네요\n",
      "부정확률=89.69%, 긍정확률=10.89%\n",
      "\n",
      "기존문장: 공항에서 리무진으로 바로 앞까지 오고 역과도 가깝습니다\n",
      "부정확률=3.8%, 긍정확률=94.13%\n",
      "\n",
      "기존문장: 방 컨디션도 넓고 괜찮아요\n",
      "부정확률=3.24%, 긍정확률=94.87%\n",
      "\n",
      "기존문장: 근데 이게 왠 일 이 추운 날에 뜨거운 물로 틀고 샤워를 하다가 찬물이 나오네요\n",
      "부정확률=83.06%, 긍정확률=18.1%\n",
      "\n",
      "기존문장: 객실에 냄새가 좀 났어요\n",
      "부정확률=92.39%, 긍정확률=7.68%\n",
      "\n",
      "기존문장: 리버 사이드 음식이 맛있다고 해서 기대했는데 조식은 메뉴도 별로 없고 음식이 름표가 전날 저녁 거 그대로 있더라구요\n",
      "부정확률=66.53%, 긍정확률=35.24%\n",
      "\n",
      "기존문장: 요즘 비즈니스로 호텔 이용하는 분들도 많은데 이 부분은 매우 불편할 거라 생각됩니다\n",
      "부정확률=87.48%, 긍정확률=13.38%\n",
      "\n",
      "기존문장: 그저 그러네요\n",
      "부정확률=48.19%, 긍정확률=52.88%\n",
      "\n",
      "기존문장: 그리고 체크아웃 시간이 12시인데 토요일은 9시까지 차를 빼달라 이건 도대체 납득이 불가합니다\n",
      "부정확률=91.89%, 긍정확률=8.4%\n",
      "\n",
      "기존문장: 담배 안피운다고 쓰레기통 다 뒤졌구요\n",
      "부정확률=76.55%, 긍정확률=25.08%\n",
      "\n",
      "기존문장: 방이 생각보다는 작은 점도 조금 아쉬웠습니다\n",
      "부정확률=92.36%, 긍정확률=7.84%\n",
      "\n",
      "기존문장: 너무 감사했어요\n",
      "부정확률=10.8%, 긍정확률=86.38%\n",
      "\n",
      "기존문장: 사진보다 좁다는 느낌이었습니다\n",
      "부정확률=91.34%, 긍정확률=8.87%\n",
      "\n",
      "기존문장: 바가 없 룸 서비스 및 하우스 레스토랑에 가까운 9시 직원들은 영어를 유창하게 하는 작은 방 저는 여 기 다시 묵지 않을 것 많은 더 좋은 곳\n",
      "부정확률=67.05%, 긍정확률=34.47%\n",
      "\n",
      "기존문장: 하지만 직원들은 친절해요\n",
      "부정확률=69.67%, 긍정확률=31.89%\n",
      "\n",
      "기존문장: 오후에 체크인하고 룸에 갔는데 화장실 문고리도 망가져 있고 바닥에 개미가 많았어요\n",
      "부정확률=92.53%, 긍정확률=7.62%\n",
      "\n",
      "기존문장: 트윈보다는 더블 베드를 선택하세요\n",
      "부정확률=73.4%, 긍정확률=28.36%\n",
      "\n",
      "기존문장: 저렴함 좋은 위치 친절한 직원\n",
      "부정확률=3.8%, 긍정확률=94.21%\n",
      "\n",
      "기존문장: 코로나로 인하여 약 1여년간 재택 근무 기간이 길어져서 몸도 마음도 지쳤습니다\n",
      "부정확률=38.7%, 긍정확률=60.46%\n",
      "\n",
      "기존문장: 위치는 괜찮습니다\n",
      "부정확률=6.23%, 긍정확률=91.35%\n",
      "\n",
      "기존문장: 위치가 고속도로 빠져나가기에 가까워 좋았습니다\n",
      "부정확률=3.39%, 긍정확률=94.65%\n",
      "\n",
      "기존문장: 식당에서도 재발급이 되는 걸 말이죠\n",
      "부정확률=86.47%, 긍정확률=14.64%\n",
      "\n",
      "기존문장: 와이파이 신호가 약했다\n",
      "부정확률=92.92%, 긍정확률=6.99%\n",
      "\n",
      "기존문장: 위치 외에는 없음\n",
      "부정확률=41.19%, 긍정확률=59.35%\n",
      "\n",
      "기존문장: 낡은 카드 키였는데 분 실시 만 원이나 디파짓을 받더라구요 ㅠㅠ\n",
      "부정확률=87.52%, 긍정확률=13.03%\n",
      "\n",
      "기존문장: 객실 3개 예약했음 ㅎ\n",
      "부정확률=37.21%, 긍정확률=62.46%\n",
      "\n",
      "기존문장: 이 호텔은 우리가 집으로 돌아가기 전 마지막으로 묵었던 호텔인데 휴가를 끝내기에는 정말 최악의 방법이었습니다\n",
      "부정확률=86.28%, 긍정확률=14.59%\n",
      "\n",
      "기존문장: 뷔페 음식도 가격 대비 좋은 편이고 위치나 객식 내부 시설도 무난한 편이었습니다\n",
      "부정확률=3.25%, 긍정확률=94.85%\n",
      "\n",
      "기존문장: 찜찜하여 침대에 눕고 싶지 않아서 앉아서 밤 새 고 나왔고 다시는 이 호텔 이용하고 싶지 않습니다\n",
      "부정확률=86.81%, 긍정확률=14.15%\n",
      "\n",
      "기존문장: 옆 객실 것씀 춥다\n",
      "부정확률=89.93%, 긍정확률=10.34%\n",
      "\n",
      "기존문장: 너무 멋진 옷을 입고 있지만 안 레스토랑에서 먹을 수도 있지 특별히 해 주는 곳이나 씻고 자는 것 하지만 위치가 좋고 걸어서 갈 수 있는 흥미로운 상점들이 있고 신사 가로수길 점 또한 저는 주변 지역은 아주 안전 하면\n",
      "부정확률=6.88%, 긍정확률=90.41%\n",
      "\n",
      "기존문장: 발렛 맡겼을 때는 몰랐는데 직접 갔을 때는 이게 호텔 주차장이 맞나 싶을 정도로 낙후한 외국인 손님 모시고 가기엔 별로요\n",
      "부정확률=85.77%, 긍정확률=15.22%\n",
      "\n",
      "기존문장: 룸 안에서 미세하게 흡연 냄새가 나는 것 같습니다\n",
      "부정확률=92.4%, 긍정확률=7.86%\n",
      "\n",
      "기존문장: 조식도 기대 이상 이었고 직원 분들의 친절한 대응도 참 좋아서 다음에도 다시 이용하고 싶습니다\n",
      "부정확률=4.14%, 긍정확률=93.77%\n",
      "\n",
      "기존문장: 다시는 숙박 안 할 예정입니다\n",
      "부정확률=88.88%, 긍정확률=11.83%\n",
      "\n",
      "기존문장: 호텔 국제 뽕꿀 그리고 조각 이름은 영어 호텔 한 가지 방법을 모르는 저는 방에서 에어컨 작동 이미 다 때마다 온 방에 이 미 에게 리셉션에 매일 켭니다\n",
      "부정확률=68.09%, 긍정확률=32.49%\n",
      "\n",
      "기존문장: 비용은 모텔의 두배를 받으면서 시설은 정말 최악 입니다\n",
      "부정확률=89.22%, 긍정확률=11.34%\n",
      "\n",
      "기존문장: 오래된 호텔이 다 보니깐 냄새가 심했어요\n",
      "부정확률=90.93%, 긍정확률=9.26%\n",
      "\n",
      "기존문장: 웨딩홀 함께 사용이라 주차가 불편했어요\n",
      "부정확률=90.33%, 긍정확률=10.06%\n",
      "\n",
      "기존문장: 신사역이랑 가까워서 좋았어요\n",
      "부정확률=3.76%, 긍정확률=94.22%\n",
      "\n",
      "기존문장: 생각지 못해 당황 스러웠습니다\n",
      "부정확률=75.37%, 긍정확률=25.73%\n",
      "\n",
      "기존문장: 저녁 뷔페는 잘 나와서 조식을 기대했었는데 스크램 블에그는 식어 있었고 가짓수도 너무 적었습니다\n",
      "부정확률=75.67%, 긍정확률=26.02%\n",
      "\n",
      "기존문장: 단체로 모임이나 연회 장소로 적합하다고 생각합니다\n",
      "부정확률=8.67%, 긍정확률=88.73%\n",
      "\n",
      "기존문장: 배정 받은 방 욕실 욕조 타일이 군데군데 깨져 있어서 다른 방 배정받았는데 깨진 곳 없을 뿐 낡은 느낌은 그대로\n",
      "부정확률=83.68%, 긍정확률=17.8%\n",
      "\n",
      "기존문장: 호텔은 그런데로 좋은데요\n",
      "부정확률=33.85%, 긍정확률=65.63%\n",
      "\n",
      "기존문장: 감사합니다 가성비 갑이라고 해서 갔는데 뷔페랑 숙소 다 기대 이 하였고 부킹에서 예약하신 분들은 방 확인하고 욕조 없으면 카운터에 얘기하세요\n",
      "부정확률=43.54%, 긍정확률=55.65%\n",
      "\n",
      "기존문장: 엄마랑 두 달에 한 번 정도 저 호텔에 가서 저녁을 먹는데 바로 구워주는 고기의 질이 정말 좋습니다\n",
      "부정확률=4.67%, 긍정확률=93.08%\n",
      "\n",
      "기존문장: 객실 커턴이 지저분하고 살아있는 나방2마리가 커턴에서 살고 있었음\n",
      "부정확률=92.42%, 긍정확률=7.96%\n",
      "\n",
      "기존문장: 가격 주요 지 근접성 남성 전용 사우나\n",
      "부정확률=6.97%, 긍정확률=90.41%\n",
      "\n",
      "기존문장: 지금 생각해보니 객실이랑 이어져 있으면 주차장 본 고객이 경악하고 다신 안 올까봐 일부러 떨어트려 놓은 거 같음\n",
      "부정확률=87.16%, 긍정확률=13.75%\n",
      "\n",
      "기존문장: 호텔 위치는 택시 타고 가서 딱히 불편하지 않음\n",
      "부정확률=15.38%, 긍정확률=81.67%\n",
      "\n",
      "기존문장: 수압이 너무 약하고 물 온도 맞추는 게 엉망이였어요\n",
      "부정확률=92.01%, 긍정확률=8.23%\n",
      "\n",
      "기존문장: 호텔 직원들은 요청 후에도 이 벽판은 룸 서비스는 존재하지 않는 카펫은 더럽고 냄새가 나는 호텔입니다\n",
      "부정확률=90.01%, 긍정확률=10.84%\n",
      "\n",
      "기존문장: 프런트 종업원도 일본어를 할 수 있습니다\n",
      "부정확률=7.77%, 긍정확률=89.43%\n",
      "\n",
      "기존문장: 딱히 좋지는 않음\n",
      "부정확률=67.02%, 긍정확률=34.19%\n",
      "\n",
      "기존문장: 찬물과 더운물을 조절하는 건 불가능 해 보였어요\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "부정확률=90.65%, 긍정확률=9.59%\n",
      "\n",
      "기존문장: 가격 대비 편안함\n",
      "부정확률=6.11%, 긍정확률=91.46%\n",
      "\n",
      "기존문장: 그리고 수영장과 사우나는 입실하는 날 결제하면 그날만 사용 가능하고 다음날 아침에 또 이용하려면 다시 결제해야 합니다\n",
      "부정확률=47.53%, 긍정확률=53.22%\n",
      "\n",
      "기존문장: 좀 더 쾌적한 환경이 필요합니다\n",
      "부정확률=89.59%, 긍정확률=10.56%\n",
      "\n",
      "기존문장: 무슨 호텔이 이럴까 호텔이라고 부를 수 없을 정도 피트니스 청소를 얼마나 하지 않았으면 1주일이 넘어도 컵이 그대로 있고 여기저기 먼지가 수북 락커에도 먼지가 목욕탕에는 스마트폰 사용하는 자들이 득실거리고 ㅎ 직원이 누구인지 리버 사이드 모텔로 이름 바꿔 라 호텔이라는 이름을 더럽히지 말고\n",
      "부정확률=90.86%, 긍정확률=9.41%\n",
      "\n",
      "기존문장: 덩치가 큰 편이라서 큰 침대를 요청했었는데 침대 두 개를 붙여줘서 잠을 잘 자지 못했습니다\n",
      "부정확률=81.72%, 긍정확률=20.08%\n",
      "\n",
      "기존문장: 위치만 좋아요\n",
      "부정확률=5.59%, 긍정확률=92.16%\n",
      "\n",
      "기존문장: 위치는 좋음\n",
      "부정확률=4.68%, 긍정확률=93.13%\n",
      "\n",
      "기존문장: 하지 않는 것을 알고 있는지 다른 일부 팬들은 추출 밖 조명을 작동하지 않습니다\n",
      "부정확률=90.97%, 긍정확률=9.11%\n",
      "\n",
      "기존문장: 무엇보다 패밀리 레스토랑 가격과 별 차이가 없는데 훨씬 다양한 메뉴가 있고 맛있는 편입니다\n",
      "부정확률=8.96%, 긍정확률=88.4%\n",
      "\n",
      "기존문장: 다른 호텔 무드 등 정도의 밝기가 여기는 가장 밝은 정도라 너무 어둡습니다\n",
      "부정확률=91.35%, 긍정확률=8.73%\n",
      "\n",
      "기존문장: 배정 받은 룸은 먼지가 가득 쌓여 있고 도저히 사용할 수 없을 거 같아서 룸 체인지 원했습니다\n",
      "부정확률=80.35%, 긍정확률=20.82%\n",
      "\n",
      "기존문장: 굿굿 휴지통은 안 비워서 일회용 팬티 그대로 있고 청소는 안 해서 칫 솔 껍질이랑 치약은 그대로 있고 화장실은 냄새나요\n",
      "부정확률=85.55%, 긍정확률=15.13%\n",
      "\n",
      "기존문장: 체크아웃을 하면서 위 내용을 전달했는데 반응이\n",
      "부정확률=86.74%, 긍정확률=14.78%\n",
      "\n",
      "기존문장: 너무나 당연한 진리를 깨닫게 되는 호텔입니다\n",
      "부정확률=36.39%, 긍정확률=62.85%\n",
      "\n",
      "기존문장: 지난번 투숙객이 먹다 버린 과자 봉지가 나오는가 하면 먼지며 곰팡이 쓴 화장실 샤워 커튼은 쇼킹이었습니다\n",
      "부정확률=90.21%, 긍정확률=10.62%\n",
      "\n",
      "기존문장: 다행히 방을 계속 바꿔줬으나 소음이 굉장히 심함\n",
      "부정확률=81.01%, 긍정확률=20.02%\n",
      "\n",
      "기존문장: 시설이 오래 되었는지 욕실 세면대가 갈라진 틈이 있고 환풍구와 천장에 얼룩이 많앗다\n",
      "부정확률=92.81%, 긍정확률=7.29%\n",
      "\n",
      "기존문장: 많은 사람들이 숙소에서 중요하게 생각하는 부분일 텐데 너무 열악했음\n",
      "부정확률=88.92%, 긍정확률=11.57%\n",
      "\n",
      "기존문장: 시설이 정말 안 좋음\n",
      "부정확률=92.44%, 긍정확률=7.42%\n",
      "\n",
      "기존문장: 제습기라도 둬야 할 거 같아요\n",
      "부정확률=69.33%, 긍정확률=31.82%\n",
      "\n",
      "기존문장: 세면대 욕실도 깨끗합니다\n",
      "부정확률=3.34%, 긍정확률=94.66%\n",
      "\n",
      "기존문장: 그나마 직원들은 친절함\n",
      "부정확률=66.93%, 긍정확률=34.59%\n",
      "\n",
      "기존문장: 위치 다만 방음이 아쉽\n",
      "부정확률=32.68%, 긍정확률=66.52%\n",
      "\n",
      "기존문장: 다만 호텔 근처엔 머가 없어요 ㅋㅋ\n",
      "부정확률=36.63%, 긍정확률=62.76%\n",
      "\n",
      "기존문장: 샤워기 자체도 완전 낡아서 그냥 사용 안함\n",
      "부정확률=75.01%, 긍정확률=26.56%\n",
      "\n",
      "기존문장: 굳이 뽑자면 신사역과 가까움\n",
      "부정확률=17.28%, 긍정확률=80.43%\n",
      "\n",
      "기존문장: 동선 배치가 이상하지 않나요\n",
      "부정확률=88.72%, 긍정확률=11.51%\n",
      "\n",
      "기존문장: 다 좋았는데 아쉬웠던 게 칫솔 치약 이런 건 원래 배치가 안 되는 건지 모르겠지만 없어서 좀 당황 스러웠어요 ㅎ\n",
      "부정확률=81.63%, 긍정확률=19.58%\n",
      "\n",
      "기존문장: 끔찍한 경험이었습니다\n",
      "부정확률=89.3%, 긍정확률=11.32%\n",
      "\n",
      "기존문장: 바닥에 카펫은 왜 이리 지져 분한 지 진짜 길 거리 바닥 수준이였습니다\n",
      "부정확률=91.08%, 긍정확률=9.37%\n",
      "\n",
      "기존문장: 모든 시설은 낙후되어 있고 침대와 베개가 매우 불편합니다\n",
      "부정확률=91.2%, 긍정확률=9.14%\n",
      "\n",
      "기존문장: 아고다에서 환불 불가 정책으로 예약한 것만 아니라면 당장 취소하고 옮겼을 텐데 후기도 잘 읽어보지 않고 예약한 스스로의 업 보려니 했네요\n",
      "부정확률=83.48%, 긍정확률=17.84%\n",
      "\n",
      "기존문장: 하지만 리버 사이드에서 좋은 추억이 있어서 언제 갈 때마다 기분은 좋네요\n",
      "부정확률=67.54%, 긍정확률=33.62%\n",
      "\n",
      "기존문장: 카운터에서 미리 요청 해야 한다고 해도 그냥 넘어가지 마세요\n",
      "부정확률=79.52%, 긍정확률=22.19%\n",
      "\n",
      "기존문장: 가성비 너무 오래된 느낌 수건에서 냄새도 나는 거 같고 그냥 80년대 느낌\n",
      "부정확률=90.91%, 긍정확률=9.2%\n",
      "\n",
      "기존문장: 서비스 청결도\n",
      "부정확률=43.55%, 긍정확률=57.12%\n",
      "\n",
      "기존문장: 위치 호텔의 노후화 담배 찌른 내\n",
      "부정확률=45.03%, 긍정확률=53.59%\n",
      "\n",
      "기존문장: 적당히 좋아요\n",
      "부정확률=15.68%, 긍정확률=81.56%\n",
      "\n",
      "기존문장: 1208호 와이파이 확인 해주세요\n",
      "부정확률=88.53%, 긍정확률=11.96%\n",
      "\n",
      "기존문장: 트리플룸 넓고 전망 좋음\n",
      "부정확률=3.25%, 긍정확률=94.83%\n",
      "\n",
      "기존문장: 직원이 불친절하고 청소가 엉망이었다\n",
      "부정확률=91.78%, 긍정확률=8.32%\n",
      "\n",
      "기존문장: 방음 최악\n",
      "부정확률=88.4%, 긍정확률=11.54%\n",
      "\n",
      "기존문장: 방 잘못 준거에요\n",
      "부정확률=89.08%, 긍정확률=11.15%\n",
      "\n",
      "기존문장: 내가 방에 있을 때 보호용 마스크를 써야 해 너무 역겨운 데이 호텔로 돌아 오지 않을 거야\n",
      "부정확률=90.5%, 긍정확률=10.06%\n",
      "\n",
      "기존문장: 호텔 내부는 생각만큼 그렇게 고급 그럽진 않습니다\n",
      "부정확률=36.63%, 긍정확률=63.16%\n",
      "\n",
      "기존문장: 뷔 폐도 사람이 너무 많아 식사 하는 시간보다 줄 서서 기다리는 시간이 더 길고 음식은 떨어졌는데 즉각 즉각 채워주지 않습니다\n",
      "부정확률=91.56%, 긍정확률=8.92%\n",
      "\n",
      "기존문장: 정말 어 이가 없어서 문도 다 열어놓고 결혼식에 동창회에 사람이 묵을 곳이 아닙니다\n",
      "부정확률=90.18%, 긍정확률=10.34%\n",
      "\n",
      "기존문장: 오래된 호텔이지만 그래도 너무 더러웠슴\n",
      "부정확률=87.26%, 긍정확률=13.49%\n",
      "\n",
      "기존문장: 조식은 7시 9시 8시 30분 이후엔 입장 불가 먹을 꺼 별로 빵 두 쪽 굽는데 5분 소요 토스트 기에 빵이 들어가 있다면 포기 바람 아침 7시부터 당일 웨딩 연회 음식 준비 자기들끼리 떠들고 시 끄러움 금 토 밤새 클럽에서 쿵짝쿵짝 클럽 아침 9시에 종료 조식당 테이블이 들 썩들썩 웨딩 연회 스케쥴 풀부킹인 듯 건물 전체가 하루 종일 음식 냄새로 찌 들어 있음\n",
      "부정확률=87.87%, 긍정확률=13.03%\n",
      "\n",
      "기존문장: 저처럼 괜히 요구할 수도 있습니다\n",
      "부정확률=90.02%, 긍정확률=10.38%\n",
      "\n",
      "기존문장: 욕조는 매우 작습니다\n",
      "부정확률=83.51%, 긍정확률=17.55%\n",
      "\n",
      "기존문장: 크게 기대는 하지 않았고 기 대 하지 않아서 다행이였음\n",
      "부정확률=37.68%, 긍정확률=61.36%\n",
      "\n",
      "기존문장: 정리도 안 돼 있고 뭐가 뭔지도 모르겠는 채로 먹었네요\n",
      "부정확률=83.24%, 긍정확률=17.91%\n",
      "\n",
      "기존문장: 위치 친절한 직원 사우나\n",
      "부정확률=3.92%, 긍정확률=93.96%\n",
      "\n",
      "기존문장: 아침에 일어나 커튼을 열어보니 밤새 눈이 내려 있는 풍경까지 볼 수 있었어요\n",
      "부정확률=7.86%, 긍정확률=89.62%\n",
      "\n",
      "기존문장: 히터 바람 건조 매우 건조\n",
      "부정확률=92.04%, 긍정확률=7.78%\n",
      "\n",
      "기존문장: 더 리버 사이드 호텔에서 숙박 자주 하는 사람입니다\n",
      "부정확률=24.99%, 긍정확률=72.53%\n",
      "\n",
      "기존문장: 이곳은 오래된 디자인과 같은 것이다\n",
      "부정확률=63.39%, 긍정확률=37.42%\n",
      "\n",
      "기존문장: 특별히 흠잡을 데 없는 호텔\n",
      "부정확률=27.49%, 긍정확률=71.14%\n",
      "\n",
      "기존문장: 제 방에 도착했을 때 한 담배 냄새가 날 뿐만 아니라 또한 벽에 얼룩이 남 가볍게 희미한 방은 조명이 더 필요해 천장 창 처럼 마지막 이 청소 되지 않은 호텔이었어요\n",
      "부정확률=91.94%, 긍정확률=8.25%\n",
      "\n",
      "기존문장: 그래서 자기네가 가지고 있는 거 복사해줬습니다\n",
      "부정확률=33.9%, 긍정확률=65.06%\n",
      "\n",
      "기존문장: 허름한 곳 카펫은 더럽고 찢어진 샤워실에 곰팡이가 있고 방은 크지만 깨끗하지 않다\n",
      "부정확률=92.65%, 긍정확률=7.6%\n",
      "\n",
      "기존문장: 음식 가짓수가 정말 많은데 아쉬운 건 초밥이나 스 시류가 적은 편입니다\n",
      "부정확률=82.31%, 긍정확률=18.98%\n",
      "\n",
      "기존문장: 먼저 룸 컨디션 최악이요 처음\n",
      "부정확률=88.18%, 긍정확률=12.46%\n",
      "\n",
      "기존문장: 근처 다른 호텔에 비해 객실비가 싸고 번화가에서도 가까워서 예약했었는데 룸 컨디션은 한강뷰도 넘 좋았고 했지만 지하주차장이 너무 노후해서 별로였어요\n",
      "부정확률=48.58%, 긍정확률=51.89%\n",
      "\n",
      "기존문장: 신사에서 술 먹고 리버 사이드에서 쉬고 사우나 가고 뷔페가 서 맛나게 밥 막고 전 너무 좋았어요\n",
      "부정확률=5.52%, 긍정확률=91.98%\n",
      "\n",
      "기존문장: 하지만 방엔 필요한 물품이 다 갖추어져 있었고요 하지만 가 너무 느렸어요\n",
      "부정확률=76.17%, 긍정확률=24.7%\n",
      "\n",
      "기존문장: 이 한마디뿐이네요\n",
      "부정확률=85.36%, 긍정확률=15.43%\n",
      "\n",
      "기존문장: 객실 이용 고객 주차 무료 레스토랑 질 좋고 맛있음\n",
      "부정확률=3.31%, 긍정확률=94.72%\n",
      "\n",
      "기존문장: 수영장 상태 알고 계셨던 모양이더라구요\n",
      "부정확률=70.04%, 긍정확률=31.38%\n",
      "\n",
      "기존문장: 위치면에선 만족 시설은 그냥 그래요\n",
      "부정확률=12.28%, 긍정확률=84.91%\n",
      "\n",
      "기존문장: 잠자리 편안 청소 상태는 별로 조식은 별로 카펫 샴푸 냄새\n",
      "부정확률=76.17%, 긍정확률=24.22%\n",
      "\n",
      "기존문장: 로비는 매우 비좁았고 직원들의 태도는 미지근 했다\n",
      "부정확률=91.52%, 긍정확률=8.8%\n",
      "\n",
      "기존문장: 위치가 좋다는 거 로비는 깨끗하다는 거 바깥에서 봣을 때 좋다는 거\n",
      "부정확률=8.41%, 긍정확률=88.96%\n",
      "\n",
      "기존문장: 1 교통이 편리 2 쾌적한 사우나 3 직원의 친절\n",
      "부정확률=3.5%, 긍정확률=94.45%\n",
      "\n",
      "기존문장: 대망의 수영장 수영모 개인이 준비해야 된 대서 구매하고 수영복까지 챙겨오느라 고생했는데 수영장 들어가니까 바닥 타일엔 이끼 잔뜩 껴 있고 레일에도 검은 곰팡이 펴 있고 물에는 이물질 둥둥 떠 다니고 도저히 수영할 수 있는 상태가 아니었어요\n",
      "부정확률=89.31%, 긍정확률=11.18%\n",
      "\n",
      "기존문장: 깨끗하고 밤에 잠을 잘 수 있는 곳 이곳에 있는 것이다\n",
      "부정확률=4.05%, 긍정확률=93.79%\n",
      "\n",
      "기존문장: 우리는 7 박 동안 호텔에 묵었습니다\n",
      "부정확률=24.17%, 긍정확률=74.03%\n",
      "\n",
      "기존문장: 다만 클럽이 있어서 주변이 너무 지저분해요\n",
      "부정확률=45.18%, 긍정확률=54.85%\n",
      "\n",
      "기존문장: 청소를 깨끗이 안함\n",
      "부정확률=89.79%, 긍정확률=10.49%\n",
      "\n",
      "기존문장: 외관에 비해 실 내는 약간 미흡했습니다\n",
      "부정확률=85.21%, 긍정확률=15.4%\n",
      "\n",
      "기존문장: 인도 파트에서도 커리만 3 4종 가까이 먹어볼 수 있구요\n",
      "부정확률=7.73%, 긍정확률=89.67%\n",
      "\n",
      "기존문장: 호텔 종업원들도 예의 바르고 일본어를 할 수 있는 사람도 몇 명인 가 있습니다\n",
      "부정확률=38.94%, 긍정확률=59.21%\n",
      "\n",
      "기존문장: 아침 식사는 아주 한정적인\n",
      "부정확률=50.0%, 긍정확률=50.69%\n",
      "\n",
      "기존문장: 또한 건강한 돈의 가치가 없고 직원들은 무례하고 기만 적인 그들은 이와 같은 중요한 것을 믿지 않을 것입니다\n",
      "부정확률=90.4%, 긍정확률=10.21%\n",
      "\n",
      "기존문장: 종종 소셜 사이트에서 할인된 가격으로 나오기도 하는데 4만원을 상회하는 가격이지만 충분히 값어치를 하는 호텔 부페로 입소문이 나 있더군요\n",
      "부정확률=15.81%, 긍정확률=81.46%\n",
      "\n",
      "기존문장: 바닥이 카펫인데 바닥 청소는 잘 안 하나 봐요 ㅠㅠ 난방도 잘 안 되고요\n",
      "부정확률=90.68%, 긍정확률=9.9%\n",
      "\n",
      "기존문장: 뷔페랑 싸우나가 있아서 남자들한테는 매우 편안한 안식 처 입니다\n",
      "부정확률=11.21%, 긍정확률=85.76%\n",
      "\n",
      "기존문장: 왠만 하면 만족하고 자는 데 씻는데 불편하면 출장 다니기 너무 불편해요\n",
      "부정확률=83.7%, 긍정확률=17.65%\n",
      "\n",
      "기존문장: 나는 그에게 갈 경우 환불 객실 가격 차이를 그는 없다\n",
      "부정확률=89.61%, 긍정확률=10.83%\n",
      "\n",
      "기존문장: 친구가 수영장을 좋아해서 고른 호텔이었는데 수영하러 들어갔다가 기 겁하고 나왔네요\n",
      "부정확률=76.32%, 긍정확률=25.08%\n",
      "\n",
      "기존문장: 조식이 너무 맛 없어요\n",
      "부정확률=89.96%, 긍정확률=10.35%\n",
      "\n",
      "기존문장: 화장실로 가야 합니다\n",
      "부정확률=89.85%, 긍정확률=10.22%\n",
      "\n",
      "기존문장: 저는 좋은 경험이 다음에 다시 올 것이다\n",
      "부정확률=19.9%, 긍정확률=78.03%\n",
      "\n",
      "기존문장: 그런데 조식이 9시까지라 8시 반 전에는 내려가야 해서 편하게 늦잠 자는 휴가를 즐기긴 어려웠습니다\n",
      "부정확률=60.63%, 긍정확률=40.68%\n",
      "\n",
      "기존문장: 조식 식당을 담당하는 직원이 혼자라 정리하고 음식 채우고 사람들 체크하고 하느라 일이 늦어짐\n",
      "부정확률=89.47%, 긍정확률=11.13%\n",
      "\n",
      "기존문장: 절대 묵지 마시길 너무 후회됩니다\n",
      "부정확률=81.55%, 긍정확률=19.66%\n",
      "\n",
      "기존문장: 침구 수건은 깨끗하고 룸에 거울이 없어요\n",
      "부정확률=53.36%, 긍정확률=46.2%\n",
      "\n",
      "기존문장: 신사역 부근이라 다양한 맛집 놀거리 커피숍 등 데이트 하기 좋구요\n",
      "부정확률=3.19%, 긍정확률=94.92%\n",
      "\n",
      "기존문장: 위치가 좋다\n",
      "부정확률=4.26%, 긍정확률=93.59%\n",
      "\n",
      "기존문장: 사진하고 완전 다름 저렴하게 나온 상품이라고 골방을 지정해준 거 같음\n",
      "부정확률=78.13%, 긍정확률=23.39%\n",
      "\n",
      "기존문장: 하지만 방음은 정말 되지 않아 옆방에서 왁자지 껄 떠드는 소리에 잠에서 깰 수 밖에 없었다\n",
      "부정확률=78.38%, 긍정확률=22.83%\n",
      "\n",
      "기존문장: 런치 뷔페를 이용했었는데 음식의 종류도 많고 고기류의 맛이 좋았습니다 가격도 일반 뷔페 수준이라 만족스러웠습니다\n",
      "부정확률=3.7%, 긍정확률=94.35%\n",
      "\n",
      "기존문장: 엘레베이터 냄새가 심함\n",
      "부정확률=90.15%, 긍정확률=10.17%\n",
      "\n",
      "기존문장: 티비는 다시보기나 영화 보기 불가능하고 딱 채널만 나오더라고요 리모컨도 그냥 음량이랑 채널만 돌릴 수 있게 돼 있었고 조명도 침대 옆에 버튼이나 리모컨에 따로 없어서 직접 왔다갔다 하면서 꺼야 됩니다\n",
      "부정확률=88.77%, 긍정확률=12.04%\n",
      "\n",
      "기존문장: 객실에서 머리카락도 나오고 하수구 배수도 제대로 안 되더라고요\n",
      "부정확률=91.36%, 긍정확률=9.06%\n",
      "\n",
      "기존문장: 방은 제대로 관리가 되고 있지 않았습니다\n",
      "부정확률=84.31%, 긍정확률=16.7%\n",
      "\n",
      "기존문장: 정말 다시는 이용하고 싶지 않은 곳이었습니다\n",
      "부정확률=90.63%, 긍정확률=9.7%\n",
      "\n",
      "기존문장: 사우나가 좋음\n",
      "부정확률=3.62%, 긍정확률=94.31%\n",
      "\n",
      "기존문장: 위치 빼고 다 별 1점짜리 였음\n",
      "부정확률=52.0%, 긍정확률=49.35%\n",
      "\n",
      "기존문장: 로비 서비스 엉망\n",
      "부정확률=43.02%, 긍정확률=55.3%\n",
      "\n",
      "기존문장: 샤워 커튼은 더러웠다\n",
      "부정확률=90.92%, 긍정확률=9.19%\n",
      "\n",
      "기존문장: 그나마 좋았던 건 트리플 룸은 생각보다 커서 넉넉하게 사용할 수 있었음\n",
      "부정확률=29.3%, 긍정확률=68.69%\n",
      "\n",
      "기존문장: 신혼여행으로 가 기 때문에 더럽혀 진 이 끔찍한 풀 거짓말을 하지 않을 것을 잡은 사람이 호텔에 우리처럼\n",
      "부정확률=89.17%, 긍정확률=11.47%\n",
      "\n",
      "기존문장: 정말 최악 최악 최악의 호텔이었습 니다\n",
      "부정확률=76.34%, 긍정확률=25.61%\n",
      "\n",
      "기존문장: 청소가 별로였어요\n",
      "부정확률=92.17%, 긍정확률=7.77%\n",
      "\n",
      "기존문장: 글쎄요 뭐 아주 편안한 침대가 있었습니다\n",
      "부정확률=51.08%, 긍정확률=49.72%\n",
      "\n",
      "기존문장: 전반적으로 다 마음에 들었습니다\n",
      "부정확률=6.19%, 긍정확률=91.25%\n",
      "\n",
      "기존문장: 그래서 바꿨는데 뭐 도 긴 개긴 이었네요\n",
      "부정확률=77.26%, 긍정확률=24.26%\n",
      "\n",
      "기존문장: 위치가 좋은 듯 안 좋은 듯 애매한 데 우선 자차 이용 하면 한남대교 건너자 마자 위치해 있어서 강남대로 막히기 전에 도착 할 수 있어 위치는 좋다고 볼 수 있습니다\n",
      "부정확률=9.87%, 긍정확률=87.4%\n",
      "\n",
      "기존문장: 수건이 오래돼서 질이 별로 구요\n",
      "부정확률=88.79%, 긍정확률=11.69%\n",
      "\n",
      "기존문장: 숙박 및 수영 시설이 많이 낙후 되어 있음\n",
      "부정확률=90.02%, 긍정확률=10.32%\n",
      "\n",
      "기존문장: 숙소 찾기가 쉬웠어요\n",
      "부정확률=3.92%, 긍정확률=93.95%\n",
      "\n",
      "기존문장: 우리는 5성급 투어에 참여했고 이 1성급도 안 되는 끔찍한 호텔에서 이틀 밤을 묵었습니다\n",
      "부정확률=40.73%, 긍정확률=58.58%\n",
      "\n",
      "기존문장: 아무래도 오래된 곳이 다 보니 위생 상태는 많이 기대하시면 실망하실 거 같습니다 가격 대비 및 위치가 좋으므로 그거 감안하시고 가시면 괜찮으실 거 같습니다\n",
      "부정확률=78.59%, 긍정확률=22.77%\n",
      "\n",
      "기존문장: 높은 층 객실 내 와이파이가 없다\n",
      "부정확률=86.57%, 긍정확률=14.17%\n",
      "\n",
      "기존문장: 피트니스 센터는 시설이 좋았습니다\n",
      "부정확률=9.68%, 긍정확률=87.66%\n",
      "\n",
      "기존문장: 위치가 좋고 가격 대비 만족합니다\n",
      "부정확률=3.77%, 긍정확률=94.24%\n",
      "\n",
      "기존문장: 이불보가 깨끗했어요\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "부정확률=3.39%, 긍정확률=94.55%\n",
      "\n",
      "기존문장: 방의 침대는 정말로 멋지다 이 호텔에 예약하기 전에 다른 옵션도 살펴보십시오\n",
      "부정확률=25.22%, 긍정확률=71.86%\n",
      "\n",
      "기존문장: 위치 좋음\n",
      "부정확률=3.62%, 긍정확률=94.43%\n",
      "\n",
      "기존문장: 어떻게 그들은 더러운 수건을 접을 수 있습니까 그 얼룩을 잡으려고 하지 않는 가사에 대한 불가능 에 대해 잊어 버리십시오 연결 한 분 다음에 없는 프런트 데스크에 전화를 걸어 에 대해 불평하면 그래서 당 신의 손님이 가 작동하지 않는다고 불평 할 때 당신은 무엇을 합니까 뭐라구 끔찍한 서비스 방에 관한 유일한 좋은 점은 추운 입니다\n",
      "부정확률=90.03%, 긍정확률=10.35%\n",
      "\n",
      "기존문장: 할 수 있는 클럽 레벨 1 액 세스할 수 있습니다\n",
      "부정확률=14.16%, 긍정확률=83.22%\n",
      "\n",
      "기존문장: 깨끗하고 조용하고 좋습니다\n",
      "부정확률=3.95%, 긍정확률=93.97%\n",
      "\n",
      "기존문장: 이 불가능합니다\n",
      "부정확률=90.74%, 긍정확률=9.27%\n",
      "\n",
      "기존문장: 침대 밑에 벌레 사체나 더러운 것들이 좀 많이 보였구요\n",
      "부정확률=91.4%, 긍정확률=8.75%\n",
      "\n",
      "기존문장: 위치 룸 컨디션이 대체적으로 많이 낡았고 욕실의 청결을 많이 높여야 할 듯 합니다\n",
      "부정확률=86.07%, 긍정확률=14.2%\n",
      "\n",
      "기존문장: 주차장 이용이 상당히 불편하고 전반적인 관리가 잘 안 되고 있음\n",
      "부정확률=92.72%, 긍정확률=7.29%\n",
      "\n",
      "기존문장: 위치가 너무 좋습니다 가로수길도 가깝고 교통도 편리합니다\n",
      "부정확률=3.16%, 긍정확률=94.95%\n",
      "\n",
      "기존문장: 습하고 어둡고 묶는 내내 너무 답답함\n",
      "부정확률=92.51%, 긍정확률=7.43%\n",
      "\n",
      "기존문장: 저는 이 호텔을 5성급 놀랐어요\n",
      "부정확률=34.07%, 긍정확률=65.03%\n",
      "\n",
      "기존문장: 외관 공사가 한창이던데 객실도 공사도 필요해 보입니다\n",
      "부정확률=91.53%, 긍정확률=8.86%\n",
      "\n",
      "기존문장: 접근성 좋음\n",
      "부정확률=3.58%, 긍정확률=94.47%\n",
      "\n",
      "기존문장: 조식 가격 자체가 저렴하여 그런 듯\n",
      "부정확률=71.17%, 긍정확률=30.63%\n",
      "\n",
      "기존문장: 아침식사가 맛있고 위치가 좋습니다\n",
      "부정확률=3.62%, 긍정확률=94.37%\n",
      "\n",
      "기존문장: 깔끔한 로비\n",
      "부정확률=3.74%, 긍정확률=94.16%\n",
      "\n",
      "기존문장: 친구들하고 오랜만에 급 계획으로 예약하여 호캉스 다녀왔습니다\n",
      "부정확률=9.38%, 긍정확률=88.05%\n",
      "\n",
      "기존문장: 그냥 냉장실에 둔 건데 왜 얼었던 건지 4 콘센 트가 딱 한 군데 밖에 없어서 핸드폰이나 다른 것들을 충전할 수가 없었음\n",
      "부정확률=90.63%, 긍정확률=9.75%\n",
      "\n",
      "기존문장: 3명이 서 3인실룸을 예약하였습니다\n",
      "부정확률=34.11%, 긍정확률=63.67%\n",
      "\n",
      "기존문장: 위치 접근성 뷰 다른 곳은 휘트니스 사용 무료 인데 여기는 1만 1천원을 내고 이용 할 수 있더라고요 그리고 조식이 조금 별로 였습니다\n",
      "부정확률=38.68%, 긍정확률=59.73%\n",
      "\n",
      "기존문장: 화장실이 별 루 입니다\n",
      "부정확률=91.28%, 긍정확률=8.8%\n",
      "\n",
      "기존문장: 이 호텔은 모든 것이 기 대 이상입니다\n",
      "부정확률=12.98%, 긍정확률=84.71%\n",
      "\n",
      "기존문장: 덮는 이 불에 립스틱 자국이 있었고 가장 윗면에 잘 보이는 곳에 묻어 있어서 교체 요청했고 추운 날이기도 했지만 실내 난방이 조금 추운 편이었습니다\n",
      "부정확률=91.85%, 긍정확률=8.36%\n",
      "\n",
      "기존문장: 위치도 좋고 직원들이 친절했습니다\n",
      "부정확률=3.24%, 긍정확률=94.85%\n",
      "\n",
      "기존문장: 신사역도 가깝고 가로수길 압구정과의 거리가 너무 좋습니다\n",
      "부정확률=3.18%, 긍정확률=94.89%\n",
      "\n",
      "기존문장: 흠 잡을 게 없었어요\n",
      "부정확률=15.7%, 긍정확률=81.51%\n",
      "\n",
      "기존문장: 연말 즐겁게 보내러 왔다가 참을 인 만 새기고 가네요\n",
      "부정확률=54.27%, 긍정확률=46.56%\n",
      "\n",
      "기존문장: 전원은 아니지만 호텔 바로 근처에 가로수길과 신사역이 있고 리무진 버스 정류장도 걸어서 100미터 정도라 매우 편리한 입지입니다\n",
      "부정확률=3.51%, 긍정확률=94.47%\n",
      "\n",
      "기존문장: 내 비즈니스 파트너가 이 호텔과 만 남 때문에 리뷰를 확인하지 않고 4 박 숙박했습니다\n",
      "부정확률=65.09%, 긍정확률=35.53%\n",
      "\n",
      "기존문장: 그런데 객실이 좀 많이 낙후되어 있더라구요\n",
      "부정확률=51.22%, 긍정확률=49.72%\n",
      "\n",
      "기존문장: 잠자리가 편안했다\n",
      "부정확률=3.75%, 긍정확률=94.13%\n",
      "\n",
      "기존문장: 요거트 없습니다\n",
      "부정확률=87.88%, 긍정확률=12.43%\n",
      "\n",
      "기존문장: 나는 이 시설이 참 좋았습니다\n",
      "부정확률=16.7%, 긍정확률=80.78%\n",
      "\n",
      "기존문장: 그래서 수영장 직원에게 나가는 길에 말했던 아 수영장 사용하셨어요\n",
      "부정확률=82.51%, 긍정확률=19.08%\n",
      "\n",
      "기존문장: 쾌쾌한 냄새도 있었음\n",
      "부정확률=86.34%, 긍정확률=14.4%\n",
      "\n",
      "기존문장: 전혀 없음\n",
      "부정확률=69.56%, 긍정확률=32.16%\n",
      "\n",
      "기존문장: 아주 깔끔하지 않아요\n",
      "부정확률=87.51%, 긍정확률=12.75%\n",
      "\n",
      "기존문장: 로비에서 방으로 들어갈 때의 실망감이 좀 있긴 하네요\n",
      "부정확률=85.91%, 긍정확률=14.49%\n",
      "\n",
      "기존문장: 침대가 푹신하고 편해요\n",
      "부정확률=3.54%, 긍정확률=94.46%\n",
      "\n",
      "기존문장: 우리가 체크인 할 시간에 다른 관광객 그룹이 체크인을 하고 있어 사람들이 짐을 끌며 길게 줄을 서 있었습니다\n",
      "부정확률=89.22%, 긍정확률=11.81%\n",
      "\n",
      "기존문장: 사람 많지 않고 조용하고 식사 후에 지하에 바에서 칵테일 한 잔 추천\n",
      "부정확률=3.92%, 긍정확률=93.91%\n",
      "\n",
      "기존문장: 청소하시는 것은 좋지만 아직 체크아웃도 할 시간이 남았음에도 불구하고 갑자기 청소하려고 들어오려는 행동이 매우 불쾌하였습니다\n",
      "부정확률=90.53%, 긍정확률=10.18%\n",
      "\n",
      "기존문장: 와 저 이런 호텔 처음\n",
      "부정확률=70.69%, 긍정확률=30.55%\n",
      "\n",
      "기존문장: 한남대교가 보여서 야경뷰도 좋았습니다\n",
      "부정확률=3.23%, 긍정확률=94.84%\n",
      "\n",
      "기존문장: 다른 방에 있는 스위치 온 오프 에서 침대 옆 테이블 또는 침대 옆 전 등이 없고 제가 전화를 충전 화솥 플러그를 뽑습니다\n",
      "부정확률=91.93%, 긍정확률=8.29%\n",
      "\n",
      "기존문장: 그리고 천장에는 거미줄도 쳐져 있었어요\n",
      "부정확률=87.56%, 긍정확률=13.62%\n",
      "\n",
      "기존문장: 모든 것이 적절했습니다\n",
      "부정확률=12.64%, 긍정확률=84.74%\n",
      "\n",
      "기존문장: 가족끼리 2 박 3일 트리플룸에 묵었는데 이렇게 더러운 곳은 처음이었음\n",
      "부정확률=80.59%, 긍정확률=20.63%\n",
      "\n",
      "기존문장: 다시는 이 용 안 할 것\n",
      "부정확률=86.04%, 긍정확률=14.44%\n",
      "\n",
      "기존문장: 요컨대 건강한 것 더불어 건강한 식생활 한 전세계 여기 만 흰색 빵 빵만 이 맛 전혀 없는 유제품 샌산성이 마가린 등 아침 식사는 비싸지만 너무 한심한 경우 기본적으로 커피 가게가 가장 비싼 커피 내가 가본 전 세계 어느 곳에서나 욕실은 샤워 커튼 물이 바닥에 간다\n",
      "부정확률=64.58%, 긍정확률=35.76%\n",
      "\n",
      "기존문장: 방 옮기느라 영수증을 잃어버려 재발급 하나 해달라고 했더니 자기네는 재발급이 안 된답니다\n",
      "부정확률=86.99%, 긍정확률=14.33%\n",
      "\n",
      "기존문장: 즉 그냥 한심한 및 위험 모두 내 동료와 저는 각 방에서 을 여러 탐탁하 지 않는 일 내 전화는 밤새 충전 소켓 그리고 나는 이 보다 제가 평소에 전원 소켓이 너무 잘 숨겨 요컨대 객실과 호텔은 해당 언어는 영어입니다\n",
      "부정확률=87.97%, 긍정확률=12.58%\n",
      "\n",
      "기존문장: 침구가 너무 편안해요\n",
      "부정확률=3.77%, 긍정확률=94.17%\n",
      "\n",
      "기존문장: 지방에 사는 친구들이 있어 어쩔 수 없이 밤을 샜네요\n",
      "부정확률=28.65%, 긍정확률=69.5%\n",
      "\n",
      "기존문장: 객실은 리뉴얼 되어 넓고 시설이 잘 되어 있어 지 내기 매우 좋았습니다\n",
      "부정확률=3.4%, 긍정확률=94.65%\n",
      "\n",
      "기존문장: 5세 미만 아동은 조식이 무료인 점도 좋은 서비스라고 생각합니다\n",
      "부정확률=7.53%, 긍정확률=89.85%\n",
      "\n",
      "기존문장: 딱 하나 위치는 좋습니다\n",
      "부정확률=14.29%, 긍정확률=82.67%\n",
      "\n",
      "기존문장: 호텔 지하는 클럽이라서 밤에는 밖에 사람이 많은데 객실에서는 시끄럽게 느껴지지 않았습니다\n",
      "부정확률=11.4%, 긍정확률=85.98%\n",
      "\n",
      "기존문장: 교통편리해서 좋음\n",
      "부정확률=3.69%, 긍정확률=94.32%\n",
      "\n",
      "기존문장: 이 있지만 별도의 이용료를 내야 되는 것도\n",
      "부정확률=86.71%, 긍정확률=13.98%\n",
      "\n",
      "기존문장: 친구들과 함께 쓰는 3인실인데 화장실 유리 아래가 투명이어서 볼일보는 게 다 보였구요\n",
      "부정확률=85.63%, 긍정확률=15.32%\n",
      "\n",
      "기존문장: 직원 분들의 친절도는 좋았습니다\n",
      "부정확률=14.7%, 긍정확률=82.6%\n",
      "\n",
      "기존문장: 다른 손님이 내가 이 호텔에 대해 언급 한 부정적인 것들의 대부분 용납 할 수 없는 것은 당신이 만지는 곳마다 먼지가 많은 방의 더러운 것과 매우 가난한 추악한 욕실입니다\n",
      "부정확률=89.02%, 긍정확률=11.62%\n",
      "\n",
      "기존문장: 딱 가격 정도 입니다\n",
      "부정확률=46.46%, 긍정확률=54.6%\n",
      "\n",
      "기존문장: 위치가 좋았다 한강 바로 옆 강남역까지 도보로 이동 가능 수영장 유료 엘리베이터에서 담배 냄새 낮은 천장\n",
      "부정확률=3.7%, 긍정확률=94.28%\n",
      "\n",
      "기존문장: 이런 뷰는 정말 난생 처음이었습니다\n",
      "부정확률=45.85%, 긍정확률=54.6%\n",
      "\n",
      "기존문장: 얼마 전에 여 자친구와 서울 강남에 예약을 잡고 갔는데 집 근처에 있는 모텔보다 더 안 좋습니다\n",
      "부정확률=90.49%, 긍정확률=9.74%\n",
      "\n",
      "기존문장: 식당 그릇 깨끗하지 않음\n",
      "부정확률=90.63%, 긍정확률=9.25%\n",
      "\n",
      "기존문장: 특히 저녁 시간은 내내 노래 부르고 마이크로 방송하고 정말 저질입니다\n",
      "부정확률=85.53%, 긍정확률=16.04%\n",
      "\n",
      "기존문장: 싱글 침대 세 개짜리 방이었는데 깨끗하고 좋았어요\n",
      "부정확률=6.78%, 긍정확률=90.98%\n",
      "\n",
      "기존문장: 수압도 좋아서 샤워하기에도 넘 좋아요\n",
      "부정확률=3.39%, 긍정확률=94.68%\n",
      "\n",
      "기존문장: 유명한 호텔이라서 후기도 안 보고 싸다고 예약한 내 자신이 정말 한심하고 바보 같았음\n",
      "부정확률=85.52%, 긍정확률=15.88%\n",
      "\n",
      "기존문장: 샤워 스폰지도 있고 목욕 용품이 엄청 잘 갖춰져 있네요\n",
      "부정확률=3.13%, 긍정확률=94.95%\n",
      "\n",
      "기존문장: 그들은 카펫이 낡은 가구와 좋은 의도하지 않은 변경할 수 있는 십 무엇을 듣고 내가 이웃이 할 수 있다\n",
      "부정확률=55.78%, 긍정확률=43.84%\n",
      "\n",
      "기존문장: 그 외에는 다 괜찮았습니다\n",
      "부정확률=77.87%, 긍정확률=22.89%\n",
      "\n",
      "기존문장: 방을 바꿔달라고 이야기를 했었는데요\n",
      "부정확률=83.25%, 긍정확률=18.41%\n",
      "\n",
      "기존문장: 실제 구멍에 있는 벽 샤워 커튼이 없 이 것은 한국인 로맨틱한 휴일을 기대하고 하는 것이 화가 나 혼자 하는 여행입니다\n",
      "부정확률=90.75%, 긍정확률=9.74%\n",
      "\n",
      "기존문장: 위치 가로수길까지 꽤 많이 걸어야 함 트리플 방을 예약하고 갔는데 이렇게 최악인 곳이 호텔이라니요\n",
      "부정확률=81.57%, 긍정확률=18.89%\n",
      "\n",
      "기존문장: 호텔은 작으나 위치 굳 부페식당 가격 대비 최고입니다\n",
      "부정확률=6.45%, 긍정확률=91.07%\n",
      "\n",
      "기존문장: 깨끗하고 주차도 여유 있어서 만족했습니다\n",
      "부정확률=3.42%, 긍정확률=94.59%\n",
      "\n",
      "기존문장: 침대에는 이불 덮개가 없어서 이 불에 비닐 포장 된 것이 다 드러나 있었고 침대 보 또한 침대에 비해 매우 작아 보였습니다\n",
      "부정확률=92.35%, 긍정확률=7.92%\n",
      "\n",
      "기존문장: 오래된 느낌이 있어요 ㅠㅠ\n",
      "부정확률=92.49%, 긍정확률=7.45%\n",
      "\n",
      "기존문장: 역까지 도 보 5분 정도라서 가깝고 간장게장 가게가 많이 있는 곳입니다\n",
      "부정확률=3.57%, 긍정확률=94.42%\n",
      "\n",
      "기존문장: 회사에 원 본 제출해야 되는데 어 이가 없더라구요\n",
      "부정확률=90.16%, 긍정확률=10.2%\n",
      "\n",
      "기존문장: 호텔 자체는 아주 낡은 카펫과 벽의 낙서 서비 스는 좋지만 객실 내 주요 포인트 2 평론가 들의 세탁 시설 5 중에 옷을 넣어 두 항목이 한 48 000 작동하는 시 인터넷 시설은 좋지 않았어요\n",
      "부정확률=90.6%, 긍정확률=9.87%\n",
      "\n",
      "기존문장: 동네 모텔보다 못합니다\n",
      "부정확률=83.12%, 긍정확률=17.65%\n",
      "\n",
      "기존문장: 위치도 좋고 조식 포함 된 저렴한 가격에 만족합니다\n",
      "부정확률=3.21%, 긍정확률=94.92%\n",
      "\n",
      "기존문장: 2시 체크인이였는데 2시 반 정도 체크인하고 방에 올라갔더니 해당 층은 모두 청소 중이여서 다시 로비에 내려와서 기다렸으나 아무도 안내를 해주지 않았음\n",
      "부정확률=89.77%, 긍정확률=11.09%\n",
      "\n",
      "기존문장: 배 게가 너무 높아서 불편 했고 객실이 좀 습했던 거 같아요\n",
      "부정확률=92.1%, 긍정확률=8.13%\n",
      "\n",
      "기존문장: 나머지는 만족\n",
      "부정확률=38.97%, 긍정확률=60.89%\n",
      "\n",
      "기존문장: 나는 할 수 없습니다\n",
      "부정확률=79.62%, 긍정확률=21.13%\n",
      "\n",
      "기존문장: 침대는 스펀지 수준 그냥 푹 꺼짐 수 페리어 옆방 말소리 다 들림 디럭스에 차 마실 테이블과 의 자 없음\n",
      "부정확률=89.72%, 긍정확률=10.66%\n",
      "\n",
      "기존문장: 건물은 번 지르르하게 좋아 보이나 내용은 그다지\n",
      "부정확률=57.29%, 긍정확률=43.55%\n",
      "\n",
      "기존문장: 특히 화장실 쪽이 급해서 하루 묵었지만 추천 하고 싶지가 않네요\n",
      "부정확률=85.32%, 긍정확률=15.92%\n",
      "\n",
      "기존문장: 코로나가 잠잠해지면 또 들려볼게요 늘 건강하세요\n",
      "부정확률=13.52%, 긍정확률=83.77%\n",
      "\n",
      "기존문장: 보도 가로수길 신사역을 통한 경복궁 등 방문에 좋고 깔끔하게 관리 잘 되는 만족스러운 곳입니다\n",
      "부정확률=3.22%, 긍정확률=94.86%\n",
      "\n",
      "기존문장: 방은 큰 침대인 줄 알았는데 싱글 2개 붙여놓은 거라 잘 때 불편했어요\n",
      "부정확률=79.86%, 긍정확률=21.3%\n",
      "\n",
      "기존문장: 조식을 그냥 한 쪽으로 몰아놓으면 좀 더 괜찮아 보일 텐데 런치 뷔페 먹는 그 위치 그대로 구석 구석 어떤 곳인 텅 비어 있고 어떤 곳인 부실하게 채워져 있고 하니까 먹고 싶은 맘이 사라져요\n",
      "부정확률=87.23%, 긍정확률=13.75%\n",
      "\n",
      "기존문장: 신사역 근접 가로수길 근접 한 강 조망권\n",
      "부정확률=3.57%, 긍정확률=94.44%\n",
      "\n",
      "기존문장: 이 부분 너무 좋네요\n",
      "부정확률=21.28%, 긍정확률=76.61%\n",
      "\n",
      "기존문장: 나는 정말 별 네 개짜리 호텔이라고 부를 것이다\n",
      "부정확률=71.18%, 긍정확률=30.88%\n",
      "\n",
      "기존문장: 만약 깨끗한 객실이나 좋은 서비스에 익숙해 진 분이시라면 차라리 다른 데 머무르시는 게 좋을 거에요\n",
      "부정확률=78.25%, 긍정확률=22.95%\n",
      "\n",
      "기존문장: 수영장 물이 너무 차가워 이용하기 어려웠습 미다 숙서 청결 상태가 좋지는 않았습니다\n",
      "부정확률=91.43%, 긍정확률=8.99%\n",
      "\n",
      "기존문장: 방은 더러웠고 굉장히 낡았어요\n",
      "부정확률=87.88%, 긍정확률=12.76%\n",
      "\n",
      "기존문장: 방 조명 밝기가 너무 어둡습니다\n",
      "부정확률=92.7%, 긍정확률=7.15%\n",
      "\n",
      "기존문장: 비싼 방은 정말 룸도 좋고 화장실은 제 외 전망도 좋고 저렴한 방은 창문 밖을 벽 부페가 유명합니다 가로수길까지 걸어서 15분 이상 소요됩니다\n",
      "부정확률=13.5%, 긍정확률=83.77%\n",
      "\n",
      "기존문장: 다만 연말 송년회가 많은 날이라 그런지 밤 늦게까지 시끄럽더라구요\n",
      "부정확률=34.09%, 긍정확률=65.05%\n",
      "\n",
      "기존문장: 정보의 부크렛 없습니다\n",
      "부정확률=91.76%, 긍정확률=8.02%\n",
      "\n",
      "기존문장: 원래 없으셨더라도 배치 좀 해주셨으면 하는 ㅎ 그거 말고는 다 좋았어요\n",
      "부정확률=76.19%, 긍정확률=24.96%\n",
      "\n",
      "기존문장: 가격 비교적 저렴 방 깨끗\n",
      "부정확률=3.81%, 긍정확률=94.18%\n",
      "\n",
      "기존문장: 욕실에 머리카락 붙어 있음\n",
      "부정확률=92.3%, 긍정확률=7.68%\n",
      "\n",
      "기존문장: 그냥 여관이나 모텔급 욕조에 서서 머리를 만 지면 팔이 천장에 다 아서 몹시 불편\n",
      "부정확률=91.91%, 긍정확률=8.3%\n",
      "\n",
      "기존문장: 사랑에 이번에는 상상하는 영어 이름 좀 진정 될 수 있습니다\n",
      "부정확률=54.19%, 긍정확률=45.45%\n",
      "\n",
      "기존문장: 화장실 문 닫아도 냄새가 밖으로 새어나옴\n",
      "부정확률=92.09%, 긍정확률=8.02%\n",
      "\n",
      "기존문장: 덕분에 혼자만의 여행과 힐링 잘 하고 갑니다\n",
      "부정확률=5.15%, 긍정확률=92.52%\n",
      "\n",
      "기존문장: 위치가 좋았습니다\n",
      "부정확률=3.78%, 긍정확률=94.21%\n",
      "\n",
      "기존문장: 그냥 싱글 하나를 쓰는 게 낫겠다 싶었네요\n",
      "부정확률=57.29%, 긍정확률=44.18%\n",
      "\n",
      "기존문장: 호텔 내 부페는 워낙에 가격 대비 맛이 좋은 곳으로 유명합니다\n",
      "부정확률=7.5%, 긍정확률=89.99%\n",
      "\n",
      "기존문장: 위치는 정말 좋은 곳에 위치 방 진짜 ㅎ 임\n",
      "부정확률=5.26%, 긍정확률=92.46%\n",
      "\n",
      "기존문장: 제 옷도 확인해 드렸 네요\n",
      "부정확률=43.33%, 긍정확률=57.21%\n",
      "\n",
      "기존문장: 여행 다니면서 숙소가 구석진데 있으면 왔다갔다 시간 낭비 교통비 낭비가 너무 아까운데 가족들도 다 매우 편한 하다고 만족했어요\n",
      "부정확률=5.52%, 긍정확률=92.02%\n",
      "\n",
      "기존문장: 주차장 엘레베이터로 연회장 음식을 나르는 거 같았는데 그 더러운 주차장을 통해서 박스를 나르는데 박스 사이로 냉동 삼겹살이 삐져 나와서 깜짝 놀랐음 여기서 밥 먹는 사람들 진짜 불쌍 카펫트 바닥도 너무 더러워서 도저히 발을 디디고 싶지 않았고 객실에 비치된 실내화는 실밥이 다 터져 있었음 도대체 몇 사람이 신었길래 이렇게 터져 있는지 화장실 벽은 곳곳에 곰팡이가 끼어 있고 바닥은 당연히 더러웠고 샤워기를 틀었더니 검은 무언가가 계속 나와서 위쪽에 달린 샤워기로 씻었음\n",
      "부정확률=89.43%, 긍정확률=11.29%\n",
      "\n",
      "기존문장: 호텔 부페를 여러 번 이용했습니다\n",
      "부정확률=15.86%, 긍정확률=81.37%\n",
      "\n",
      "기존문장: 다신 가지 않을 호텔\n",
      "부정확률=57.29%, 긍정확률=44.4%\n",
      "\n",
      "기존문장: 수영장 및 부대시설 이용 시 추가요금을 지불해야 합니다\n",
      "부정확률=86.69%, 긍정확률=14.09%\n",
      "\n",
      "기존문장: 저는 원래 한 방에 있는 11 담배 냄새가 카펫 욕실 바닥이 너무 더러워서 맨발로 걸어 다니는 수 있습니다\n",
      "부정확률=91.24%, 긍정확률=8.95%\n",
      "\n",
      "기존문장: 당신이 얻는 수건은 표준 이하 낡고 싸고 작습니다\n",
      "부정확률=87.45%, 긍정확률=12.92%\n",
      "\n",
      "기존문장: 전망 좋은 11층에서 묵었는데 도시 도로 뷰 너무 아름답습니다\n",
      "부정확률=4.67%, 긍정확률=93.3%\n",
      "\n",
      "기존문장: 요약 1 담배 냄새가 났다고 하는 오후 8시경 제가 이용한 시간에 바로 제보 또는 확인을 하러 오지 않고 체크아웃할 때 와서 10만원 벌금 요구한 것 2 청소부가 키가 없다\n",
      "부정확률=90.43%, 긍정확률=10.12%\n",
      "\n",
      "기존문장: 근데 그 정도의 가치가 안 되는 것 같네요\n",
      "부정확률=75.99%, 긍정확률=24.83%\n",
      "\n",
      "기존문장: 전화번호 드렸으니 이 글 보면 꼭 불만 피드백 좀 부탁드릴게요\n",
      "부정확률=84.4%, 긍정확률=16.64%\n",
      "\n",
      "기존문장: 에어컨도 가동 잘 됩니다\n",
      "부정확률=3.56%, 긍정확률=94.33%\n",
      "\n",
      "기존문장: 근데 창 밖 뷰가 좀 가성비 좋음\n",
      "부정확률=65.38%, 긍정확률=35.98%\n",
      "\n",
      "기존문장: 화장실 청결도가 좀 아쉬웠어요\n",
      "부정확률=91.53%, 긍정확률=8.79%\n",
      "\n",
      "기존문장: 주말에는 가끔 엄청 시끄러운 사람들이 옆방에 들어서 휴식을 방해할 때도 있습니다\n",
      "부정확률=86.32%, 긍정확률=14.75%\n",
      "\n",
      "기존문장: 가격 대비 깨끗하고 뷰가 좋아요\n",
      "부정확률=3.59%, 긍정확률=94.4%\n",
      "\n",
      "기존문장: 위치는 괜찮았어요\n",
      "부정확률=5.26%, 긍정확률=92.47%\n",
      "\n",
      "기존문장: 서울여행 예전 느낌 생각하며 리버 사이드 호텔 방문 했습니다\n",
      "부정확률=12.28%, 긍정확률=85.09%\n",
      "\n",
      "기존문장: 리버 사이드 호텔 뷔페 더가든 키친 다녀왔습니다\n",
      "부정확률=16.62%, 긍정확률=81.17%\n",
      "\n",
      "기존문장: 객실로 올라가는 엘리베이터 바로 옆이 나이트 클럽 입장하는 곳 엘리베이터 타러 가는데 나이트클럽 안내하는 분이 어디가십니까 자러 갑니다\n",
      "부정확률=84.96%, 긍정확률=16.3%\n",
      "\n",
      "기존문장: 위치와 방에서 한남대교와 한강이 보이는 뷰가 좋았습니다\n",
      "부정확률=3.21%, 긍정확률=94.83%\n",
      "\n",
      "기존문장: 서울에 있는 지하철을 타고 이동할 때 필요하거나 필요 변경할 수 있습니다\n",
      "부정확률=11.54%, 긍정확률=85.76%\n",
      "\n",
      "기존문장: 단체 방문객이 있어서 조금 소란스러워서 별한 개 뺐어요\n",
      "부정확률=79.22%, 긍정확률=21.88%\n",
      "\n",
      "기존문장: 가격만큼 이었어요\n",
      "부정확률=53.61%, 긍정확률=47.09%\n",
      "\n",
      "기존문장: 한 30년 정도 과거 수준인데 더 더러워져 있다고 생각하시면 되요 지하 나이트클럽에서 올라오는 원나잇 손님용 객실 토할 뻔 했어요\n",
      "부정확률=83.36%, 긍정확률=17.58%\n",
      "\n",
      "기존문장: 객실 상태가 너무 낡았고 홀웨이에서 음식 냄새와 바베큐 냄새가 너무 많이 났다\n",
      "부정확률=92.85%, 긍정확률=7.27%\n",
      "\n",
      "기존문장: 겨울철 객실 온도가 추움 객실 난방 조절 불가능\n",
      "부정확률=92.48%, 긍정확률=7.59%\n",
      "\n",
      "기존문장: 위치와 가성비 면으로는 좋았는데 호텔 치고는 비위생적이었습니다\n",
      "부정확률=27.06%, 긍정확률=71.35%\n",
      "\n",
      "기존문장: 뒷편에 한 강 야경이 좋고 위치가 강남과 가까워 차 타고 다니기 편했습니다\n",
      "부정확률=3.11%, 긍정확률=94.97%\n",
      "\n",
      "기존문장: 두 번 다시 이용할 일은 없을 것 같아요\n",
      "부정확률=71.76%, 긍정확률=30.03%\n",
      "\n",
      "기존문장: 욕실에 곰팡이 제거는 좀 해야 할 듯 하네요\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "부정확률=86.0%, 긍정확률=14.56%\n",
      "\n",
      "기존문장: 모텔보다 못함\n",
      "부정확률=81.68%, 긍정확률=19.16%\n",
      "\n",
      "기존문장: 조식 서비스는 만족 빵 종류 몇 개와 잼 음료 시리얼이 준비되어 있고 가짓수는 적 다 몇 개의 조식 메뉴를 선택하면 직접 조리해서 테이블로 가져다 준다 따끈따끈한 스크램블이 마음에 들어 아침만큼은 두둑히 먹었다\n",
      "부정확률=6.76%, 긍정확률=90.49%\n",
      "\n",
      "기존문장: 객실 욕실도 머리카락 등 이물질이 많고 컵도 얼룩이 많았어요\n",
      "부정확률=92.32%, 긍정확률=7.75%\n",
      "\n",
      "기존문장: 바로 앞에 편의점도 있고 직원분도 굉장히 친절했어요\n",
      "부정확률=3.51%, 긍정확률=94.48%\n",
      "\n",
      "기존문장: 화장실에선 뭔가 특유의 좋지 않은 냄새가 났고 이불은 설명하고 싶지 않지만 뭔가 이물질이 두 군데나 묻어 있었음\n",
      "부정확률=90.33%, 긍정확률=10.23%\n",
      "\n",
      "기존문장: 연말 모임으로 인기가 많아요\n",
      "부정확률=10.56%, 긍정확률=86.67%\n",
      "\n",
      "기존문장: 코로나 때문에 뷔페가 폐쇄된 것까진 이해하는데 그렇다고 주스 하나에 간에 기 별도 안 가는 작은 빵 두 덩어리는 해도 해도 너무하다 그럼 조식비를 감해 주셔야 하는데 카운터에서 그런 말도 없고 그냥 조식이 사라졌대 미리 알림도 전혀 없었다\n",
      "부정확률=89.26%, 긍정확률=11.47%\n",
      "\n",
      "기존문장: 부페는 처음에 돌잔치로 방문했다가 그 이후에는 개인적으로 자주 가는 편인데 음식이 전반적으로 가격 대비 괜찮습니다\n",
      "부정확률=11.93%, 긍정확률=85.49%\n",
      "\n",
      "기존문장: 수건이랑 옷에서 토 냄새 같은 거 나 고 좁고 가격 대비 별로입니다\n",
      "부정확률=92.34%, 긍정확률=7.91%\n",
      "\n",
      "기존문장: 기대 1도 안 하고 가면 괜찮음\n",
      "부정확률=43.14%, 긍정확률=57.27%\n",
      "\n",
      "기존문장: 신경 써 주세요\n",
      "부정확률=74.02%, 긍정확률=26.81%\n",
      "\n",
      "기존문장: 욕조에 천천히 겁 먹을 밤에는 두 개의 죽은 파리 그냥 놀아요\n",
      "부정확률=85.53%, 긍정확률=15.2%\n",
      "\n",
      "기존문장: 가격 대비 괜찮은 뷔페라고 생각합니다\n",
      "부정확률=10.42%, 긍정확률=86.71%\n",
      "\n",
      "기존문장: 찾기 쉬운 위치 친절한 직원 및 서비스 양호 조식이 매우 아쉬웠습니다\n",
      "부정확률=9.11%, 긍정확률=88.14%\n",
      "\n",
      "기존문장: 사우나가 좋고 음식이 꽤 맛있다\n",
      "부정확률=3.34%, 긍정확률=94.71%\n",
      "\n",
      "기존문장: 호텔 무난합니다\n",
      "부정확률=5.32%, 긍정확률=92.46%\n",
      "\n",
      "기존문장: 역이랑 가까워서 접근성이 좋고 침구가 편안했습니다\n",
      "부정확률=3.12%, 긍정확률=94.97%\n",
      "\n",
      "기존문장: 거의 매년 이용하고 있습니다\n",
      "부정확률=14.38%, 긍정확률=82.88%\n",
      "\n",
      "기존문장: 그 점을 빼곤 서비스 전체 모든 면에서 괜찮았습니다\n",
      "부정확률=87.23%, 긍정확률=12.77%\n",
      "\n",
      "기존문장: 가성비 무난한 호텔임\n",
      "부정확률=5.92%, 긍정확률=91.65%\n",
      "\n",
      "기존문장: 물은 오조오 억 년 고 인물같이 밑바닥에 부산물과 이끼 같은 것들이 가득했네요\n",
      "부정확률=82.48%, 긍정확률=18.42%\n",
      "\n",
      "기존문장: 바로 앞에 클럽이 있어서 그런지 새벽에 들어올 때 매우 시끄럽고 술 취한 사람이 많아서 좀 위험해 보임 바닥에 담배꽁초 고 많고 호텔은 좋음\n",
      "부정확률=58.13%, 긍정확률=43.18%\n",
      "\n",
      "기존문장: 급하게 잡은 거고 젤 싼 객실이었지만 그래도 호텔인데 이 가격에 그냥 조식 포기하구 좀 비싼 모텔가시는 게 나을 듯 오래된 호텔인 건 알지만 좀 너무 하네요\n",
      "부정확률=73.04%, 긍정확률=28.1%\n",
      "\n",
      "기존문장: 창문 밖은 옆 건물 지붕으로 막혀 있어서 아 무것도 안 보였어요\n",
      "부정확률=90.31%, 긍정확률=9.96%\n",
      "\n",
      "기존문장: 주변이 많이 지저분하고 특히 주차장도 청결 상태가 불량하고 승강기 주변도 관리 상태가 좋지 아니함\n",
      "부정확률=92.95%, 긍정확률=7.25%\n",
      "\n",
      "기존문장: 와서 기대하는 검사는 4성급 호텔로 제가 잘못된 한 남자 아이 온 것 같은 냄새가 나는 복도에 있는 굽기 덕분에 저는 이 호텔이 다른 냄새에 익숙해 있습니다\n",
      "부정확률=88.72%, 긍정확률=12.09%\n",
      "\n",
      "기존문장: 모텔보다 더 별로 고 여기는 절대 추천 안 할 거 같네요\n",
      "부정확률=77.1%, 긍정확률=23.81%\n",
      "\n",
      "기존문장: 에어컨도 청소가 잘 안 되어 있었어요\n",
      "부정확률=92.62%, 긍정확률=7.26%\n",
      "\n",
      "기존문장: 내려가서 말해야 됨\n",
      "부정확률=82.54%, 긍정확률=18.91%\n",
      "\n",
      "기존문장: 리뉴얼된 룸이어서 그런지 넓고 쾌적하게 하루 머물기 너무 좋았습니다 가로수길도 근처이고 요즘 특가 행사도 많이 해서인지 가성 비 대비 너무 만족합니다\n",
      "부정확률=3.31%, 긍정확률=94.79%\n",
      "\n",
      "기존문장: 다음에도 갈게요\n",
      "부정확률=16.66%, 긍정확률=80.91%\n",
      "\n",
      "기존문장: 비싸게 먹은 주말 디너의 추억 외엔 너무 불쾌했습니다\n",
      "부정확률=87.76%, 긍정확률=12.93%\n",
      "\n",
      "기존문장: 휴지도 여분을 두지 않아서 당황스러웠습니다\n",
      "부정확률=91.6%, 긍정확률=8.66%\n",
      "\n",
      "기존문장: 일식 파트 의 튀김류는 아주 바삭하고 고기도 웬만한 종류의 고기를 다 맛볼 수 있습니다\n",
      "부정확률=4.73%, 긍정확률=93.0%\n",
      "\n",
      "기존문장: 객실이 비어 있으니 좋은 층으로도 바꿔주셨습니다\n",
      "부정확률=7.49%, 긍정확률=90.09%\n",
      "\n",
      "기존문장: 여기는 클럽이나 성형외과로 돈 버는 곳인가봐요\n",
      "부정확률=25.05%, 긍정확률=72.8%\n",
      "\n",
      "기존문장: 뷔페라고 알고 있었지만 뷔페란 말은 보지도 못했네요\n",
      "부정확률=46.43%, 긍정확률=54.47%\n",
      "\n",
      "기존문장: 일이 있어서 신사동 쪽에 호텔을 잡은 건데 가격 대비 위치 인테리어 위생 상태 직원 친절도 다 마음에 들었습니다\n",
      "부정확률=3.84%, 긍정확률=94.06%\n",
      "\n",
      "기존문장: 3 모텔 같은 냉장고 뭐가 잘못됐는지 물이 꽁꽁 얼어버려서 먹지를 못했음\n",
      "부정확률=92.16%, 긍정확률=8.04%\n",
      "\n",
      "기존문장: 욕조에 머리카락 화장대 대리석은 곰팡이가 피어 있었음\n",
      "부정확률=92.79%, 긍정확률=7.45%\n",
      "\n",
      "기존문장: 진짜 시끄러움 엘리베이터 기다리는데 왠만한 모텔이 낫겠다 싶었어요\n",
      "부정확률=82.48%, 긍정확률=18.59%\n",
      "\n",
      "기존문장: 아래층이라 전망이 안 좋고 욕조 및 숙소 시설이 노후화가 된 듯 합니다\n",
      "부정확률=91.76%, 긍정확률=8.57%\n",
      "\n",
      "기존문장: 우리는 다시 방을 옮겨야 했는데 에어컨 작동 하지 않았다\n",
      "부정확률=89.43%, 긍정확률=11.53%\n",
      "\n",
      "기존문장: 어이없고 황당하네요\n",
      "부정확률=72.69%, 긍정확률=28.45%\n",
      "\n",
      "기존문장: 후문이 좀 더러웠어요\n",
      "부정확률=91.17%, 긍정확률=9.04%\n",
      "\n",
      "기존문장: 방음이 안 됨 조식이 떨어졌는데 채워지지 않아서 늦게 오는 사람들은 먹을 만한 게 없었음\n",
      "부정확률=91.07%, 긍정확률=9.56%\n",
      "\n",
      "기존문장: 비데가 없음\n",
      "부정확률=85.34%, 긍정확률=15.58%\n",
      "\n",
      "기존문장: 위치가 좋아 욤 아래 클럽도 있음\n",
      "부정확률=4.56%, 긍정확률=93.24%\n",
      "\n",
      "기존문장: 카운터에 가서 체크인 한 방이 기 때문에 더 이상 사용할 수 없는 결혼식 저는 왜 지난 6월 예약 및 관리자가 이전에 예약을 해 두 번 반입 그의 대답은 나는 몰라 죄송합니다\n",
      "부정확률=85.34%, 긍정확률=15.97%\n",
      "\n",
      "기존문장: 적어 보네요\n",
      "부정확률=79.09%, 긍정확률=21.98%\n",
      "\n",
      "기존문장: 천장에 휴지가 붙어 있었음\n",
      "부정확률=92.57%, 긍정확률=7.74%\n",
      "\n",
      "기존문장: 두 번 올 생각 전혀 없음\n",
      "부정확률=78.2%, 긍정확률=22.65%\n",
      "\n",
      "기존문장: 이런 손님들은 안 받을 수는 없는 건가요 술 마시고 욕하고 엄청 시끄러웠습니다\n",
      "부정확률=91.53%, 긍정확률=8.83%\n",
      "\n",
      "기존문장: 청결도 엉망이 고 침대 밑 생수병 처음부터 퇴실하는 4일 후까지 그대로 있네요\n",
      "부정확률=15.69%, 긍정확률=81.6%\n",
      "\n",
      "기존문장: 아이들도 정말 맛있게 잘 먹었어요\n",
      "부정확률=4.9%, 긍정확률=92.87%\n",
      "\n",
      "기존문장: 엘레베이터가 객실 전용이 아니라서 연회장 드나드는 고객 때문에 번잡한 점은 별로였어요\n",
      "부정확률=80.7%, 긍정확률=20.47%\n",
      "\n",
      "기존문장: 수영장은 실제로 해 기본적인 한국식 은 얕은 수영장과 사우나가 있다\n",
      "부정확률=72.92%, 긍정확률=27.84%\n",
      "\n",
      "기존문장: 호텔의 디자인은 좋지 못했고 특히 로비가 그랬습니다\n",
      "부정확률=77.8%, 긍정확률=23.91%\n",
      "\n",
      "기존문장: 호텔 뷔페 답게 다양한 음식이 잘 준비되어 있습니다\n",
      "부정확률=3.87%, 긍정확률=94.04%\n",
      "\n",
      "기존문장: 호텔 이용보다는 부 페 이용으로 좀 더 추천하고 싶네요\n",
      "부정확률=25.96%, 긍정확률=72.04%\n",
      "\n",
      "기존문장: 너무 많아서 뭐부터 말해야 할지 모르겠네요\n",
      "부정확률=76.85%, 긍정확률=24.95%\n",
      "\n",
      "기존문장: 담배 냄새가 역함\n",
      "부정확률=90.06%, 긍정확률=10.07%\n",
      "\n",
      "기존문장: 나는 이 호텔을 여행사에 확인 후 예약을 온라인 사진이 온라인 나타내는 실제 호텔 수 없습니다\n",
      "부정확률=91.53%, 긍정확률=8.83%\n",
      "\n",
      "기존문장: 식당이 많습니다\n",
      "부정확률=18.02%, 긍정확률=79.82%\n",
      "\n",
      "기존문장: 직원의 응대가 조금 더 상냥했으면 좋겠음\n",
      "부정확률=90.43%, 긍정확률=9.87%\n",
      "\n",
      "기존문장: 전송 예약을 등이 있다\n",
      "부정확률=91.45%, 긍정확률=8.44%\n",
      "\n",
      "기존문장: 가로수길도 가깝고 지하철역도 가까워서 여행하기 너무 좋아요\n",
      "부정확률=3.41%, 긍정확률=94.68%\n",
      "\n",
      "기존문장: 우선 지하철역이랑 가까운 편이 이라서 괜찮았는데 객실 청소가 제대로 안 되어 있어서 좀 실망스러웠습니다\n",
      "부정확률=83.84%, 긍정확률=17.03%\n",
      "\n",
      "기존문장: 대중 교통 이용하셔도 지하철은 도보로 5분만 가면 되서 편 한데 버스는 중앙 차선 까지 걸어가는데 10분 이상 소요되서 좀 불편합니다\n",
      "부정확률=42.07%, 긍정확률=57.91%\n",
      "\n",
      "기존문장: 욕실도 넓어서 괜찮았고 휴식을 취하기엔 충분히 좋았습니다\n",
      "부정확률=3.48%, 긍정확률=94.55%\n",
      "\n",
      "기존문장: 식당과는 다르게 굉장히 오래된 느낌이 듭니다\n",
      "부정확률=87.91%, 긍정확률=12.29%\n",
      "\n",
      "기존문장: 아직 체크아웃 시간이 안 되서 자고 있었는데 갑자기 초인종을 누르시더니 대답할 시간도 안 주고 바로 문을 열으셔서 당화했고 기분이 상당히 불쾌하였습니다\n",
      "부정확률=90.94%, 긍정확률=9.5%\n",
      "\n",
      "기존문장: 냄새나 고 이미지와 전혀 다른 룸에 전혀 호텔이라고 할 수 없는 룸 상태 샤워 가 운조차 없음\n",
      "부정확률=89.16%, 긍정확률=10.94%\n",
      "\n",
      "기존문장: 뭐 다른 객실은 어 떨지 모르겠다만 제가 묵었던 트리플룸은 이 두 가지만 제외하면 위치도 좋고 깨끗하고 좋습니다\n",
      "부정확률=46.51%, 긍정확률=52.48%\n",
      "\n",
      "기존문장: 실내는 청결감이 있습니다\n",
      "부정확률=16.05%, 긍정확률=81.1%\n",
      "\n",
      "기존문장: 안 되어\n",
      "부정확률=72.78%, 긍정확률=29.4%\n",
      "\n",
      "기존문장: 최악의 숙소\n",
      "부정확률=80.18%, 긍정확률=21.0%\n",
      "\n",
      "기존문장: 객실이 일반 모텔보다도 훨씬 더럽 움 디럭스룸에서 묶었는데 바닥에 실핀들이 아주 많이 발견됨 입장하자 마자 보이는 그 많은 실핀들이 왜 호텔 메이드 눈에는 안 보였을까\n",
      "부정확률=86.47%, 긍정확률=14.77%\n",
      "\n",
      "기존문장: 룸에서 쾌쾌한 냄새 나고 더블룸으로 예약 햇는데 트윈룸으로 배정해주고 사이트에서 본 룸 상태와 완전 다른 룸을 배정 받아서 확인하니 요청을 해야 배정해준다는 어의 없는 답변 같은 디럭스지만 전혀 다른 디럭스 룸을 경험했네요\n",
      "부정확률=48.29%, 긍정확률=51.78%\n",
      "\n",
      "기존문장: 조절 할 수 있는 리모콘도 있구요\n",
      "부정확률=3.52%, 긍정확률=94.34%\n",
      "\n",
      "기존문장: 너무 충격적 리모델링해야 할 듯 처음\n",
      "부정확률=91.24%, 긍정확률=8.62%\n",
      "\n",
      "기존문장: 수건에는 콧물인지 뭔지 이상한 게 붙어 있고 청소 하지 말라는 사인 걸었는데도 그냥 들어와 맘대로 청소하고 청소도 깨끗하게도 하지 않으면서 정말 돈 아깝고 애인한테 미안하고 다시는 절대 가 고 싶지 않고 누가 간다고 해도 짐싸들고 말리고 싶네요\n",
      "부정확률=89.38%, 긍정확률=11.46%\n",
      "\n",
      "기존문장: 밤새 고 새벽에 나오는데 입구 쪽에는 나이트 클럽 이용객들이 있어서 불편했고 객실 올라가는 엘리베이터 타는 쪽에도 클럽 담배 냄새와 소음으로 내가 호텔을 이용하는지 알 수 없을 정도로 짜증이 났습니다\n",
      "부정확률=91.43%, 긍정확률=9.08%\n",
      "\n",
      "기존문장: 강남 고속터미널도 호텔 앞에 지하철을 타니 2 정거장 위치 정말 최고로 마음에 들었습니다\n",
      "부정확률=3.42%, 긍정확률=94.6%\n",
      "\n",
      "기존문장: 그곳에는 작은 엘레베이터 3개가 있었는데 그 중 하나는 호텔 관리 직원이 세간을 옮기는데 사용되어 투숙객들은 겨우 두개의 엘레베이터만 이용할 수 있었습니다\n",
      "부정확률=86.81%, 긍정확률=13.97%\n",
      "\n",
      "기존문장: 당연히 있을 거라 생각해서 갔는데 없더라구요 ㅎ\n",
      "부정확률=77.56%, 긍정확률=24.0%\n",
      "\n",
      "기존문장: 식사는 좋았습니다\n",
      "부정확률=10.51%, 긍정확률=86.71%\n",
      "\n",
      "기존문장: 워낙에 지저분하다는 평을 듣고 가서 그런지 실제로도 어둡고 좁아서 싱숭생숭해서 도로 맥주 사와 서 한 잔 들이 키고 자버렸어요\n",
      "부정확률=91.09%, 긍정확률=9.39%\n",
      "\n",
      "기존문장: 카펫을 교체할 팔요가 절실함 더러워도 너무 더러운 상태\n",
      "부정확률=91.97%, 긍정확률=8.04%\n",
      "\n",
      "기존문장: 아침 조식을 줘서 좋았어요\n",
      "부정확률=6.14%, 긍정확률=91.37%\n",
      "\n",
      "기존문장: 저는 후기 잘 남기는 사람 아닌데 정말 남겨야 할 거 같아서 이렇게 찾아서 남깁니다\n",
      "부정확률=37.76%, 긍정확률=62.7%\n",
      "\n",
      "기존문장: 장마철에 창문을 열어놓고 비가 들이 쳣는지 침대 시트에서 시큼한 냄새가 이불을 교체했음에도 가 시질 않아 밤새 고생 좌변기 청소가 덜 되어 누렇게 튀긴 자국 그대로여서 호출 재청소 부탁함\n",
      "부정확률=92.03%, 긍정확률=8.4%\n",
      "\n",
      "기존문장: 아침 식사도 호텔 근처에 카 페나 설렁탕 가게가 있어 좋았습니다\n",
      "부정확률=3.88%, 긍정확률=94.0%\n",
      "\n",
      "기존문장: 방음이 잘 안 되어 있는 것 같습니다\n",
      "부정확률=92.98%, 긍정확률=6.96%\n",
      "\n",
      "기존문장: 방문의 자물쇠 걸이는 제대로 설치되지 않았고 이것은 비단 우리 방만 그런 것은 아니었습니다\n",
      "부정확률=81.19%, 긍정확률=20.59%\n",
      "\n",
      "기존문장: 오래된 호텔이니 그저 그러려니 그냥 위치 좋은 모텔 정도라고 생각하심 될 듯 ㅎㅎ 아 프론트 전화 겁 내 안 받아요\n",
      "부정확률=46.27%, 긍정확률=54.02%\n",
      "\n",
      "기존문장: 첫날 수영장 운영을 하지 않았음\n",
      "부정확률=83.32%, 긍정확률=17.82%\n",
      "\n",
      "기존문장: 참고로 수프는 다 식어서 나오고 고기는 급하게 나오다 보니 안에 피가 흥건히 고 여 나옵니다 가족 모임 주말에 잡아야 되면 서울에 호텔이 많으니 여기는 가지 마세요\n",
      "부정확률=33.09%, 긍정확률=65.84%\n",
      "\n",
      "기존문장: 부페식당이 상대적으로 저렴하고 좋습니다\n",
      "부정확률=5.21%, 긍정확률=92.52%\n",
      "\n",
      "기존문장: 직원 분들의 빠른 업무 대응은 좋았습니다\n",
      "부정확률=9.74%, 긍정확률=87.58%\n",
      "\n",
      "기존문장: 조금 너무 어둡다는 거 빼고는 요ㅠㅠ\n",
      "부정확률=80.89%, 긍정확률=20.35%\n",
      "\n",
      "기존문장: 친구와 함께 1박했는데 시설이 세련되거나 새로운 느낌은 아니지만 무난하고 편안한 느낌이었습니다\n",
      "부정확률=16.12%, 긍정확률=81.37%\n",
      "\n",
      "기존문장: 모두 좋은데 약간의 담배 냄새가 나요\n",
      "부정확률=70.65%, 긍정확률=29.81%\n",
      "\n",
      "기존문장: 그 교체 2 3만 워이면 될 텐데 수건도 뽀송한 느낌 보다 그냥 집에서 한 번 빨아 쓰는 기분 누군가 사용한 흔적처럼 검은 잉크 같은 게 있었음\n",
      "부정확률=89.62%, 긍정확률=10.73%\n",
      "\n",
      "기존문장: 프론트 직원 분도 서비스 마인드가 부족했어요\n",
      "부정확률=90.73%, 긍정확률=9.79%\n",
      "\n",
      "기존문장: 여친이 서울에 오고 내려 갈 때면 첫 차 버스 타야 해서 센티럴 시티에서 타기에 가까운 호텔인 리버 사이드 호텔이 저렴하게 나오면 무조껀 예약해서 가죠\n",
      "부정확률=71.5%, 긍정확률=30.19%\n",
      "\n",
      "기존문장: 전체적인 시설 이용이 별로였음\n",
      "부정확률=84.77%, 긍정확률=16.0%\n",
      "\n",
      "기존문장: 나는 이 호텔에 숙박할 수 있는 큰 이곳 가족들 점심 식사 절대적으로 멋진 것도 상상할 수 있는 모든 요리 생선 고기 파스타 스시 피자 멋진 그릴 모두 맛있고 신선한 맛있는 샐러드 모든 것이 거의 불가능 해 특이한 소프트 드링크 디저트와 커피도 포함되어 있습니다\n",
      "부정확률=7.66%, 긍정확률=90.09%\n",
      "\n",
      "기존문장: 진짜 앵간 한 삼성급도 이 정돈 아닐텐데 진짜 뭐가 없었어요\n",
      "부정확률=76.2%, 긍정확률=25.4%\n",
      "\n",
      "기존문장: 비용이 아까울 정도입니다\n",
      "부정확률=88.01%, 긍정확률=12.81%\n",
      "\n",
      "기존문장: 다음날 런치도 먹을 겸해서 1박을 했습니다\n",
      "부정확률=28.11%, 긍정확률=70.56%\n",
      "\n",
      "기존문장: 이용한 당일은 객실 바로 아래의 식장에서 결혼식이 있어 조금 시 끄러웠지만 객실 인테리어가 조금 싸구려 느낌이었던 것과 조명이 조금 어두웠던 것이 유감이지만 충분히 쾌적하게 지 낼 수 있습니다\n",
      "부정확률=72.7%, 긍정확률=28.45%\n",
      "\n",
      "기존문장: 카펫 교체해야 피곤 장식 시트가 얼룩이 에어컨을 조절할 수 없는 서랍장 목록 등의 설비 등으로 위치는 괜찮습니다\n",
      "부정확률=19.98%, 긍정확률=76.58%\n",
      "\n",
      "기존문장: 침대 헤드 부분 너머로 전에 머물던 투숙객이 놓고 간 장난감도 안 치워져 있고 먼지도 있었지만 대체로 보이는 곳의 청소 상태는 양호한 편이고 좁 고 답답한 느낌이 있지만 그 가격이면 큰 불만은 없었고 조식은 과 하지도 크게 부족하지도 않았습니다\n",
      "부정확률=60.94%, 긍정확률=39.79%\n",
      "\n",
      "기존문장: 여기서 우리는 파리의 도시 중 가장 흥미로운 아시아 국가로 인상적 이 싶네요\n",
      "부정확률=9.64%, 긍정확률=87.79%\n",
      "\n",
      "기존문장: 노후한 상태를 가만 하더라도 위생상태가 정말 최악이었습니다\n",
      "부정확률=92.54%, 긍정확률=7.58%\n",
      "\n",
      "기존문장: 다만 호텔 입구를 찾기 힘들어 안내 표시가 더 눈에 띄면 좋겠습니다\n",
      "부정확률=50.35%, 긍정확률=50.29%\n",
      "\n",
      "기존문장: 공간도 넓고 침대도 좋았습니다\n",
      "부정확률=3.21%, 긍정확률=94.86%\n",
      "\n",
      "기존문장: 아무것도 모르고 들어갔다가 피부질환이라도 생길까 급하게 나와서 씻고 환불했어요\n",
      "부정확률=85.16%, 긍정확률=15.8%\n",
      "\n",
      "기존문장: 장점 위치적인 요인이 가장 플러스로 작용합니다\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "부정확률=13.99%, 긍정확률=83.36%\n",
      "\n",
      "기존문장: 방은 작고 더러운 객실 전화도 없었어요\n",
      "부정확률=85.67%, 긍정확률=14.59%\n",
      "\n",
      "기존문장: 단점 편하지 않은 직원들의 응대\n",
      "부정확률=91.64%, 긍정확률=8.27%\n",
      "\n",
      "기존문장: 좋은 가격에 잘 자 고 나왔습니다\n",
      "부정확률=7.09%, 긍정확률=90.48%\n",
      "\n",
      "기존문장: 다음 날 수영장 이용이 8시 부터 가능하다고 했으나 10시에 물 높이가 무릎 정도 였음\n",
      "부정확률=90.31%, 긍정확률=10.28%\n",
      "\n",
      "기존문장: 앞으로는 청소하시려면 미리 체크아웃을 하였는지 파악하고 하셨으면 좋겠습니다\n",
      "부정확률=87.79%, 긍정확률=13.03%\n",
      "\n",
      "기존문장: 호텔은 깨끗하고 청결감이 있으며 2명이 이용했는데 슈트 케이스를 펼쳐도 넓게 사용할 수 있었습니다\n",
      "부정확률=3.83%, 긍정확률=94.01%\n",
      "\n",
      "기존문장: 계속 불평을 했지만 전혀 들어 주 지도 않았어요\n",
      "부정확률=66.14%, 긍정확률=35.9%\n",
      "\n",
      "기존문장: 의전을 위해서 선택했던 곳인데 호텔이라고 하기엔 객실이 모텔보다 못한 듯 하고 객실에서 담배 냄새가 많이 나는데 그대로 고객에게 제공하는 것을 보면 서비스 마인드도 그렇게 좋다고 하긴 어렵지 않을까 싶은 뷔페는 참 좋은데 객실을 왜 이렇게 관리하는지\n",
      "부정확률=81.8%, 긍정확률=19.65%\n",
      "\n",
      "기존문장: 엘레베이터가 너무 좁고 노후해서 이용 시 겁이나요\n",
      "부정확률=90.36%, 긍정확률=9.89%\n",
      "\n",
      "기존문장: 위치 좋고 가격 합리적임\n",
      "부정확률=3.56%, 긍정확률=94.48%\n",
      "\n",
      "기존문장: 깨끗하고 좋았습니다\n",
      "부정확률=4.61%, 긍정확률=93.19%\n",
      "\n",
      "기존문장: 위치는 좋은데 다시 가진 않을 것 같아요\n",
      "부정확률=69.05%, 긍정확률=32.11%\n",
      "\n",
      "기존문장: 친구는 소리에 민감하지 않아 잘 잤지만 저처럼 민감한 사람이라면 좀 이용하기엔 불편했습니다\n",
      "부정확률=84.6%, 긍정확률=16.41%\n",
      "\n",
      "기존문장: 하지만 괜찮습니다\n",
      "부정확률=71.87%, 긍정확률=29.67%\n",
      "\n",
      "기존문장: 미니바 없습니다\n",
      "부정확률=90.12%, 긍정확률=10.05%\n",
      "\n",
      "기존문장: 일반 모텔들과 달리 특별히 좋은 건 없었음\n",
      "부정확률=61.74%, 긍정확률=39.65%\n",
      "\n",
      "기존문장: 침대는 매우 편함\n",
      "부정확률=4.8%, 긍정확률=92.93%\n",
      "\n",
      "기존문장: 5 아침 조식이 너무너무 형편없었음\n",
      "부정확률=89.23%, 긍정확률=11.59%\n",
      "\n",
      "기존문장: 욕실에 물 때가 껴 있고 머리카락도 구석에 남아 있는 걸 보고 충격 먹었습니다\n",
      "부정확률=92.62%, 긍정확률=7.39%\n",
      "\n",
      "기존문장: 신사동 가로수길 근처에 있어 매력적이라고 생각했으나 그것은 오산 가로수길까지 가려면 굉장히 큰 대로변을 건너야 하는데 따로 횡단보도가 없기 때문에 호텔 바로 앞에서 길을 건널 수 없고 빙 돌아 지하도를 이용해야 했 다 여간 번거로운 일이 아니였다\n",
      "부정확률=30.32%, 긍정확률=67.9%\n",
      "\n",
      "기존문장: 만약 가야 되면 차는 집에 두고 가세요\n",
      "부정확률=53.38%, 긍정확률=47.35%\n",
      "\n",
      "기존문장: 객실 내 에어컨은 무엇이 문 젠지 온도를 내려도 더워서 미칠 지경이였고 프론트로 전화를 하려 하니 전화기도 고장이 나있네요\n",
      "부정확률=90.84%, 긍정확률=9.88%\n",
      "\n",
      "기존문장: 대체로 괜찮았음\n",
      "부정확률=25.33%, 긍정확률=72.45%\n",
      "\n",
      "기존문장: 한강 잘 보여요\n",
      "부정확률=3.73%, 긍정확률=94.29%\n",
      "\n",
      "기존문장: 이곳에서 2박을 했습니다\n",
      "부정확률=19.99%, 긍정확률=77.47%\n",
      "\n",
      "기존문장: 그리고 제일 문제였던 건 화장실이었어요\n",
      "부정확률=90.47%, 긍정확률=9.84%\n",
      "\n",
      "기존문장: 깔끔하고 괜찮았어요\n",
      "부정확률=5.09%, 긍정확률=92.6%\n",
      "\n",
      "기존문장: 방에서 조명 무용지물 나는 책상에 앉아 있는 지금 이 대부분이 암흑 제가 가본 항목을 찾으려면 가방 밤에 단지 볼 수 없습니다\n",
      "부정확률=91.33%, 긍정확률=8.58%\n",
      "\n",
      "기존문장: 기 타 거래 저희는 주어진 방은 작고 더럽고 냄새 그냥 참을 수 저는 그 방은 더 이상 말할 수 있는 모터 인 북미 스타 등급 또는 신뢰할 수 없는 웹 사이트에 나온 사진 다음으로 우리는 1박당 약 160에 결국 좋은 호텔이지만 그 날 귀찮게 하지 않을 때는 4성급 호텔로 책정되어 2성급 종시설 또한 만약 주말 동안 머물고 음악 클럽에서 듣게 타오르는 밤에 모든 아침 외출을 하면 담배 연기가 가득 찬 엘리베이터 복도의 메인 층 그냥 전체적으로 최악의 경험이었습니다\n",
      "부정확률=90.94%, 긍정확률=9.52%\n",
      "\n",
      "기존문장: 아침에 가볍게 먹을 수 있는 죽이나 스프 종류도 많았고 메인 요리도 제법 많아서 아침 한 끼 식사로는 조금 과하다 싶을 만큼 푸짐했습니다\n",
      "부정확률=8.01%, 긍정확률=89.58%\n",
      "\n",
      "기존문장: 화장실도 유리문이라 씻으면 룸 전체가 습기차서 창문이 뿌옇게 됩니다 ㅠ\n",
      "부정확률=92.22%, 긍정확률=8.27%\n",
      "\n",
      "기존문장: 화장실도 놀랍고 전반적으로 그냥 모텔 수준입니다\n",
      "부정확률=42.74%, 긍정확률=56.97%\n",
      "\n",
      "기존문장: 뭐 가격 대비 하룻밤 정도는 괜찮았으나 다시는 안 갈 것 같음\n",
      "부정확률=61.15%, 긍정확률=40.37%\n",
      "\n",
      "기존문장: 제가 이곳에 머물게 된 이유는 제 동료가 예약을 잡아 주었기 때문이었어요\n",
      "부정확률=44.15%, 긍정확률=55.41%\n",
      "\n",
      "기존문장: 그리고 방문을 닫았는데도 복도에 사람 지나다니는 소리와 이야기 소리까지 그대로 다 들리더군요\n",
      "부정확률=91.3%, 긍정확률=9.33%\n",
      "\n",
      "기존문장: 욕실 조명은 에너지 절약 차원에서 자동으로 꺼졌다\n",
      "부정확률=89.32%, 긍정확률=11.4%\n",
      "\n",
      "기존문장: 수건은 걸레 수준이고 가운에는 피 자국 있고 룸 안의 냄새는 어디 오래된 상가 화장실 냄새가 방안에 어찌나 진동하던지 싼 가격을 가만 하고라도 절대 가지 마세요 ㅠㅠ\n",
      "부정확률=92.25%, 긍정확률=7.96%\n",
      "\n",
      "기존문장: 몇 년 전만 해도 가성비의 끝판 왕인 듯 정말 저렴한 가격에 다양한 부페를 맛 볼 수 있었는데 최근에는 다소 가격이 오른 편이지만 그래도 여전히 만족스런 가격 대비 부페가 보장되는 개인적으로 참 좋아라 하는 곳\n",
      "부정확률=18.2%, 긍정확률=78.88%\n",
      "\n",
      "기존문장: 최소한 화장실에 콘센트 정도는 만들어서 머리를 말리고 준비를 할 때 거울은 볼 수 있도록 해줘야 하는 게 아닌가 하는 아쉬움이 들었습니다\n",
      "부정확률=87.5%, 긍정확률=12.95%\n",
      "\n",
      "기존문장: 동네 아줌마처럼 웃으면서 미안해 라는 둥 4 확인됬으니 가려고 엘리베이터 기다리는데 여직원과 청소부가 안 들리는 줄 알고 피운 거 무조건 맞다면서 웃고 떠들며 뒤에서 좋지 않게 이야기한 것 너무 황당하고 어이없고 억울해서 올려봅니다\n",
      "부정확률=88.43%, 긍정확률=12.49%\n",
      "\n",
      "기존문장: 침대에 누울 때마다 먼지가 계속 나 고 카펫에서 퀴퀴한 냄새가 났습니다\n",
      "부정확률=91.76%, 긍정확률=8.66%\n",
      "\n",
      "기존문장: 주차와 주차비는 아 쉬움\n",
      "부정확률=54.86%, 긍정확률=45.38%\n",
      "\n",
      "기존문장: 잠자리가 아늑 주차난 세탁물 청결 상태 직원 응대 모두 불쾌했음\n",
      "부정확률=89.33%, 긍정확률=10.74%\n",
      "\n",
      "기존문장: 우리 방 1002호 의 문 가장자리는 깨져 있어 누군가 방안에 침입했던 것 처럼 보였습니다\n",
      "부정확률=92.47%, 긍정확률=7.85%\n",
      "\n",
      "기존문장: 전기가 공급되지 않아 방을 두 번이나 바꿨는데 결국 멀티탭을 받았습니다\n",
      "부정확률=89.57%, 긍정확률=11.06%\n",
      "\n",
      "기존문장: 우리는 이 방을 예약하는 여행사를 통해 지난 6월 압지방법에 관한 를 사용하여 모든 리뷰를 보고 나는 방을 바꿔달라고 하는 것들은 볼 수 없어 어쩔 수 없이 묵는 호텔 나는 이미 도착했을 때와 결국은 문제 없이 해결 됐군 두려워 만 사실 였음\n",
      "부정확률=87.9%, 긍정확률=12.79%\n",
      "\n",
      "기존문장: 두 번 다시는 안 가요 복도에서는 삼겹살 냄새나고 방음도 잘 안 되서 옆 방 초인종 소리 다 들리고 아침에는 옆 방서 피는 담배 냄새 올라오고 아늑한 건 기대도 못하고 춥기만 하고 방 온도조절으누어땋게 하는지 천장 에어컨인지 온풍기 인기 청소는 제대로 했을까 먼지날 것 같아 틀지도 못하고 트리플룸 18만원 인데 허참 말도 안 나와요 못 갑니다\n",
      "부정확률=88.14%, 긍정확률=12.31%\n",
      "\n",
      "기존문장: 이 정도면 싸게 잘 지낸 거 같아요\n",
      "부정확률=36.74%, 긍정확률=62.57%\n",
      "\n",
      "기존문장: 지 않을 것입니다\n",
      "부정확률=83.52%, 긍정확률=17.15%\n",
      "\n",
      "기존문장: 복도부터 객실 가는 데까지 음식 냄새 및 케케묵은 냄새가 코를 찌르고 객실의 화장실은 하수구 냄새에 침대 베개는 누렇고 침대 시트에는 머리카락에 여러 개 있었고 방 안으로는 담배 냄새에 계속 유입 되었습니다\n",
      "부정확률=90.98%, 긍정확률=9.39%\n",
      "\n",
      "기존문장: 가성비 좋았어요\n",
      "부정확률=5.32%, 긍정확률=92.41%\n",
      "\n",
      "기존문장: 가격 대비 나쁘지 않음\n",
      "부정확률=12.68%, 긍정확률=84.51%\n",
      "\n",
      "기존문장: 욕실 재정비 해야 할 듯합니다\n",
      "부정확률=89.42%, 긍정확률=10.97%\n",
      "\n",
      "기존문장: 아래에 클럽이 있어서 시끄럽지만 기본적으로 가격 대비 괜찮은 시설인 듯\n",
      "부정확률=27.82%, 긍정확률=70.19%\n",
      "\n",
      "기존문장: 위치 빼고는 장점 없음\n",
      "부정확률=61.69%, 긍정확률=39.57%\n",
      "\n",
      "기존문장: 클럽이 있어서 늦은 시간엔 주위에 사람들이 많았지만 화장실 변기가 벽과의 사이가 좀 좁 좁았지만 어 쩔 수 없어요\n",
      "부정확률=90.18%, 긍정확률=9.97%\n",
      "\n",
      "기존문장: 방에 들어가자마자 침대 시트며 욕실이며 더럽길래 컴플 걸었더니 직원들이 40년 된 호텔이라 그런다며 소비자인 고객에게 이해를 요구하고 그날 담당으로 보이는 30대 남자 직원은 이건 빨아도 안 지워진다면서 안 그러면 다시 사야 된다고 함 근데 호텔이면 다시 사야 죠 안 그렇습니까 대체 무슨 생각으로 일 하는 건지 응 대하는 태도며 마인드가 형편없네요\n",
      "부정확률=89.78%, 긍정확률=10.86%\n",
      "\n",
      "기존문장: 직원 분들이 정말 친절히 잘해 주시고 안내도 정확히 해주셨고 위치는 말할 것도 없이 좋았어요\n",
      "부정확률=3.66%, 긍정확률=94.36%\n",
      "\n",
      "기존문장: 와인 잔 요청했는데 이것 또한 ㅠㅠ 제대로 된 와인 잔도 없고 조식 최악 차갑고 토스터기는 작동이 되는 건지 8시에 갔는데 계란 후라이 1개밖에 없 어서 달라고 했는데 9시까지 안 나옴 리필 안 되고 쥬스나 물도 제대로 구비되어 있지 않아요\n",
      "부정확률=90.19%, 긍정확률=10.42%\n",
      "\n",
      "기존문장: 가격 위치\n",
      "부정확률=4.57%, 긍정확률=93.34%\n",
      "\n",
      "기존문장: 사실 외관에서 풍겨져 나오는 느낌으로는 만족도가 높으리라 기대했으나 생각보다 그렇지 못했다\n",
      "부정확률=87.61%, 긍정확률=13.63%\n",
      "\n",
      "기존문장: 위치는 좋은데 방에서 먼지 냄새 같은 게 좀 나 고 ㅠ 지하주차장에서 객실로 바로 가는 엘리베이터 없어서 매우 불편 ㅠ ㅠ\n",
      "부정확률=81.08%, 긍정확률=19.42%\n",
      "\n",
      "기존문장: 여행자들이 며칠씩 묵어 가기엔 비추함\n",
      "부정확률=87.16%, 긍정확률=13.57%\n",
      "\n",
      "기존문장: 미소를 보여달라가 아니라 기본적 안내나 본인들 잡 영역에서 케어할 수 있는 부분에 대한 서비스가 아쉬웠어요\n",
      "부정확률=91.89%, 긍정확률=8.5%\n",
      "\n",
      "기존문장: 방이 남아 있었으면 처음부터 여기로 줬어야 하는 거 아닌가요 911호는 이용하기 전에 무조건 안내 해주셔야 할 것 같아요\n",
      "부정확률=90.09%, 긍정확률=10.29%\n",
      "\n",
      "기존문장: 화장실도 다른 호텔에 비해 크기도 커서 좋았구요\n",
      "부정확률=3.13%, 긍정확률=94.93%\n",
      "\n",
      "기존문장: 친구나 회사 사람들도 만나기 힘들고 가족들은 매주 보기도 하고 그래서 혼자서 쉴 장소를 찾다가 제가 개인적으로 토요일에 볼 일이 있던 장소가 마침 이곳 근처 여서 예약을 했습니다\n",
      "부정확률=45.48%, 긍정확률=54.25%\n",
      "\n",
      "기존문장: 슈트 케이스를 바닥에 펼치기에는 너무 좁아서 침대 위에서 짐을 정리했습니다\n",
      "부정확률=87.79%, 긍정확률=13.29%\n",
      "\n",
      "기존문장: 호텔에 도착한 시간까지 대기하 는 데 4 10 00 및 체크인 정말 가 고 싶은 우리 방 이다\n",
      "부정확률=30.71%, 긍정확률=67.43%\n",
      "\n",
      "기존문장: 투숙객인데 어면히 주차가 가능하다고 되어 있는데 주차공간 협소하다고 9시 이후에 출차시 10분당 2000원 부과한다고 하니 12시까지 숙박 가능하지만 차량 때문에 쫒겨나는 기분이네요\n",
      "부정확률=86.57%, 긍정확률=14.5%\n",
      "\n",
      "기존문장: 이 최소 필요 깨끗하고 편안한 침대와 샤워실 창문이 있는 방을 예약하는 경우 저는 놀란 시스템 방 중앙에 있다\n",
      "부정확률=15.24%, 긍정확률=81.94%\n",
      "\n",
      "기존문장: 일단 자가 주차가 안 되고 발레 주차만 가능한데 주말에는 이용자가 많아 너무 복잡하고 산만 합니다\n",
      "부정확률=90.7%, 긍정확률=9.68%\n",
      "\n",
      "기존문장: 특히 디저트 류가 맛있었네요 ㅎ\n",
      "부정확률=5.4%, 긍정확률=92.31%\n",
      "\n",
      "기존문장: 최고의 서비스 화려한 장소에 위치한 서울의 바로 중심에 위치 다음 출장이나 관광객들은 인천 공항까지 무료 셔틀 버스를 완벽한 중가\n",
      "부정확률=3.72%, 긍정확률=94.18%\n",
      "\n",
      "기존문장: 카펫 청소는 한 적도 없는지 굉장히 더러웠어요\n",
      "부정확률=90.33%, 긍정확률=10.14%\n",
      "\n",
      "기존문장: 그리고 8시까지 클럽 노래 소리가 식당까지 들렸습니다\n",
      "부정확률=91.74%, 긍정확률=8.42%\n",
      "\n",
      "기존문장: 하지만 날 없는 하나는 수건을 말리 층 사용해야 합니다\n",
      "부정확률=66.72%, 긍정확률=34.38%\n",
      "\n",
      "기존문장: 신사 압구정을 자주 방문하게 되어 이용하였는데 클럽 이용객들 때문에 여러 가지 불편한 점이 많네요\n",
      "부정확률=62.89%, 긍정확률=38.42%\n",
      "\n",
      "기존문장: 아주 좋은 컨디션 이었습니다\n",
      "부정확률=7.11%, 긍정확률=90.3%\n",
      "\n",
      "기존문장: 목이 아플 정도로 그거 빼고는 가격 대비 괜찮은 호텔이예요\n",
      "부정확률=56.66%, 긍정확률=44.52%\n",
      "\n",
      "기존문장: 나는 놀라운 가격에 이 호텔 약 65 할인 제 방에 들어서니 되는 것이다\n",
      "부정확률=27.04%, 긍정확률=71.68%\n",
      "\n",
      "기존문장: 외국인들한테 묻어라고 말하면 나라 창피인 정도예요\n",
      "부정확률=73.64%, 긍정확률=28.78%\n",
      "\n",
      "기존문장: 아침에 일어나면 눈이 부심 5 블라인드 안에 죽은 날 파리들 많음\n",
      "부정확률=77.03%, 긍정확률=23.82%\n",
      "\n",
      "기존문장: 더워서 죽는 줄 알았음\n",
      "부정확률=59.43%, 긍정확률=41.82%\n",
      "\n",
      "기존문장: 나는 화장실에 가 볼 자신을 나쁘지 않음\n",
      "부정확률=44.63%, 긍정확률=54.83%\n",
      "\n",
      "기존문장: 오래된 호텔이기도 했지만 내부 수리가 좀 필요하다 생각이 드네요\n",
      "부정확률=86.57%, 긍정확률=13.95%\n",
      "\n",
      "기존문장: 침대 상태도 깔끔해서 아주 좋구요\n",
      "부정확률=3.31%, 긍정확률=94.75%\n",
      "\n",
      "기존문장: 바닥도 청소도 안 되있고 창문도 없었음\n",
      "부정확률=92.35%, 긍정확률=7.87%\n",
      "\n",
      "기존문장: 침대는 딱딱한 편이고 진정한 사용자 에 관한 다음 날 체크 아웃 하루 종일 내 낭비하고 환불을 받으려고 하고 총 시간 낭비 이는 매우 잘못된 이 4 개의 별 2 의 별\n",
      "부정확률=89.89%, 긍정확률=10.84%\n",
      "\n",
      "기존문장: 긴급한 요구 사항을 방지할 수 있습니다\n",
      "부정확률=14.83%, 긍정확률=82.21%\n",
      "\n",
      "기존문장: 더 센터 호텔 보다 정말 기능입니다\n",
      "부정확률=24.11%, 긍정확률=74.17%\n",
      "\n",
      "기존문장: 슬레이트 지붕에 살짝 막혀 있는 10층이었지만 그 너머로 시선을 확장시키면 강도 보이고 산도 보이는 객실에 배정 받았습니다\n",
      "부정확률=86.04%, 긍정확률=15.12%\n",
      "\n",
      "기존문장: 너무 좁았어요\n",
      "부정확률=91.87%, 긍정확률=8.11%\n",
      "\n",
      "기존문장: 생각보다 별로였습니다\n",
      "부정확률=87.49%, 긍정확률=12.74%\n",
      "\n",
      "기존문장: 너무 오래된 인테리어 부서진 옷장문\n",
      "부정확률=92.28%, 긍정확률=7.86%\n",
      "\n",
      "기존문장: 위치는 좋은 편이 긴 했지만 생각보다는 별로였어요\n",
      "부정확률=61.14%, 긍정확률=39.45%\n",
      "\n",
      "기존문장: 싼 게 비지 떡이라고 다신안갑니다\n",
      "부정확률=72.66%, 긍정확률=28.74%\n",
      "\n",
      "기존문장: 음식 위치 없다\n",
      "부정확률=85.3%, 긍정확률=15.01%\n",
      "\n",
      "기존문장: 이 호텔은 이 시대의 명성은 전원 생활 을 보여 주는 방은 낡고 담배로부터 좋은 점은 아침 만 매우 다양한 스시 및 해당 합니다\n",
      "부정확률=18.36%, 긍정확률=78.74%\n",
      "\n",
      "기존문장: 겨울에 추운데 히터가 엄청난 소음으로 무슨 공장 소리 같아서 잠이 안와 끄면 너무 춥고 키면 시끄럽고 공기가 안 좋아 벼서 죽을 것 같았음\n",
      "부정확률=91.75%, 긍정확률=8.8%\n",
      "\n",
      "기존문장: 그리고 조식 먹고 있으면 런치 뷔페 준비하 느라 분주한 모습을 보면서 밥 먹어야 되서 밥을 먹는 건지 눈치 밥을 먹는 건지 애매합니다\n",
      "부정확률=90.14%, 긍정확률=10.54%\n",
      "\n",
      "기존문장: 요즘 우 한으로 외국인 관광객들 줄어서 그런지 별거에 요구하시는 건가 이용 시 건물 자체에서 쾌쾌한 냄새가 나지만 많이 나면 방을 바꾸세요\n",
      "부정확률=91.04%, 긍정확률=9.38%\n",
      "\n",
      "기존문장: 강남 지역을 중심으로 쇼핑이나 관광할 때 또 이용하고 싶은 호텔입니다\n",
      "부정확률=6.86%, 긍정확률=90.65%\n",
      "\n",
      "기존문장: 다신 이용하고 싶지 않네요\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "부정확률=89.88%, 긍정확률=10.28%\n",
      "\n",
      "기존문장: 이건 아니다 싶어서 한층 위로 옮겼네요\n",
      "부정확률=68.85%, 긍정확률=32.65%\n",
      "\n",
      "기존문장: 침대는 좋았어요\n",
      "부정확률=9.2%, 긍정확률=88.01%\n",
      "\n",
      "기존문장: 방 크기는 넓고 창문도 커서 답답한 느낌이 전혀 없었습니다\n",
      "부정확률=4.94%, 긍정확률=92.84%\n",
      "\n",
      "기존문장: 시티뷰의 개념이 달라졌네요 ㅎ\n",
      "부정확률=56.49%, 긍정확률=44.45%\n",
      "\n",
      "기존문장: 룸 들어서자마자 곰팡이 냄새 났어요\n",
      "부정확률=91.2%, 긍정확률=9.18%\n",
      "\n",
      "기존문장: 더 리버 사이드 호텔은 서양 기준을 아래에서 지금껏 가본 호텔 중 최악 이 한국 세계 객실과 로비 레스토랑에서 담배 냄새 에나 3 객실까지 우리는 방 냄새가 약간 견딜 수 있는 그래도 매일 두통이 서버에 3박을 하는 냄새 화장실에는 새는 방은 증거 화장실 화장지 던지 마십시오 카펫에 얼룩 방과 복도 수영장은 닫혀 머무는 내내 다른 대안을 제공 저는 절대 이 호텔을 추천합니다\n",
      "부정확률=88.63%, 긍정확률=11.69%\n",
      "\n",
      "기존문장: 하지만 시설이나 규모나 전반적으로 낙후된 호텔이 고 방음도 그닥 조식이 맛있다는 후기가 많았지만 솔직히 준비되어 있는 종류도 몇 없었고 먹을 만한 게 없었음\n",
      "부정확률=59.16%, 긍정확률=42.3%\n",
      "\n",
      "기존문장: 나름 호텔인데 느낌이 좋지 않네요\n",
      "부정확률=83.64%, 긍정확률=16.92%\n",
      "\n",
      "기존문장: 한남대교 초입에 위치해 있어 강남이나 이태원 명동 다니기는 아주 좋은 위치\n",
      "부정확률=3.58%, 긍정확률=94.43%\n",
      "\n",
      "기존문장: 4성이란 걸 믿을 수 없을 만큼 낙후된 시설 낡은 화장실 낡은 객실 정말 먹을 게 없었던 조식 ㅜㅜ 위치만 좋았던 데다가 직원들도 뭘여쭤볼 때마다 귀찮아 하시던 택시조차 안 불러주시던\n",
      "부정확률=54.98%, 긍정확률=45.93%\n",
      "\n",
      "기존문장: 오래된 호텔이라 그렇다고 할 수는 없는 침대 시트 및 욕실 청소 상태에 실망했어요\n",
      "부정확률=90.92%, 긍정확률=9.25%\n",
      "\n",
      "기존문장: 저는 예약 없이 일요일 디너 방문했는데 가족 단위 손님이 많아 대기가 길었습니다\n",
      "부정확률=88.64%, 긍정확률=12.17%\n",
      "\n",
      "기존문장: 9 배달 음식 제지하고 배달라이더 호텔 안으로 못 들어오게 함 10 체크아웃 시 이런 불만 말씀드리고 불만 내용의 피드백 요청을 지배원님에게 전화번호까지 드렸는데 아직까지 연락이 없음\n",
      "부정확률=86.15%, 긍정확률=15.11%\n",
      "\n",
      "기존문장: 숙소 상태요 택시가 없음\n",
      "부정확률=70.75%, 긍정확률=30.67%\n",
      "\n",
      "기존문장: 조식도 다양하고 맛있었습니다\n",
      "부정확률=3.49%, 긍정확률=94.54%\n",
      "\n",
      "기존문장: 룸에 편 백향의 룸 스프레이를 많이 뿌려 두시면 냄새를 많이 잡을 수 있을 거에요\n",
      "부정확률=87.17%, 긍정확률=13.61%\n",
      "\n",
      "기존문장: 제가 이곳에 온 목적은 이상적인 수술 외관 걸어서 갈 수 있는 수술이 라 회사 호텔을 예약했는데 이 정직한 하지만 이곳에 도착했을 때는 하지 않는 직원이 방을 예약 했습니다\n",
      "부정확률=48.46%, 긍정확률=51.2%\n",
      "\n",
      "기존문장: 연회장 앞에 방이 있는 게 말이 되나요\n",
      "부정확률=83.49%, 긍정확률=17.88%\n",
      "\n",
      "기존문장: 냉장고에 무료 생수 두병이 다입니다\n",
      "부정확률=6.14%, 긍정확률=91.41%\n",
      "\n",
      "기존문장: 직원 분들은 친절하고 좋았는데 ㅠㅠ\n",
      "부정확률=75.22%, 긍정확률=26.28%\n",
      "\n",
      "기존문장: 뒤에서 예민한 고객이라며 얘기까지 하는데 장난하십니까 내 돈 주고 호텔까지 왔는데 그런 대접을 받아들이는 게 이 상한 거 아닌가요\n",
      "부정확률=91.7%, 긍정확률=8.75%\n",
      "\n",
      "기존문장: 공항버스 역에서 가까워서 무척 편리 스탭들도 친절함\n",
      "부정확률=3.35%, 긍정확률=94.71%\n",
      "\n",
      "기존문장: 눕자 마자 잠이 스르르 오는 완벽한 배게와 이불 하우스키핑 하시는 분들도 너무 친절해서 기분 좋았어요\n",
      "부정확률=3.61%, 긍정확률=94.35%\n",
      "\n",
      "기존문장: 하루 먹고 다음 날 부터는 안 먹고 그냥 나왔습니다\n",
      "부정확률=68.28%, 긍정확률=32.93%\n",
      "\n",
      "기존문장: 구석구석 먼지가 있고 쾌쾌한 느낌이 좀 있었어요\n",
      "부정확률=89.71%, 긍정확률=10.48%\n",
      "\n",
      "기존문장: 야경도 좋음\n",
      "부정확률=3.27%, 긍정확률=94.84%\n",
      "\n",
      "기존문장: 시설 보충 및 직원 태도부터 교육 다시 시키는 게 좋을 듯합니다\n",
      "부정확률=90.31%, 긍정확률=10.17%\n",
      "\n",
      "기존문장: 3인실이면 적어도 가족 아님 친구랑 올 텐데 가족 친구끼리도 지키고 싶은 게 있잖아요 ㅋㅋ 소리도 소린 데 시각적으로 너무 강력합니다ㅋㅋㅋ 불키고는 도저히 갈 수가 없어서 민망 민망 불 끄고 볼일 봤어요 ㅋㅋㅋ\n",
      "부정확률=88.25%, 긍정확률=12.35%\n",
      "\n",
      "기존문장: 청소해주시는 분들이 친절했고 직원 분들보다 실습생 분들이 오히려 더 친절했습니다\n",
      "부정확률=20.87%, 긍정확률=77.14%\n",
      "\n",
      "기존문장: 그리고 예약 사진에는 창문은 없다고 되어 있지만 밖은 볼 수 있는 듯하게 올라와 있네요\n",
      "부정확률=81.53%, 긍정확률=19.87%\n",
      "\n",
      "기존문장: 빠른 개선이 필요함\n",
      "부정확률=87.12%, 긍정확률=13.2%\n",
      "\n",
      "기존문장: 호텔 내에 베이커리 커피숍 남성 전용 사우나와 휘트니스 수영장이 갖춰져 있고 레스토랑도 있지만 호텔에서 가장 중요한 객실은 내가 호텔에 온 건지 저렴한 모텔에 온 건지 싶은 생각이 들 정도였네요\n",
      "부정확률=26.21%, 긍정확률=71.88%\n",
      "\n",
      "기존문장: 위치 빼고는 장점 없어요\n",
      "부정확률=58.63%, 긍정확률=42.61%\n",
      "\n",
      "기존문장: 방에 여유가 있을 때는 무료로 한 강이 보이는 방으로 업그레이드 바로 맞은 편에 여성 전용 스파도 있어서 일석이조 다만 한 가지 조명이 조금 어두운 게 잘 지 내다 왔습니다\n",
      "부정확률=25.46%, 긍정확률=71.96%\n",
      "\n",
      "기존문장: 창문이 없는 거랑 똑같았어요\n",
      "부정확률=74.1%, 긍정확률=27.41%\n",
      "\n",
      "기존문장: 오히려 가로수길까지 가는 게 귀찮아졌다\n",
      "부정확률=82.86%, 긍정확률=18.34%\n",
      "\n",
      "기존문장: 샤워하기 힘들어요\n",
      "부정확률=89.69%, 긍정확률=10.76%\n",
      "\n",
      "기존문장: 직원 들 태도와 서비스가 불쾌할 정도로 불만족하며 호텔인지 결혼식장인지 구분이 안 가며 주차대행도 마찬가지였다\n",
      "부정확률=88.73%, 긍정확률=12.09%\n",
      "\n",
      "기존문장: 갑자기 눈보라가 내려 체크인과 동시에 차 한 잔 하려고 커피 포트를 찾은 순간 뚜껑 위에 뽀얀 먼지 거기에 하얀 찻잔에 찍힌 주홍빛 립스틱 자국 침대 헤드 레스트위의 먼지와 얼룩들 티지 브라운관과 프레임에 하얀 먼지들 히터 온도 조절기를 최대로 올려도 22도 이상 올려지지 않는 온도 조절기 요즘은 호텔들의 할인 행사가 많은데 이곳은 아무 리 싸게 줘도 다시는 가 곳 싶지 않습니다\n",
      "부정확률=91.16%, 긍정확률=9.33%\n",
      "\n",
      "기존문장: 전반적으로 위치나 상태는 좋았습니다\n",
      "부정확률=5.71%, 긍정확률=91.84%\n",
      "\n",
      "기존문장: 또한 호텔 내 뷔페 식당은 워낙 유명한 곳이라 숙박하는 동안에 이용해 보는 것도 좋습니다\n",
      "부정확률=4.71%, 긍정확률=93.09%\n",
      "\n",
      "기존문장: 그냥 신사 역에서 걸어서 5 10 편리하고 편안한 저는 디럭스 룸을 예약했는데 제 생각 13 레벨이 아주 큰 방 나는 이 거대한 침대가 있었습니다\n",
      "부정확률=12.0%, 긍정확률=85.42%\n",
      "\n",
      "기존문장: 돈을 주고 투숙을 하는 건지 돈을 받고 극기 체험을 하고 있는 거지 싶을 정도였습니다\n",
      "부정확률=72.2%, 긍정확률=28.84%\n",
      "\n",
      "기존문장: 더 리버 사이드 모텔 저렴한 모텔급 숙소 입니다\n",
      "부정확률=11.67%, 긍정확률=85.75%\n",
      "\n",
      "기존문장: 가격 위치 서비스\n",
      "부정확률=5.24%, 긍정확률=92.47%\n",
      "\n",
      "기존문장: 싸게 나와서 기 대안 했는데 조식 맛있었음\n",
      "부정확률=5.13%, 긍정확률=92.8%\n",
      "\n",
      "기존문장: 금연방인데 샤워기 쓸 때 타층 화장실에서 담배 폈는지 담배 냄새 좀 나는 것 빼곤 괜찮았어요\n",
      "부정확률=81.36%, 긍정확률=19.1%\n",
      "\n",
      "기존문장: 32 모든 저는 이 호텔을 다시 이용할 것이 다 이 호텔은 정말 사실 이 기 때문입니다 한다\n",
      "부정확률=63.77%, 긍정확률=37.33%\n",
      "\n",
      "기존문장: 그리고 생각보다 방이 좁 았고 신발을 객실 내에 벗어두는 공간이 따로 없어 불편했습니다\n",
      "부정확률=93.01%, 긍정확률=7.11%\n",
      "\n",
      "기존문장: 리버 사이드 호텔은 정말 좋지 걸어갈 때 끔찍한 냄새 모든 것이 더러운 속보 다만 같은 사람 지불한 만큼 원하지 않는 것이 더 이상 호텔 수영장 내야\n",
      "부정확률=81.46%, 긍정확률=19.39%\n",
      "\n",
      "기존문장: 없는 화장실이 유리막으로 되어 있음\n",
      "부정확률=92.02%, 긍정확률=8.15%\n",
      "\n",
      "기존문장: 그들은 상상을 먹고 싶은 달콤한 아메리칸 스타일의 곡물 없음\n",
      "부정확률=88.61%, 긍정확률=12.14%\n",
      "\n",
      "기존문장: 화장실 배수 역류 수건 쉰 냄새 등 등 최악입니다\n",
      "부정확률=91.7%, 긍정확률=8.23%\n",
      "\n",
      "기존문장: 하나님은 무엇을 알고 계십니다\n",
      "부정확률=70.84%, 긍정확률=30.56%\n",
      "\n",
      "기존문장: 가로수길이랑 가까워서 구경하기에 좋았어요\n",
      "부정확률=3.78%, 긍정확률=94.23%\n",
      "\n",
      "기존문장: 휘트니스 이용하는데도 좋아요\n",
      "부정확률=4.27%, 긍정확률=93.55%\n",
      "\n",
      "기존문장: 저렴한 가격에 스테이크 괜찮음\n",
      "부정확률=3.54%, 긍정확률=94.5%\n",
      "\n",
      "기존문장: 조식의 메뉴가 다른 호텔에 비해 많았음\n",
      "부정확률=30.44%, 긍정확률=68.45%\n",
      "\n",
      "기존문장: 한강 뷰 좋았어요\n",
      "부정확률=3.49%, 긍정확률=94.57%\n",
      "\n",
      "기존문장: 1 따뜻한 물이 안 나왔다 세면대 2 욕조에 죽은 날 파리들 3 조명 안에 날 파리 4 암막 커텐이 아닌 블라인드 설치되어 있음\n",
      "부정확률=90.26%, 긍정확률=10.26%\n",
      "\n",
      "기존문장: 타월은 더러워서 갈색과 노란색 얼룩이 묻어 있었습니다\n",
      "부정확률=92.31%, 긍정확률=8.01%\n",
      "\n",
      "기존문장: 4일 밤을 보내기 때문에 이 호텔을 선택할 수 있을 것 강남에 있는 10분 정도 걸어가면 쇼핑 지역 이 호텔은 아주 기본 적이다\n",
      "부정확률=38.18%, 긍정확률=61.43%\n",
      "\n",
      "기존문장: 노후된 곳이 몇 군데 보여서 조금 아쉽네요\n",
      "부정확률=83.21%, 긍정확률=17.39%\n",
      "\n",
      "기존문장: 흰 천으로 덮개를 한다고 해도 불편함은 덮을 수 없어요\n",
      "부정확률=87.93%, 긍정확률=12.98%\n",
      "\n",
      "기존문장: 둘이 가면 아늑할 것 같아 요 위치도 신사역까지 걸어서 갈 수 있어서 좋습니다\n",
      "부정확률=3.73%, 긍정확률=94.32%\n",
      "\n",
      "기존문장: 이런 저런 일도 겹치다 보니 어디 혼자 있고 싶더라구요\n",
      "부정확률=34.88%, 긍정확률=64.7%\n",
      "\n",
      "기존문장: 무엇보다도 넌 여기에 좋은 수면 예 의 외로 동양인 기준 침대는 편안했습니다\n",
      "부정확률=12.9%, 긍정확률=84.04%\n",
      "\n",
      "기존문장: 가로수길 인근 침대에 머리카락 있었고 환기가 안 되어 있어서 먼지가 너무 많았음\n",
      "부정확률=92.56%, 긍정확률=7.47%\n",
      "\n",
      "기존문장: 맛있게 잘 먹고 갑니다\n",
      "부정확률=5.69%, 긍정확률=91.9%\n",
      "\n",
      "기존문장: 했기 때문에 오랜 시간 샤워를 하거나 할 수 없었고 욕실 조명 스위치는 문에서 멀리 떨어져 있어 다시 불을 켜는 데 어려움이 있었습니다\n",
      "부정확률=92.74%, 긍정확률=7.47%\n",
      "\n",
      "기존문장: 위치 빼고는 전부 엉망입니다\n",
      "부정확률=47.66%, 긍정확률=53.43%\n",
      "\n",
      "기존문장: 인터넷도 이용할 수 없었고요 숙박료에 포함된 아침 식사로는 오믈렛이나 스크램블 에 크 중 선택하도록 되어 있었어요\n",
      "부정확률=65.7%, 긍정확률=35.43%\n",
      "\n",
      "기존문장: 침대가 매우 편했습니다\n",
      "부정확률=3.38%, 긍정확률=94.66%\n",
      "\n",
      "기존문장: 위치 가격 없음\n",
      "부정확률=30.28%, 긍정확률=68.79%\n",
      "\n",
      "기존문장: 그냥 주변에 다른 좋은 곳이 많기 때문에 딴 곳을 알아보시는 게\n",
      "부정확률=77.01%, 긍정확률=24.45%\n",
      "\n",
      "기존문장: 하지만 저녁 식사는 훌륭했고 직원들은 매우 도움이 되었습니다\n",
      "부정확률=69.93%, 긍정확률=31.63%\n",
      "\n",
      "기존문장: 위치와 전망은 좋았습니다\n",
      "부정확률=5.32%, 긍정확률=92.29%\n",
      "\n",
      "기존문장: 넘 시끄럽고 호텔 밑에 클럽이 있어 너무 자기 힘들었다\n",
      "부정확률=90.42%, 긍정확률=9.91%\n",
      "\n",
      "기존문장: 아주 낡은 호텔 7만원 선에서 묵었던 호텔 중 가장 최악이었음\n",
      "부정확률=91.07%, 긍정확률=9.21%\n",
      "\n",
      "기존문장: 가성비 안락한 침대 호텔 위치\n",
      "부정확률=3.29%, 긍정확률=94.72%\n",
      "\n",
      "기존문장: 방을 두개나 예약했는데 객실 상태 확인하고 왜 환불 요청이나 룸 업그레이드를 요청하지 않았는지 계속 후회 중입니다\n",
      "부정확률=87.89%, 긍정확률=13.24%\n",
      "\n",
      "기존문장: 조식의 음식 종류가 몇 가지 없었으며 별로 먹을 게 없었습니다\n",
      "부정확률=90.15%, 긍정확률=10.23%\n",
      "\n",
      "기존문장: 먼지도 있고 끈적거리는 곳도 많았습니다\n",
      "부정확률=91.46%, 긍정확률=8.69%\n",
      "\n",
      "기존문장: 건물 자체가 환기가 안 되는지 모르겠으나 수건이 습기를 머금고 있네요\n",
      "부정확률=90.34%, 긍정확률=10.11%\n",
      "\n",
      "기존문장: 이 가격에 이 런 호텔을 국낸에서 이용하다니 이 거 실화인가요 저는 정말 만족하게 잘 머물다 왔습니다\n",
      "부정확률=22.37%, 긍정확률=75.56%\n",
      "\n",
      "기존문장: 생각보다 숙소가 매우 좁고 복도에 사람이 지나갈 때 마다 문 앞 자동 센서 불이 켜져서 매우 불편했습니다\n",
      "부정확률=92.79%, 긍정확률=7.2%\n",
      "\n",
      "기존문장: 눈이 다 침침해지는 기 분ㅠㅠ 이 부분은 개선하시는 게 좋을 듯해요\n",
      "부정확률=86.45%, 긍정확률=14.31%\n",
      "\n",
      "기존문장: 아침식사가 너무 안 좋았어요\n",
      "부정확률=63.23%, 긍정확률=38.2%\n",
      "\n",
      "기존문장: 아무것도 이용한 게 없음\n",
      "부정확률=73.35%, 긍정확률=28.24%\n",
      "\n",
      "기존문장: 지하철역에서 가까움\n",
      "부정확률=4.07%, 긍정확률=93.84%\n",
      "\n",
      "기존문장: 7080 호텔인 듯\n",
      "부정확률=33.1%, 긍정확률=66.3%\n",
      "\n",
      "기존문장: 침구가 너무 편안했습니다\n",
      "부정확률=3.55%, 긍정확률=94.44%\n",
      "\n",
      "기존문장: 직원 분들이나 청결 상태가 너무너무 좋아서 추천합니 당 객실은 조용하고 좋았습니다용\n",
      "부정확률=7.36%, 긍정확률=90.09%\n",
      "\n",
      "기존문장: 다만 중앙 등이 없어서 낮인데도 방 자체가 굉장히 어두웠어요\n",
      "부정확률=51.55%, 긍정확률=48.91%\n",
      "\n",
      "기존문장: 웨딩홀 교통 친절\n",
      "부정확률=3.73%, 긍정확률=94.23%\n",
      "\n",
      "기존문장: 카페트도 말할 것도 없이 더럽고요 깨끗하게 유지하고 싶다는 의 지 자체가 없는 인상입니다\n",
      "부정확률=91.02%, 긍정확률=9.31%\n",
      "\n",
      "기존문장: 예약해도 줄을 서지요 ㅎㅎ\n",
      "부정확률=71.02%, 긍정확률=30.95%\n",
      "\n",
      "기존문장: 아침 식사는 1층에 있는 클럽에서 무거운 비트와 함께 춤을 의 그것이 이 호텔의 유일한 구속 기능 토요일까지 목요일에 클럽 에는 9 열려 같은\n",
      "부정확률=43.58%, 긍정확률=55.58%\n",
      "\n",
      "기존문장: 다 만 변기 위치가 너무 좁아서 고생했습니다ㅠ 그것 빼곤 좋았습니다\n",
      "부정확률=68.16%, 긍정확률=33.15%\n",
      "\n",
      "기존문장: 지하에 있는 나이트 클럽이 있는 아침에 조금 불안한 사람들이 머물 고 있는 임의 카운티까지 술에 취한 사람들 프론트 데스크 직원들은 평범한\n",
      "부정확률=90.64%, 긍정확률=10.01%\n",
      "\n",
      "기존문장: 좋습니다 만족스러웠습니다\n",
      "부정확률=16.14%, 긍정확률=81.16%\n",
      "\n",
      "기존문장: 지하 주차장은 진짜 한 20년은 청소 안 한 것 같고 엘레베이터는 시트지 범벅에 버튼도 잘 안 눌리고 객실 문도 시트지 범벅에 고장이 난 건지 출입시마다 자꾸 알람이 계속 울려서 당황스러움 내가 출장 다녀서 오지 아프리카도 몇 번 까봤지만 음 여기는 진짜 필리핀 시골 오지의 일박 15000원 짜리 수준임 위치와 이름에 낚였네\n",
      "부정확률=84.53%, 긍정확률=16.74%\n",
      "\n",
      "기존문장: 저는 2박을 하지 않을 수도 서 나는 첫 번째 밤 저는 다시 가지\n",
      "부정확률=72.31%, 긍정확률=28.84%\n",
      "\n",
      "기존문장: 방 넘작고 청결도도 그저 그랬어요\n",
      "부정확률=22.93%, 긍정확률=75.5%\n",
      "\n",
      "기존문장: 방은 약간 작은 편이지만 룸 컨디션은 괜찮았습니다\n",
      "부정확률=13.59%, 긍정확률=83.8%\n",
      "\n",
      "기존문장: 문 위에는 연기 감지 장치가 붙어 있었는데 온전히 설치되지 않아 온갖 부품들 이 다 보였습니다\n",
      "부정확률=89.68%, 긍정확률=11.01%\n",
      "\n",
      "기존문장: 신사 쪽에 볼 일이 있어서 호텔을 이용하게 되었는데요\n",
      "부정확률=34.56%, 긍정확률=64.5%\n",
      "\n",
      "기존문장: 아무튼 맘에 드는 호텔이에요\n",
      "부정확률=10.79%, 긍정확률=86.32%\n",
      "\n",
      "기존문장: 그리고 조식 돈 내고 먹엇는데 역대급 아무것도 그곳엔 없었다\n",
      "부정확률=52.71%, 긍정확률=48.98%\n",
      "\n",
      "기존문장: 2명이 트윈 룸에 숙박했습니다\n",
      "부정확률=48.08%, 긍정확률=51.67%\n",
      "\n",
      "기존문장: 사진과 많이 다름 벽에 얼룩 시트에 얼룩 심해서 방을 바꿈\n",
      "부정확률=91.79%, 긍정확률=8.43%\n",
      "\n",
      "기존문장: 침대 편하고 직원들 친절하고 위치 좋아요\n",
      "부정확률=3.26%, 긍정확률=94.8%\n",
      "\n",
      "기존문장: 모임용 룸이 많아 동문회 가족 모임 등을 자주 하는 것을 볼 수 있었습니다\n",
      "부정확률=7.31%, 긍정확률=89.95%\n",
      "\n",
      "기존문장: 냉장고는 반찬 냄새남\n",
      "부정확률=89.22%, 긍정확률=11.26%\n",
      "\n",
      "기존문장: 위치가 역에서 가까워서 좋았어 여 창문이 옛날 것이여서 그런지 찬 기운도 많이 들어오고 특히 밤에는 차 지나가는 소리 때문에 시 끄러워서 잠이 들기 어려웠습니다\n",
      "부정확률=13.6%, 긍정확률=83.57%\n",
      "\n",
      "기존문장: 규모는 큰 편이고 신사역에서 도보로 10 정도에 위치해 있습니다\n",
      "부정확률=4.53%, 긍정확률=93.32%\n",
      "\n",
      "기존문장: 직원 분들은 친절하 셧습니다\n",
      "부정확률=25.37%, 긍정확률=72.32%\n",
      "\n",
      "기존문장: 저녁은 6시부터 시작인데 뷔폐에서 가까운 좌석을 원하시면 꼭 사 전에 예약하고 가 세여 음식은 정말 역시 최고입니다\n",
      "부정확률=8.59%, 긍정확률=88.33%\n",
      "\n",
      "기존문장: 조식 메뉴가 많이 식어 있었습니다\n",
      "부정확률=91.83%, 긍정확률=8.41%\n",
      "\n",
      "기존문장: 6 뒤에 배경인 거울에 손자국 하나도 안 지워짐 7 화장대에는 먼지가 가득 8 냉장고는 기능을 잘못해서 뭘 넣기만 하면 다 얼음 냉장고가 아니라 냉동 실인 거 같음\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "부정확률=90.9%, 긍정확률=9.79%\n",
      "\n",
      "기존문장: 에어컨이 고장 났어요\n",
      "부정확률=92.56%, 긍정확률=7.64%\n",
      "\n",
      "기존문장: 방안에 있는 창문은 어찌나 허술한지 방 한 개는 아예 사용조차 안하고 한 방에서 네 명이 같이 밤을 샜습니다\n",
      "부정확률=85.66%, 긍정확률=15.09%\n",
      "\n",
      "기존문장: 방은 나쁘지 않았다\n",
      "부정확률=15.55%, 긍정확률=81.63%\n",
      "\n",
      "기존문장: 침구가 편하고 좋았습니다\n",
      "부정확률=3.6%, 긍정확률=94.4%\n",
      "\n",
      "기존문장: 실내 불은 엄청 어두워서 낮에도 책도 못 볼 정도 새벽에 방이 엄청 울려서 일어나 보니 누가 방 잘못 찾아서 벨을 눌러서 깨버림 너무 놀람 호 텔에 벨이 왜 달려 있음\n",
      "부정확률=91.53%, 긍정확률=8.85%\n",
      "\n",
      "기존문장: 횡단보도가 멀리 있어서\n",
      "부정확률=32.4%, 긍정확률=66.25%\n",
      "\n",
      "기존문장: 커피가 잇엇으면 좋잇을 텐데 녹차만 잇어서 아쉽\n",
      "부정확률=80.47%, 긍정확률=20.67%\n",
      "\n",
      "기존문장: 객실에서 냄새나 고 가격 대비 최악이네요 ㅠㅠ\n",
      "부정확률=84.29%, 긍정확률=17.06%\n",
      "\n",
      "기존문장: 친구가 남친이랑 권태기일 때 오라며 우스 갯소리도 했네요 ㅋㅋ\n",
      "부정확률=81.62%, 긍정확률=19.59%\n",
      "\n",
      "기존문장: 부킹 측에서 피드백이 빨라서 기분이 덜 상해서 좋았어요\n",
      "부정확률=9.85%, 긍정확률=87.57%\n",
      "\n",
      "기존문장: 뷔페가 유명한 호텔답게 조식도 알차고 맛있었네요\n",
      "부정확률=3.26%, 긍정확률=94.79%\n",
      "\n",
      "기존문장: 욕실 타일이 모두 갈라져 있었고 청결 상태도 만족스럽지 못했다\n",
      "부정확률=92.1%, 긍정확률=8.2%\n",
      "\n",
      "기존문장: 일부러 후기를 적지 않는데 다음 예약하시는 분들께 꼭 알려드려야 할 것 같아 남겨요\n",
      "부정확률=43.9%, 긍정확률=56.76%\n",
      "\n",
      "기존문장: 방에 먼지 많음\n",
      "부정확률=92.62%, 긍정확률=7.41%\n",
      "\n",
      "기존문장: 조용하고 한강이 보여서 경치가 좋았습니다\n",
      "부정확률=3.26%, 긍정확률=94.8%\n",
      "\n",
      "기존문장: 더블 침대로 배정 받은 객실은 싱글 침대 두개를 붙여 논 침대였습니다\n",
      "부정확률=76.76%, 긍정확률=25.62%\n",
      "\n",
      "기존문장: 약 10분 후에 그는 하는 것이 우리에게 바가지를 씌우 다 하는 포켓 및 수행해야 하는 것 그는 알고 있지만 그는 아무것도 할 것이다\n",
      "부정확률=90.6%, 긍정확률=9.78%\n",
      "\n",
      "기존문장: 전철역 바로 옆이라 이동 좋습니다\n",
      "부정확률=3.87%, 긍정확률=94.08%\n",
      "\n",
      "기존문장: 위치 친절\n",
      "부정확률=4.08%, 긍정확률=93.86%\n",
      "\n",
      "기존문장: 온도는 적당했어요\n",
      "부정확률=8.92%, 긍정확률=88.37%\n",
      "\n",
      "기존문장: 호텔 숙박과 함께 포함되는 수영장 서비스를 이용하려고 하려고 수영장 위치를 물어보았으나 수영장이 어느 층에 위치하고 있는지 제 때 대답하지 못해 여러 번 질문하게 되었다\n",
      "부정확률=88.39%, 긍정확률=12.46%\n",
      "\n",
      "기존문장: 별로 추천하지 않아요\n",
      "부정확률=64.55%, 긍정확률=37.56%\n",
      "\n",
      "기존문장: 시설은 오래 됬지만 위치와 편안함은 좋습니다\n",
      "부정확률=13.82%, 긍정확률=83.19%\n",
      "\n",
      "기존문장: 욕실 환풍기가 없어요\n",
      "부정확률=84.03%, 긍정확률=16.57%\n",
      "\n",
      "기존문장: 809 에 유지되지 않습니다\n",
      "부정확률=89.77%, 긍정확률=10.48%\n",
      "\n",
      "기존문장: 가격 대비 괜찮음\n",
      "부정확률=7.23%, 긍정확률=90.25%\n",
      "\n",
      "기존문장: 위치는 좋은 편이었지만 아침 식사가 부실한 느낌이었습니다\n",
      "부정확률=49.09%, 긍정확률=51.37%\n",
      "\n",
      "기존문장: 지하철도 가깝고 가로수길도 가깝고 맛집들도 즐비한 곳에 있어 주변 환경이 좋은 호텔입니다 가격도 저렴한 편입니다\n",
      "부정확률=3.31%, 긍정확률=94.81%\n",
      "\n",
      "기존문장: 위치 좋고 직원 친절하고 침대 편했습니다\n",
      "부정확률=3.21%, 긍정확률=94.88%\n",
      "\n",
      "기존문장: 그리고 주말 밤에는 지하층 유흥장으로 인해 소음과 신변에 불안을 느낄 정도로 투숙객에 대한 배려가 전혀 없다\n",
      "부정확률=92.25%, 긍정확률=8.16%\n",
      "\n",
      "기존문장: 위생 안 좋음\n",
      "부정확률=85.24%, 긍정확률=14.95%\n",
      "\n",
      "기존문장: 복도에서 대화소리가 들리고 제가 묵엇던 1208호 현관 잠금 상태는 여자가 혼자 자기에 상당 히 상태가 좋지 않앗습니다\n",
      "부정확률=92.38%, 긍정확률=7.95%\n",
      "\n",
      "기존문장: 가격 대비 여러 가지로 만족스러웠어요\n",
      "부정확률=7.44%, 긍정확률=89.96%\n",
      "\n",
      "기존문장: 욕실엔 칫솔 이나 치약을 뺀 나머지는 비치 되어 있구요\n",
      "부정확률=44.1%, 긍정확률=56.46%\n",
      "\n",
      "기존문장: 리버 사이드 호텔 뷔페는 깔끔하고 서비스도 좋고 무엇보다 재료가 정말 좋아요\n",
      "부정확률=3.58%, 긍정확률=94.46%\n",
      "\n",
      "기존문장: 스크램블 에그에 윤기라고는 없는 푸석푸석함에 고 장난 빵 굽는 기계 식어 있는 음식들\n",
      "부정확률=87.25%, 긍정확률=13.84%\n",
      "\n",
      "기존문장: 욕조에 돈벌레 다리 많은 거 시체에 룸 컨디션은 그냥 그냥 원룸이 낫겟네요\n",
      "부정확률=84.33%, 긍정확률=16.52%\n",
      "\n",
      "기존문장: 새벽 1시에 방을 바꾸는 기 이한 현상이 일어났습니다\n",
      "부정확률=91.21%, 긍정확률=9.09%\n",
      "\n",
      "기존문장: 밥 담배 냄새 불편한 주차 차량 내부 물건 분실 및 파손 불친절 전혀 싼 가격도 아니며 청소 상태가 너무 불량하며 발렛 후 내부 물품 파손 및 분실\n",
      "부정확률=91.96%, 긍정확률=8.06%\n",
      "\n",
      "기존문장: 방음도 안 되서 옆방에서 신음하는 소리까지 들려 너무 민망하더군요\n",
      "부정확률=92.27%, 긍정확률=8.03%\n",
      "\n",
      "기존문장: 근데 더 최악이었던 것은 수영장이었습니다\n",
      "부정확률=73.37%, 긍정확률=28.09%\n",
      "\n",
      "기존문장: 저는 흡연자가 아닙니다\n",
      "부정확률=83.98%, 긍정확률=16.72%\n",
      "\n",
      "기존문장: 설마 설마했는데 정말 모텔보다 열악한 수 페리어 처음으로 사진 찍고 싶은 마음이 안생김 디럭스는 좀 괜찮음\n",
      "부정확률=89.07%, 긍정확률=11.13%\n",
      "\n",
      "기존문장: 위치는 좋았지만 다른 곳으로 가 질 가치가 있습니다\n",
      "부정확률=17.29%, 긍정확률=80.15%\n",
      "\n",
      "기존문장: 너무나도 더러운 호텔방과 화장실 그리고 식당에서 끝임 없이 올라오는 음식 냄새 18일을 예약하고도 이곳 호텔에 도저히 머물 수가 도저히 없어 다 지불한 돈도 포기하고 참다 참다 일주일 만에 나와 버렸습니다\n",
      "부정확률=86.23%, 긍정확률=14.77%\n",
      "\n",
      "기존문장: 건물이 오래되서 그런지 좀 더럽습니다\n",
      "부정확률=79.75%, 긍정확률=21.66%\n",
      "\n",
      "기존문장: 객실은 작은 욕실을 갖춘 매우 표준적인 곳입니다\n",
      "부정확률=21.32%, 긍정확률=76.36%\n",
      "\n",
      "기존문장: 금연 객실인데 화장실에선 담배 냄새가 엄청나 고 타일 벽에는 곰팡이가 군데군데 심하게 있고 매트리스 상태는 좋은데 화장실 청결에 좀 신경 쓰셨으면 해요\n",
      "부정확률=91.18%, 긍정확률=9.1%\n",
      "\n",
      "기존문장: 잠자리가 가장 아늑해서 좋았습니다\n",
      "부정확률=3.67%, 긍정확률=94.3%\n",
      "\n",
      "기존문장: 밤늦게 가서 미안했는데 웃으며 안내해주셔서 감사했습니다ㅎ 매트리스도 얼마나 푹신한 지 정말 일어나 기 싫을 정도 였어요 ㅎㅎㅎ\n",
      "부정확률=13.25%, 긍정확률=84.2%\n",
      "\n",
      "기존문장: 손님에게 끔찍한 상 태의 방을 주는 것은 손님을 배려하는 마음이 없음과 호텔을 정비하는데 직원들이 전혀 자부심을 느끼지 않는다는 것도 보여주었습니다\n",
      "부정확률=90.18%, 긍정확률=10.55%\n",
      "\n",
      "기존문장: 위치 좋고 침대 넓고 히터빵빵 화장실 문이 힘으로도 안 닫힘 냉장고 성능이 너무 좋아서 물과 술이 다 얼엇어요\n",
      "부정확률=4.87%, 긍정확률=92.81%\n",
      "\n",
      "기존문장: 방은 만족스러웠습니다\n",
      "부정확률=13.99%, 긍정확률=83.27%\n",
      "\n",
      "기존문장: 초대 도 도움을 주는 객실 서비스나 에어컨 텔레비젼 또는 서비스 의 호텔 호텔 직원들이 언제나 그렇듯이 매우 정 중하고 도와 주려 기본이지만 여기서 나는 이 스파는 좋은 하 지만 아주 비싸지 한국 표준 요컨대 이 호텔은 서비스의 개편 의 경우 훌륭한 4성 5 혼자 하자\n",
      "부정확률=51.16%, 긍정확률=49.24%\n",
      "\n",
      "기존문장: 사진을 올리고 싶은데 올리는 란이 없는 거 같네요\n",
      "부정확률=87.02%, 긍정확률=13.81%\n",
      "\n",
      "기존문장: 위치 교통편리 식당\n",
      "부정확률=4.2%, 긍정확률=93.68%\n",
      "\n",
      "기존문장: 바닥 테이블은 청소도 제대로 안 되어 있고 드라이기는 언제 마지막으로 썼던 건지 먼지가 많아서 쓸 생각도 못했구요\n",
      "부정확률=90.61%, 긍정확률=10.01%\n",
      "\n",
      "기존문장: 2일 밤을 묵었는데 남편과 아이 한 명 추가 요금을 지불하지 않기 때문에 거주할 수 있는 가족 룸은 그냥 방은 꼭대기 층에 있는 방은 단 한 2 5 성급 그 방에 있을 것이다\n",
      "부정확률=86.1%, 긍정확률=15.28%\n",
      "\n",
      "기존문장: 하지만 관리가 너무도 너무도 안 되있음\n",
      "부정확률=65.79%, 긍정확률=35.7%\n",
      "\n",
      "기존문장: 노래 말하는 나는 돈을 지불해야 갈 수 있는 체육관 호텔 직원들은 예의 바르고 좋더군요\n",
      "부정확률=11.74%, 긍정확률=85.6%\n",
      "\n",
      "기존문장: 위치 요금 접근성\n",
      "부정확률=4.27%, 긍정확률=93.51%\n",
      "\n",
      "기존문장: 조식도 메뉴들이 많이 빠져 있더라구요\n",
      "부정확률=87.07%, 긍정확률=13.77%\n",
      "\n",
      "기존문장: 프로모션 할 때 방문하면 더 만족도가 높습니다\n",
      "부정확률=32.39%, 긍정확률=67.17%\n",
      "\n",
      "기존문장: 깨진 변기 물새는 욕조 구겨져 있는 침구 침구에 머리가락 등 등 침대 가까이에서 핸드폰 충전을 할 콘센트도 없고 리버 사이드 호텔은 그냥 인도어 골프나 쳐야겠어여\n",
      "부정확률=89.29%, 긍정확률=11.2%\n",
      "\n",
      "기존문장: 아쉬움점은 침대 매트리스 쿠션이 푹 들어가서 불편했습니다\n",
      "부정확률=91.06%, 긍정확률=9.05%\n",
      "\n",
      "기존문장: 원격 방에서 작동하지 않음 이 플랫 오늘 내 동료와 저는 데 필요한 조퇴 여행 우리가 생각했던 이른 아침 식사 척척 7 이전 가능하고 반대로 변경 다른 호텔 및 때 허용되는 영역으로 척척 준비 한심한 곡물 의 선택입니다\n",
      "부정확률=77.31%, 긍정확률=24.03%\n",
      "\n",
      "기존문장: 창밖의 고가 도로 풍경과 모든 시설들이 아주 기대 이상이였습니다\n",
      "부정확률=6.19%, 긍정확률=91.35%\n",
      "\n",
      "기존문장: 정말 방이었기 때문에 3 방 번호 없는 방을 변경 시간 였습니다\n",
      "부정확률=91.16%, 긍정확률=9.29%\n",
      "\n",
      "기존문장: 룸 서비스가 안 되서 아쉬움\n",
      "부정확률=88.77%, 긍정확률=11.8%\n",
      "\n",
      "기존문장: 조식이 맛 났어요\n",
      "부정확률=21.48%, 긍정확률=76.06%\n",
      "\n",
      "기존문장: 프론트 데스크 직원들의 친절한 서비스도 여행 내내 참 기분 좋아지게 만들어 주었습니다\n",
      "부정확률=3.56%, 긍정확률=94.32%\n",
      "\n",
      "기존문장: 욕조랑 샤워호스랑 헤드도 너무 낡았고 교체 시기가 지나 보이는 듯해요\n",
      "부정확률=88.2%, 긍정확률=12.5%\n",
      "\n",
      "기존문장: 화장실 샤워 부스 모두 낡은 것이었고 퀘퀘한 냄새가 나서 매우 불쾌했음\n",
      "부정확률=92.82%, 긍정확률=7.27%\n",
      "\n",
      "기존문장: 그럼에도 불구하고 위치와 가격이 좋아서 또 이용해야 될 거 같지만 여유가 있으시다면 추천하지는 않습니다\n",
      "부정확률=70.37%, 긍정확률=31.21%\n",
      "\n",
      "기존문장: 리버 사이드 호텔의 강점은 전망이 좋고 푹신한 침대에 요 다른 호텔에 비해서 침대가 폭신하고 아주 좋아요\n",
      "부정확률=3.9%, 긍정확률=93.91%\n",
      "\n",
      "기존문장: 진짜 해도 해도 너무 한 거 아닌가요 관리자분이 사과는 하셨지만 정말 믿기지 않았어요\n",
      "부정확률=67.38%, 긍정확률=34.03%\n",
      "\n",
      "기존문장: 욕실이 지저분 하고 냄새가 좀 났습니다\n",
      "부정확률=92.75%, 긍정확률=7.22%\n",
      "\n",
      "기존문장: 위치는 좋습니다\n",
      "부정확률=5.44%, 긍정확률=92.23%\n",
      "\n",
      "기존문장: 호텔 주변에 음식점도 많고 부대시설도 잘 되어 있어서 편햇습니다\n",
      "부정확률=3.2%, 긍정확률=94.93%\n",
      "\n",
      "기존문장: 단 밤에는 지하에서 클럽을 운영을 하고 있더라구요\n",
      "부정확률=51.63%, 긍정확률=48.68%\n",
      "\n",
      "기존문장: 복도에서 냄새가 나는 화장실에서 냄새가 배 이다\n",
      "부정확률=91.67%, 긍정확률=8.42%\n",
      "\n",
      "기존문장: 벽에 얼룩도 있고 방음도 잘 안 되더라구요\n",
      "부정확률=92.89%, 긍정확률=7.16%\n",
      "\n",
      "기존문장: 위치 교통 뷰\n",
      "부정확률=4.31%, 긍정확률=93.5%\n",
      "\n",
      "기존문장: 역겨운 검은 곰팡이 곰팡이 카펫 얼룩 스테인드 벽 녹 채택 바닥이나 벽이 되어 메스껍 개구 부에 이 호텔이 별 4 고객에게 잠재적인 문제에서 잘못된 호주 제가 연락을 여행사 연락하여 해당 공급업체 사람들에게 호텔은 꿈쩍도 하지 않을 아무 것도 없습니다\n",
      "부정확률=91.18%, 긍정확률=9.07%\n",
      "\n",
      "기존문장: 가족 모임이 있어 리버 사이드 뷔페를 다녀와 서 차에 심한 기스를 발견 했습니다\n",
      "부정확률=91.59%, 긍정확률=8.64%\n",
      "\n",
      "기존문장: 주차 팀장과 통화한 결과 호텔은 잘못한 게 없다는 답을 받았습니다\n",
      "부정확률=88.95%, 긍정확률=11.56%\n",
      "\n",
      "기존문장: 호텔 벽도 굉장히 얇아서 복도 소리가 다 들릴 정도였어요\n",
      "부정확률=90.31%, 긍정확률=10.4%\n",
      "\n",
      "기존문장: 그리고 호텔 앞에 스파 레이 여성 전용 가 있으므로 화장을 하지 않고 갈 수 있어 편리했습니다\n",
      "부정확률=3.58%, 긍정확률=94.42%\n",
      "\n",
      "기존문장: 뮤즐리 오트밀 과일 없습니다\n",
      "부정확률=87.73%, 긍정확률=12.74%\n",
      "\n",
      "기존문장: 오래된 속담 에 돈을 얻을 수 있는 것이 적절하게 입니다\n",
      "부정확률=16.63%, 긍정확률=80.42%\n",
      "\n",
      "기존문장: 조식은 미리 지불하시는 게 더 저렴하게 이용할 수 있더라고요 잘 묵 다 갑니다\n",
      "부정확률=8.9%, 긍정확률=88.38%\n",
      "\n",
      "기존문장: 나는 이 호텔이 좋아요\n",
      "부정확률=27.57%, 긍정확률=70.99%\n",
      "\n",
      "기존문장: 오래됐지만 최대한 깨끗하게 한 느낌은 있었어요\n",
      "부정확률=50.42%, 긍정확률=49.97%\n",
      "\n",
      "기존문장: 친구와 강 남쪽에서 깨끗한 컨디션 호텔 찾던 중에 묵었는데 만족했습니다\n",
      "부정확률=4.33%, 긍정확률=93.64%\n",
      "\n",
      "기존문장: 하루 정도 도심에서 묵 기엔 만족스러웠습니다\n",
      "부정확률=18.15%, 긍정확률=79.19%\n",
      "\n",
      "기존문장: 그리고 바로 역도 가까이 있어서 이동이 편하더라구요\n",
      "부정확률=3.28%, 긍정확률=94.76%\n",
      "\n",
      "기존문장: 또 추워서 온풍기를 틀었더니 상당히 건조했어요\n",
      "부정확률=90.46%, 긍정확률=10.64%\n",
      "\n",
      "기존문장: 외관은 멋있음\n",
      "부정확률=7.22%, 긍정확률=90.1%\n",
      "\n",
      "기존문장: 화장실도 깨끗한 편이였으며 침대의 디자인도 예뻐 만족 했다\n",
      "부정확률=3.11%, 긍정확률=94.95%\n",
      "\n",
      "기존문장: 저는 감동\n",
      "부정확률=40.43%, 긍정확률=59.11%\n",
      "\n",
      "기존문장: 그리고 방음이 잘 되지 않아서 자 다 몇 번 설쳤습니다\n",
      "부정확률=90.81%, 긍정확률=9.25%\n",
      "\n",
      "기존문장: 이 호텔은 내가 머물렀던 최악의 호텔입니다\n",
      "부정확률=74.18%, 긍정확률=27.71%\n",
      "\n",
      "기존문장: 야경이 좋아요\n",
      "부정확률=3.94%, 긍정확률=93.97%\n",
      "\n",
      "기존문장: 금연 객실이였던 거 같은데 담배 냄새가 조금 났어요\n",
      "부정확률=87.75%, 긍정확률=12.76%\n",
      "\n",
      "기존문장: 환기 안 됨 수영장 수심 1미터 팔 저으면 땅 짚음 수온은 냉탕 하루 종일 호스로 온수 공급 의미 없음\n",
      "부정확률=91.2%, 긍정확률=9.1%\n",
      "\n",
      "기존문장: 가서 쥬스 한 잔이랑 씨리얼 우유 먹고 나왔는데 이걸 돈 2 3만 흠 생각이 필요해지네요\n",
      "부정확률=90.03%, 긍정확률=10.12%\n",
      "\n",
      "기존문장: 한남대교가 보이는 창이 있어서 야경 전망은 좋았음\n",
      "부정확률=3.63%, 긍정확률=94.39%\n",
      "\n",
      "기존문장: 위치와 가성비 짱 이것 말곤 노답 지 하나 1층에 클럽 있나요\n",
      "부정확률=25.7%, 긍정확률=72.25%\n",
      "\n",
      "기존문장: 예약 전 미리 안내가 되었으면 좋겠어요\n",
      "부정확률=89.88%, 긍정확률=10.58%\n",
      "\n",
      "기존문장: 옆 룸 샤워시에 기계 웅 돌아가는 소리가 다 들립니다\n",
      "부정확률=92.73%, 긍정확률=7.15%\n",
      "\n",
      "기존문장: 시설 노후\n",
      "부정확률=89.73%, 긍정확률=10.05%\n",
      "\n",
      "기존문장: 위치 사우나 시설 맛있는 1층 식당\n",
      "부정확률=3.87%, 긍정확률=94.06%\n",
      "\n",
      "기존문장: 조식은 맛있었어요\n",
      "부정확률=11.9%, 긍정확률=85.19%\n",
      "\n",
      "기존문장: 실제로는 창문을 열 수 없었고요 이 점 수정 되었으면 좋겠어요\n",
      "부정확률=88.99%, 긍정확률=11.2%\n",
      "\n",
      "기존문장: 이것이 할 것 입니다\n",
      "부정확률=84.28%, 긍정확률=16.72%\n",
      "\n",
      "기존문장: 그냥 침대와 하얀 이불 하나 끝 1 좁은 객실 2 화장실이 바닥만 깨긋하면 되는 줄 아나봄 샤워커튼에 물 때와 검은 물곰팡이가 조금이 아니고 샤워 커튼 전체에 역겹고 토 나올 만큼 다 퍼져 있었음\n",
      "부정확률=90.59%, 긍정확률=9.85%\n",
      "\n",
      "기존문장: 가로수길과 한 강 접근성이 좋았습니다\n",
      "부정확률=3.39%, 긍정확률=94.66%\n",
      "\n",
      "기존문장: 객실이 좁은 건 그렇다 치더라도 욕실 도어는 유리에 붙은 시트지는 지져 분하게 찢어져 있고 샤워 끝내고 닦으려고 보니 타올 도 셋팅 안 되어 있 었습니다\n",
      "부정확률=91.51%, 긍정확률=9.12%\n",
      "\n",
      "기존문장: 심지어 전에 있던 사람 걸로 보이는 체 모가 있었어요\n",
      "부정확률=88.56%, 긍정확률=12.12%\n",
      "\n",
      "기존문장: 한 두 번도 아니고 씻는 내내 다음날 아침까지 그래서 호텔 직원한테 체크아웃하며 말하니 그냥 아 그러셨어요\n",
      "부정확률=84.91%, 긍정확률=16.06%\n",
      "\n",
      "기존문장: 정말 이런 호텔 최악이었습니다\n",
      "부정확률=89.99%, 긍정확률=10.33%\n",
      "\n",
      "기존문장: 그리고 더운 여름에 시원한 에어컨이 짱인데 다른 호텔과 비교 할 때 에이컨이 아주 좋아요\n",
      "부정확률=7.33%, 긍정확률=90.29%\n",
      "\n",
      "기존문장: 식기에도 먼지나 이물질 같은 게 자꾸 보였어요\n",
      "부정확률=92.0%, 긍정확률=8.06%\n",
      "\n",
      "기존문장: 지하에 큰 나이트클럽이 있어 소음과 주변 환경이 가족 여행으로는 적합하지 않습니다\n",
      "부정확률=90.39%, 긍정확률=10.09%\n",
      "\n",
      "기존문장: 이런 곳이 어떻게 호텔이라는 이름으로 장사하는지 모르겠네요\n",
      "부정확률=75.23%, 긍정확률=26.5%\n",
      "\n",
      "기존문장: 그냥 혼자만의 공 간이 필요했는데 시티와 한강 너머 남산까지 보이는 룸으로 배정을 해 주셨더라구요\n",
      "부정확률=16.12%, 긍정확률=81.21%\n",
      "\n",
      "기존문장: 가격 대비 위치도 컨디션도 괜찮았습니다\n",
      "부정확률=3.6%, 긍정확률=94.4%\n",
      "\n",
      "기존문장: 위치가 좋고 편해요\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "부정확률=3.67%, 긍정확률=94.34%\n",
      "\n",
      "기존문장: 더블 예악을 방 없다고 트윈으로 접수되었다고 알아서 하지를 않나 역대 수많은 출장을 다니지만 최악입니다\n",
      "부정확률=89.48%, 긍정확률=11.22%\n",
      "\n",
      "기존문장: 직원도 친절하고 끝방으로 예약되어서 넓고 좋았어요\n",
      "부정확률=3.5%, 긍정확률=94.52%\n",
      "\n",
      "기존문장: 감자튀김 케찹 맛집 에어컨 끄고 잘라니까 에어컨에서 물 떨어집니다\n",
      "부정확률=91.14%, 긍정확률=9.16%\n",
      "\n",
      "기존문장: 나는 그에게 해줄 수 있는 모든 것을 던져 적어도 한 아침 조간 동일한 없다고 하더군요\n",
      "부정확률=90.83%, 긍정확률=9.68%\n",
      "\n",
      "기존문장: 종류도 다양하구요\n",
      "부정확률=4.28%, 긍정확률=93.62%\n",
      "\n",
      "기존문장: 근데 1층에 클럽 쪽에 안 좋은 냄새가 나요\n",
      "부정확률=51.45%, 긍정확률=49.12%\n",
      "\n",
      "기존문장: 욕실의 물이 줄줄 세고 퀘퀘한 냄새가 전반적으로 많이 났어요\n",
      "부정확률=91.44%, 긍정확률=8.58%\n",
      "\n",
      "기존문장: 깜짝 놀라서 뛰쳐 나왔어요\n",
      "부정확률=79.21%, 긍정확률=21.84%\n",
      "\n",
      "기존문장: 주변의 상점들이 있어 편리한 위치였다\n",
      "부정확률=3.4%, 긍정확률=94.54%\n",
      "\n",
      "기존문장: 강남 위치\n",
      "부정확률=8.77%, 긍정확률=88.63%\n",
      "\n",
      "기존문장: 수건에 무슨 건 더기 같은 거 붙어 있어서 안 썻네요\n",
      "부정확률=88.98%, 긍정확률=11.34%\n",
      "\n",
      "기존문장: 그는 노골 로 인정 하 않으셔도 아무것도 여행에 지쳐 포기하고 있는 우리의 스탠다드 더블 룸에 도착했습니다\n",
      "부정확률=34.97%, 긍정확률=63.32%\n",
      "\n",
      "기존문장: 잠자는데 조용했음\n",
      "부정확률=9.69%, 긍정확률=87.64%\n",
      "\n",
      "기존문장: 다음에 조식을 경험래 보고 싶네요\n",
      "부정확률=8.46%, 긍정확률=88.85%\n",
      "\n",
      "기존문장: 로비 엘리베이터 안내문 등은 별로 이나 남성 사우나는 매우 좋음\n",
      "부정확률=44.6%, 긍정확률=54.99%\n",
      "\n",
      "기존문장: 주차가 대박입니다\n",
      "부정확률=5.53%, 긍정확률=92.05%\n",
      "\n",
      "기존문장: 비 데는 고장 나서 작동도 안하고요 온니 잠과 가벼운 조식을 원한다면 가 볼만 하지만 휴식으로써는 아닌 듯 합니다\n",
      "부정확률=67.1%, 긍정확률=34.42%\n",
      "\n",
      "기존문장: 콘센트 위치가 화장실도 없고 티비 아래 멀티 소켓 한 자리만 있어서 머리를 말리면서 거울을 확인도 못하고 핸드폰 충전도 너무 떨어져서 해야 했습니다\n",
      "부정확률=92.52%, 긍정확률=7.63%\n",
      "\n",
      "기존문장: 하지만 대체적으로 만족합니다 면도기가 없드라구요\n",
      "부정확률=69.05%, 긍정확률=32.28%\n",
      "\n",
      "기존문장: 하지만 객실은 너무 낡아서 숙박은 다시 하고 싶진 않았습니다\n",
      "부정확률=61.06%, 긍정확률=40.59%\n",
      "\n",
      "기존문장: 금간 욕실 타일 바닥 벗겨진 페인트칠 얼룩덜륵 한 테이블 등 침대 빼곤 전체적으로 너무 낡았다는 생각이 드는 곳이었어요 ㅠㅠ\n",
      "부정확률=91.66%, 긍정확률=8.63%\n",
      "\n",
      "기존문장: 다만 예약을 할 때 연회장이 있어서 시끄럽다는 안내가 없었어요\n",
      "부정확률=44.03%, 긍정확률=55.81%\n",
      "\n",
      "기존문장: 주차장은 스릴러 영화에 나오는 더러운 창고 같은 곳이었고 어둡고 더럽고 냄새나고 정말 최악 객실이랑 이어져 있지도 않아서 계속 왔다 갔다\n",
      "부정확률=86.64%, 긍정확률=14.33%\n",
      "\n",
      "기존문장: 그런데 환불 말씀도 없으시더라구요\n",
      "부정확률=58.21%, 긍정확률=43.22%\n",
      "\n",
      "기존문장: 주변 식당가들도 많고 가로수길도 가까워서 주 편 편의도 잘 되어 있습니다\n",
      "부정확률=3.23%, 긍정확률=94.82%\n",
      "\n",
      "기존문장: 두시까지 체크아웃 되서 편안함\n",
      "부정확률=9.3%, 긍정확률=87.98%\n",
      "\n",
      "기존문장: 객실에 먼지가 많은 지 계속 기침이 나옵니다\n",
      "부정확률=92.55%, 긍정확률=7.43%\n",
      "\n",
      "기존문장: 냉장고에 생수 두 개도 있어요\n",
      "부정확률=24.83%, 긍정확률=72.74%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "\n",
    "test_correct = 0\n",
    "test_total = 0\n",
    "result = []\n",
    "for input_ids_batch, attention_masks_batch, original_text in tqdm(test_loader):\n",
    "    #y_batch = y_batch.to(device)\n",
    "    y_pred = model(input_ids_batch.to(device), attention_mask=attention_masks_batch.to(device))[0]\n",
    "    for idx in range(len(y_pred)):\n",
    "        tmp = []\n",
    "        tmp.append(original_text[idx])\n",
    "        print(f'기존문장: {original_text[idx]}')\n",
    "        pred_tensor = F.sigmoid(y_pred[idx])\n",
    "        print(f'부정확률={round(float(pred_tensor[0]*100),2)}%, 긍정확률={round(float(pred_tensor[1]*100),2)}%')\n",
    "        print(\"\")\n",
    "        \n",
    "        tmp.append(round(float(pred_tensor[0]*100),2))\n",
    "        tmp.append(round(float(pred_tensor[1]*100),2))\n",
    "        result.append(tmp)\n",
    "    _, predicted = torch.max(y_pred, 1)\n",
    "    #test_correct += (predicted == y_batch).sum()\n",
    "    #test_total += len(y_batch)\n",
    "\n",
    "#print(\"Accuracy:\", test_correct.float() / test_total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "W32RoC-iVJKA"
   },
   "outputs": [],
   "source": [
    "# 모델 저장하기\n",
    "torch.save(model.state_dict(), \"result/MODEL_KoELECTRA-Base-v3-discriminator__1.pt\")\n",
    "r = pd.DataFrame(result)\n",
    "r.columns = ['text','0','1']\n",
    "r.to_csv('result/Result_KoELECTRA-Base-v3-discriminator__1.csv', index=False, encoding=\"utf-8-sig\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1: 92.45\n",
    "# 2: 92.63\n",
    "# 2:"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "nhs-huggingface-koelectra.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "1a8dc4b92ea249a397303abfd6291294": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "36ac2994831e49b6851fbddda65abb27": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "39250dc036644093a6e93f7f33199ce7": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "39bdbe27bc4a4b9ebf1b72b2eb6f1545": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_36ac2994831e49b6851fbddda65abb27",
      "placeholder": "​",
      "style": "IPY_MODEL_e94e58052fd2492483004fb61b91d961",
      "value": " 1285/1285 [01:28&lt;00:00, 14.48it/s]"
     }
    },
    "3f97a9a16f394f79a48d4d81c3a8f52a": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "100%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_6948f6f65b5f442da4a9e69251a26426",
      "max": 1285,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_88eb7cf803c149d79db9c220894d3d79",
      "value": 1285
     }
    },
    "475a67da89c84bf59da6757d417c2bfb": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_90ec055800934145ada75fec0189d7a8",
      "placeholder": "​",
      "style": "IPY_MODEL_1a8dc4b92ea249a397303abfd6291294",
      "value": " 1238/1238 [10:30&lt;00:00,  1.96it/s]"
     }
    },
    "5238716048584ce88492fda4f2188bde": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "52a0477faf594d9eb4be099faf241034": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "6857abb746dc4b81b0b60b5e5bd1c30e": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a9b89d30fb9742fd9db5d87ad6f7f77d",
      "placeholder": "​",
      "style": "IPY_MODEL_bdba4111e4b74175852037315d39696d",
      "value": " 1238/1238 [10:30&lt;00:00,  1.96it/s]"
     }
    },
    "6948f6f65b5f442da4a9e69251a26426": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "726995c90ca1424c90dcc3ba2a1203f1": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_7d72674cd0e941a2a58c9477981bd8ff",
       "IPY_MODEL_bd8883390af54afd8a757af54093bef6"
      ],
      "layout": "IPY_MODEL_b487bf573b7f41398447c5f8f63c1e04"
     }
    },
    "7a6c71ef3b7a4308b47198739df410e1": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "7d72674cd0e941a2a58c9477981bd8ff": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "100%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_52a0477faf594d9eb4be099faf241034",
      "max": 1238,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_39250dc036644093a6e93f7f33199ce7",
      "value": 1238
     }
    },
    "7f4e77863c944fdd8d9e014109740698": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "8750469d909844c8a46b175b280c3cef": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "88eb7cf803c149d79db9c220894d3d79": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "893d3ddd1faf4261864154222d665926": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "100%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_e75e6f02eda7438d869a3c3e0efa2bec",
      "max": 1238,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_5238716048584ce88492fda4f2188bde",
      "value": 1238
     }
    },
    "90ec055800934145ada75fec0189d7a8": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a1d81cedb665463096f3160000c131e5": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a9b89d30fb9742fd9db5d87ad6f7f77d": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b487bf573b7f41398447c5f8f63c1e04": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "bd47b0c2081f4c9e8a5c4214b9704c50": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "bd8883390af54afd8a757af54093bef6": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a1d81cedb665463096f3160000c131e5",
      "placeholder": "​",
      "style": "IPY_MODEL_7a6c71ef3b7a4308b47198739df410e1",
      "value": " 1238/1238 [11:43&lt;00:00,  1.76it/s]"
     }
    },
    "bdafcfe4bdd04aca9f04479900008253": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_fa0791a8b0894c91910f7022429778b7",
       "IPY_MODEL_6857abb746dc4b81b0b60b5e5bd1c30e"
      ],
      "layout": "IPY_MODEL_7f4e77863c944fdd8d9e014109740698"
     }
    },
    "bdba4111e4b74175852037315d39696d": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "cc5dbed4ea034dc29af76a67f0e36692": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_3f97a9a16f394f79a48d4d81c3a8f52a",
       "IPY_MODEL_39bdbe27bc4a4b9ebf1b72b2eb6f1545"
      ],
      "layout": "IPY_MODEL_bd47b0c2081f4c9e8a5c4214b9704c50"
     }
    },
    "d777dd668d0f4dc196db2632ac657558": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e75e6f02eda7438d869a3c3e0efa2bec": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e94e58052fd2492483004fb61b91d961": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "e9cc6c95f26e4fe98dff51c8a7ac7389": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "f45a29b35b4a44e5bf2a635d7525659c": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_893d3ddd1faf4261864154222d665926",
       "IPY_MODEL_475a67da89c84bf59da6757d417c2bfb"
      ],
      "layout": "IPY_MODEL_d777dd668d0f4dc196db2632ac657558"
     }
    },
    "fa0791a8b0894c91910f7022429778b7": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "100%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_8750469d909844c8a46b175b280c3cef",
      "max": 1238,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_e9cc6c95f26e4fe98dff51c8a7ac7389",
      "value": 1238
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
